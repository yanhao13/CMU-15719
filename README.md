# CMU-15719
### Introduction, Use cases, and Elasticity
### A View of Cloud Computing
### [Armbrust2010](https://dl.acm.org/doi/10.1145/1721654.1721672)
### Clearing the clouds away from the true potential and obstacles posed by this computing capability.
### Oracle's CEO Larry Ellison vent his frustration: "The interesting thing about cloud computing is that we've redefined cloud computing to include exerything that we already do.... I don't understand what we would do differently in the light of cloud computing other than change the wording of some of our ads."
### Just as large ISPs use multiple network providers so that failure by a single company will not take them off the air, we believe the only plausible solution to very high availability is multiple cloud computing providers.
cloud computing, the long-held dream of computing as a utility, has the potential to transform a large part of it industry, making sw even more attractive aaS and shaping the way it hw is designed and purchased; 
developers with innovative ideas for new internet services no longer require the large capital outlays in hw to deploy their service or the human expense to operate it, they need not be concerned about overprovisioning for a service whose popularity does not meet their predictions, hus wasting costly resources, or underprovisioning for one that becomes wildly popular, thus missing potential customers and revenue; 
moreover, companies with large batch-oriented tasks can get results as quickly as their programs can scale, since using 1000 servers for 1hr costs no more than using 1 server for 1000hrs, this elasticity of resources, without paying a premium for large scale, is unprecedented in history of it.
### Defining Cloud Computing
cloud computing refers to both the applications delivered aaS over the internet and the hw and systems sw in the datacenters that provide those services, the services themselves have long been referred to as SaaS(*here we use the term SaaS to mean applications delivered over the internet, the broadest definition would encompass any on demand sw, including those that run sw locally but control use via remote sw licensing), some vendors use terms such as IaaS, PaaS to describe their products, but we eschew these because accepted definitions for them still vary widely, the line bt low-level infra and a higher-level platform is not crisp, we believe the two are more alike than different, and we consider them together; similarly, the related term grid computing, from the hp computing community, suggests protocols to offer shared computation and storage over long distances, but those protocols did not lead to a sw environment that grew beyond its community; 
the datacenter hw and sw is what we will call a **cloud**, when a cloud is made available in a pay-as-you-go manner to the general public, we call it a **public cloud**, the service being sold is **utility computing**, we use the term **private cloud** to refer to internal datacenters of a business or other org, not made available to the general public, when they are large enough to benefit from the advantages of cloud computing that we discuss here; thus cloud computing is sum of SaaS and utility computing, but does not include small or medium sized datacenters, even if these rely on virtualization for mgmt, people can be users or providers of SaaS, or users or providers of utility computing; we focus on SaaS providers(cloud users) and cloud providers, which have received less attention than SaaS users, [click for making provider-user relationships clear](./img/making-provider-user-relationships-clear.png), in some cases, the same actor can play multiple roles, for example, a cloud provider might also host its own customer-facing services on cloud infra; 
from a hw provisioning and pricing point of view, 3 aspects are new in cloud computing including: the appearance of infinite computing resources available on demand, quickly enough to follow load surges, thereby eliminating need for cloud computing users to plan far ahead for provisioning; elimination of an up-front commitment by cloud users, thereby allowing companies to start small and increase hw resources only when there is an increase in their needs(*note, however, that upfront commitments can still be used to reduce per-usage charges, for exmaple, AWS also offers long-term rental of servers, which they call reserved instances); the ability to pay for use of computing resources on a short-term basis as needed(such as processors by the hr and storage by the day) and release them as needed, thereby rewarding conservation by letting machines and storage go when they are no longer useful; 
we argue that construction and operation of extremely large scale, commodity-computer datacenters at low cost locations was the key necessary enabler of cloud computing, for they uncovered factors of 5-7 decrease in cost of electricity, network bdwidth, ops, sw, and hw available at these very large economies of scale; these factors, combined with statistical multiplexing to increase utilization compared to traditional datacenters, meant that cloud computing could offer services below the costs of a medium sized datacenter and yet still make a good profit; 
out proposed definition allows us to clearly identify certain installations as examples and non-examples of cloud computing, consider a public-facing internet service hosted on an ISP who can allocate more machines to the service given 4hrs notice, since load surges on the public internet can happen much more quickly than that(we saw its load to double every 12 hrs for nearly 3 days), this is not cloud computing, in contrast, consider an internal enterprise datacenter whose applications are modified only with significant advance notice to admins, in this scenario large load surges on the scale of mins are highly unlikely, so as long as allocation can track expected load increases, this scenario fulfills one of the necessary conditions for operating as a cloud, the enterprise datacenter may still fail to meet other conditions for being a cloud, however, such as the appearance of infinite resources for fine-grained billing; a private datacenter may also not benefit from economies of scale that make public clouds financially attractive; 
omitting private clouds from cloud computing has led to considerable debate in the blogosphere, we believe the confusion and skepticism illustrated by Larry Ellison's quote occurs when advantages of public clouds are also claimed for medium sized datacenters, except for extremely large datacenters of hundreds of thousands of machines, such as those that might be operated by Google, Microsoft, [most datacenters enjoy only a subset of the potential advantages of public clouds, click to view](./img/most-data-centers-enjoy-only-a-subset-of-the-potential-advantages-of-public-clouds.png), hence we believe that including traditional datacenters in the definition of cloud computing will lead to exaggerated claims for smaller so-called private clouds, which is why we exclude them, however, here we describe how so-called private clouds can get more of the benefits of public clouds through surge computing, hybrid cloud computing.
### Classes of Utility Computing
any application needs a model of computation, a model of storage, and a model of comm, the statistical multiplexing necessary to achieve elasticity and the appearance of infinite capacity available on demand requires automatic allocation and mgmt, in practice, this is done with virtualization of some sort, we argue that different utility computing offerings will be distinguished based on the cloud sytem sw's level of abstraction and on the level of mgmt of resources; 
Amazon EC2 is at one end of the spectrum, an EC2 instance looks much like physical hw, and users can control nearly the entire sw stack, from the kernel upward, this low level makes it inherently difficult for Amazon to offer automatic scalability and failover because the semantics associated with replication and other state mgmt issues are highly application-dependent; at the other extreme of the spectrum are application domain-specific platforms such as Google AppEngine, which is targeted exclusively at traditional web applications, enforcing an application structure of clean separation bt a stateless computation tier and a stateful storage tier, AppEngine's impressive automatic scaling and high-availability mechanisms, and the proprietary MegaStore data storage available to AppEngine applications, all rely on these constraints; applications for Microsoft's Azure are written using .NET libs, and compiled to the Common Language Runtime, a lang-independent managed environment, the framework is significantly more flexible than AppEngine's, but still constrains user's choice of storage model and application structure, hence Azure is intermediate bt hw vms like EC2 and application frameworks like AppEngine.
### Cloud Computing Economics
we see 3 particularly compelling use cases that favor utility computing over conventional hosting, including: (i)when demand for a service varies with time, for example, provisioning a datacenter for the peak load it must sustain a few days per month leads to underutilization at other times, instead, cloud computing lets an org pay by hr of computing resources, potentially leading to cost savings even if the hourly rate to rent a machine from a cloud provider is higher than the rate to own one; (ii)when demand is unknown in advance, for example, a web startup will need to support a spike in demand when it becomes popular, followed potentially by a reduction once some visitors turn away; (iii)orgs that perform batch analytics can use the cost associativity of cloud computing to finish computations faster :using 1000 EC2 machines for 1hr costs the same as using 1 machine for 1000hrs; 
although the economic appeal of cloud computing is often described as CapEx2OpEx -converting capital expenses to operating expenses, we believe the phrase pay-as-you-go more directly captures the economic benefit to the buyer, hrs purchased via cloud computing can be distributed non-uniformly in time(for example, use 100 server hrs today and no server hrs tomorrow, and still pay only for 100), in the networking community, this way of selling bdwidth is already known as usage-based pricing(*usage-based pricing is not renting, renting a resources involves paying a negotiated cost to have the resource over some time period, whether or not you use the resource, vs., pay-as-you-go involves metering usage and charging based on actual use, independently of the time period over which the usage occurs); in addition, absence of upfront capital expense allows capital to be rediected to core business investment; 
hence, even if Amazon's pay-as-you-go pricing was more expensive than buying and depreciating a comparable server over the same period, we argue that the cost is outwrighed by the extremely important cloud computing economic benefits of elasticity and transference of risk, esp. risks of overprovisioning(underutilization) and underprovisioning(saturation); 
we start with elasticity, key observation is that cloud computing's ability to add or remove resources at a fine grain(1 server at a time with EC2) and with a lead time of mins rather than weeks allows matching resources to wkload much more closely, real world estimates of average server utilization in datacenters range from 5%-20%, this may sound shockingly low, but it is consistent with the observation that for many services the peak wkload exceeds the average by factors of 2-10, since few users deliberately provision for less than the expected peak, resources are idle at nonpeak times, the more pronounced the variation, the more the waste, [click for example in fig2a](./img/elasticity.png), here, we assume our service has a predictable demand where the peak requires 500 servers at noon but the trough requires only 100 servers at midnight, as long as the average utilization over a whole day is 300 servers, the actual cost per day(area under the curve) is 300times24=7200 server hrs, but since we must provision to the peak of 500 servers, we pay for 500times24=12000 server hrs, a factor of 1.7 more; hence, as long as the pay-as-you-go cost per server hr over 3 years(typically amortization time) is less than 1.7 times cost of buying the server, utility computing is cheaper; in fact, this example underestimates benefits of elasticity, because in addition to simple diurnal patterns, most services also experience seasonal or other periodic demand variation(such as e-commerce in December and photo sharing sites after holidays) and some unexpected demand bursts due to external events(such as news events), since it can take weeks to acquire and rack new equipment, to handle such spikes you must provision for them in advance, we already saw that even if service operators predict the spike sizes correctly, capacity is wasted, and if they overestimate the spike they provision for, it is even worse; 
[they may also underestimate spike, click to view in fig2b](./img/elasticity.png), however, accidentally turning away excess users, while cost of overprovisioning is easily measured, cost of underprovisioning is more difficult to measure yet potentially equally serious :not only do rejected users generate 0 revenue, they may never come back; [for example, Friendster's decline in popularity relative to competitors Facebook, MySpace, is believed to have resulted partly from user dissatisfaction with slow repsonse times(up to 40 secs), click to view in fig2c](./img/elasticity.png) :users will desert an underprovisioned service until peak user load equals datacenter's usable capability, at which point users again receive acceptable service; for another simplified example, assume that users of a hypothetical site fall into 2 classes :active users(those who use the site regularly) and defectors(those who abandon the site due to poor perf), further, suppose that 10% of active users who receive poor service due to underprovisioning are permanently lost opportunities(become defectors), i.e., users who would have remained regular visitors with a better experience, the site is initially provisioned to handle an expected peak of 400000 users(1000 users per server times 400 servers), but unexpected positive press drives 500000 users in the first hr, of the 100000 who are turned away or receive bad service, by our assumption 10000 of them are permanently lost, leaving an active user base of 390000, the next hr sees 250000 new unique users, the first 10000 do fine, but the site is still overcapacity by 240000 users, this results in 24000 additional defections, leaving 376000 permanent users, if this pattern continues, after lg(500000) or 19hrs, #new users will approach 0 and the site will be at capacity in steady state, clearly, the service operator has collected less than 400000 users' worth of steady revenue during those 19hrs, however, again illustrating the underutilization argument, to say nothing of the bad reputation from disgruntled users; 
do such scenarios really occur in practice? when we made our service available via Facebook, it experienced a demand surge that resulted in growing from 50 servers to 3500 servers in 3 days, even if the average utilization of each server was low, no one could have foreseen that resource needs would suddenly double every 12hrs for 3 days, after the peak subsided, traffic fell to a lower level, so in this real world example, scale-up elasticity allowed the steady-state expenditure to more closely match the steady-state wkload.
### Top 10 Obstacles and Opportunities for Cloud Computing
[click to view](./img/top-ten-obstacles-and-opportunities-for-cloud-computing.png), the first 3 affect adoption, the next 5 affect growth, the last 2 are policy and business obstacles, each obstacle is paired with an opportunity to overcome that obstacle, ranging from product dev to research projects.
### 拨开云雾，展现云计算的真正潜力，并消除其带来的障碍。
### Oracle 首席执行官 Larry Ellison 表达了他的不满：“云计算的有趣之处在于，我们重新定义了云计算，将我们已经在做的一切都涵盖其中……除了修改一些广告措辞之外，我不知道在云计算的背景下，我们还能做什么不同的事情。”
云计算，作为计算作为一种实用工具的长期梦想，有可能改变 IT 行业的很大一部分，使软件即服务 (aaS) 更具吸引力，并塑造硬件的设计和购买方式；
拥有创新互联网服务理念的开发人员不再需要投入大量的硬件资金来部署他们的服务，也不再需要投入大量的人力来运营它，他们不必担心为受欢迎程度未达预期的服务过度配置资源，从而浪费昂贵的资源，也不必担心为已经非常流行的服务配置资源不足，从而错失潜在客户和收入；
此外，拥有大量批处理任务的公司能够像其程序扩展一样快速获得结果，因为使用 1000 台服务器 1 小时的成本不超过使用 1 台服务器 1000 小时的成本，这种资源弹性在历史上是前所未有的，无需为大规模支付额外费用。
### 定义云计算
云计算既指通过互联网以“即服务”（aaS）形式交付的应用程序，也指数据中心中提供这些服务的硬件和系统软件。这些服务本身长期以来被称为 SaaS（*此处我们使用 SaaS 一词表示通过互联网交付的应用程序，最广泛的定义涵盖任何按需软件，包括在本地运行软件但通过远程软件许可控制使用的软件）。一些供应商使用 IaaS、PaaS 等术语来描述他们的产品，但我们避免使用这些术语，因为它们的公认定义仍然差异很大，低级基础设施和高级平台之间的界限并不明确，我们认为两者的相同之处多于不同之处，我们认为将它们结合在一起；同样，来自惠普计算社区的相关术语“网格计算”提出了提供远距离共享计算和存储的协议，但这些协议并没有催生出一个超越其社区的软件环境；数据中心硬件和软件就是我们所说的**云**，当云以按需付费的方式向公众开放时，我们称之为**公共云**，所出售的服务是**效用计算**，我们使用术语**私有云**来指代企业或其他组织的内部数据中心，这些数据中心不向公众开放，但当它们规模足够大到能够从我们在此讨论的云计算优势中受益时；因此，云计算是 SaaS 和效用计算的总和，但不包括中小型数据中心，即使这些数据中心依赖虚拟化进行管理，人们可以是 SaaS 的用户或提供商，也可以是效用计算的用户或提供商；我们关注的是 SaaS 提供商（云用户）和云提供商，这两者的关注度低于 SaaS 用户。[点击查看提供商-用户关系](./img/making-provider-user-relationships-clear.png)，在某些情况下，同一个参与者可以扮演多个角色，例如，云提供商可能还会在云端基础设施上托管自己的面向客户的服务；
从硬件配置和定价的角度来看，云计算有三个新特点：无限计算资源的出现，可以按需提供，并且能够快速应对负载激增，从而消除了云计算用户提前规划配置的需要；取消了云用户的前期承诺，从而允许公司从小规模开始，仅在需求增加时才增加硬件资源（*但请注意，前期承诺仍然可以用来降低每次使用的费用，例如，AWS 也提供服务器的长期租赁，他们称之为预留实例）；能够根据需要短期支付计算资源的使用费用（例如按小时支付处理器费用和按天支付存储费用），并根据需要释放这些资源，从而通过在不再使用时释放机器和存储来奖励节约；
我们认为，在低成本地点建设和运营超大规模商用计算机数据中心是云计算的关键必要因素，因为它们揭示了在这些非常大的规模经济下，电力、网络带宽、操作、软件和硬件成本降低 5 至 7 倍的因素；这些因素与统计复用相结合，提高了与传统数据中心相比的利用率，意味着云计算可以提供低于中型数据中心成本的服务，同时仍然获得良好的利润；
我们提出的定义使我们能够清楚地识别某些安装作为示例和非云计算的例子，考虑一个托管在 ISP 上的面向公众的互联网服务，该 ISP 可以在提前 4 小时通知的情况下为该服务分配更多机器，因为公共互联网上的负载激增可能发生得更快（我们看到它的负载每 12 小时翻一番，持续了近 3 天），这不是云计算。相反，考虑一个内部企业数据中心，其应用程序只有在提前通知管理员的情况下才会进行修改，在这种情况下，分钟级的大规模负载激增极不可能发生，因此只要分配可以跟踪预期的负载增长，这种情况就满足了作为云运营的必要条件之一。然而，企业数据中心可能仍然无法满足成为云的其他条件，例如出现无限资源以实现细粒度计费；私有数据中心也可能无法从使公共云具有经济吸引力的规模经济中受益；
将私有云从云计算中剔除在博客圈引发了相当大的争议，我们认为，拉里·埃里森的引言所体现的困惑和怀疑，发生在公有云的优势也适用于中型数据中心的情况下，除了拥有数十万台机器的超大型数据中心，例如谷歌、微软运营的那些。[大多数数据中心只享有公有云的部分潜在优势，点击查看](./img/most-data-centers-enjoy-only-a-subset-of-the-potential-advantages-of-public-clouds.png)，因此，我们认为将传统数据中心纳入云计算的定义会导致对规模较小的所谓私有云的夸大宣传，这就是我们将其排除在外的原因。然而，我们在这里描述了所谓的私有云如何通过激增计算、混合云计算获得更多公有云的优势。
### 效用计算的类别
任何应用程序都需要一个计算模型、一个存储模型和一个comm，实现弹性和按需无限容量所需的统计复用需要自动分配和管理，实际上，这是通过某种虚拟化实现的，我们认为不同的效用计算产品将根据云系统软件的抽象级别和资源管理级别进行区分；
Amazon EC2 处于一个极端，EC2 实例看起来很像物理硬件，用户可以控制几乎整个软件堆栈，从内核向上。这种低级别使得 Amazon 很难提供自动可扩展性和故障转移，因为与复制和其他状态管理问题相关的语义高度依赖于应用程序；另一个极端是应用程序领域特定平台，例如 Google AppEngine，它专门针对传统的 Web 应用程序，强制执行无状态计算层和有状态存储层干净分离的应用程序结构，AppEngine 令人印象深刻的自动扩展和高可用性机制，以及 AppEngine 应用程序可用的专有 MegaStore 数据存储，都依赖于这些约束；微软 Azure 的应用程序使用 .NET 库编写，并编译为公共语言运行时（Common Language Runtime，一个独立于语言的托管环境）。该框架比 AppEngine 的框架灵活得多，但仍然限制了用户对存储模型和应用程序结构的选择，因此 Azure 介于 EC2 等硬件虚拟机和 AppEngine 等应用程序框架之间。
### 云计算经济学
我们发现 3 个特别引人注目的用例表明，效用计算优于传统托管，包括：(i) 当服务需求随时间变化时，例如，为数据中心配置每月必须维持几天的峰值负载，会导致其他时间利用率不足。相反，云计算允许组织按小时支付计算资源费用，即使从云提供商租用机器的小时费率高于拥有机器的费率，也有可能节省成本； (ii) 当需求事先未知时，例如，一家网络初创公司需要支持其在流行时出现的需求激增，而一旦一些访问者流失，需求可能会减少；(iii) 执行批量分析的组织可以利用云计算的成本关联性来更快地完成计算：使用 1000 台 EC2 机器 1 小时的成本与使用 1 台机器 1000 小时的成本相同；
虽然云计算的经济吸引力通常被描述为资本支出转化为运营支出（CapEx2OpEx），即将资本支出转化为运营支出，但我们认为“按需付费”这一说法更直接地体现了买方的经济效益，通过云计算购买的服务器小时数可以不均匀地分配（例如，今天使用 100 小时服务器时间，明天不使用，仍然只需支付 100 小时的费用），在网络社区中，这种销售带宽的方式被称为基于使用量的定价（*基于使用量的定价不是租用，租用资源涉及支付协商好的费用以在一段时间内拥有资源，无论是否使用资源；而按需付费则涉及计量使用量并根据实际使用情况收费，与使用发生的时间段无关）；此外，由于没有前期资本支出，资本可以重新用于核心业务投资；因此，即使亚马逊的按需付费定价比在同一时期购买和折旧一台同类服务器的成本更高，我们认为，其成本也被云计算极其重要的经济效益所抵消，即弹性和风险转移，尤其是过度配置（利用不足）和配置不足（饱和）的风险；
我们从弹性开始，关键的观察是云计算能够以细粒度添加或删除资源（使用 EC2 一次添加或删除一台服务器），并且只需几分钟而不是几周的准备时间，就可以更紧密地匹配资源以进行负载均衡，现实世界中数据中心的平均服务器利用率估计在 5% 到 20% 之间，这听起来可能低得惊人，但这与以下观察结果一致：对于许多服务而言，峰值负载是平均值的 2 到 10 倍，因为很少有用户故意提供低于预期峰值的资源，资源在非峰值时段处于闲置状态，变化越明显，浪费就越多，[单击图 2a 中的示例](./img/elasticity.png)，在这里，我们假设我们的服务具有可预测的需求，峰值在中午需要 500 台服务器，但低谷在午夜只需要 100 台服务器，只要全天的平均利用率为 300 台服务器，则每天的实际成本（曲线下面积）为300x24=7200 服务器小时，但由于我们必须为 500 台服务器的峰值进行配置，因此我们需要支付 500x24=12000 服务器小时的费用，比峰值高出 1.7 倍；因此，只要 3 年内（通常是摊销期）每服务器小时的即用即付成本小于购买服务器成本的 1.7 倍，效用计算就会更便宜；事实上，这个例子低估了弹性的优势，因为除了简单的昼夜模式外，大多数服务还会经历季节性或其他周期性需求变化（例如 12 月的电子商务和节假日后的照片分享网站）以及一些由于外部事件（例如新闻事件）导致的意外需求爆发，因为购买和上架新设备可能需要数周时间，为了应对这样的峰值，您必须提前做好准备，我们已经看到，即使服务运营商正确预测了峰值规模，容量也会被浪费，如果他们高估了他们所准备的峰值，情况会更糟；
[他们也可能低估峰值，单击查看图 2b](./img/elasticity.png)，然而，意外地拒绝了多余的用户，虽然过度配置的成本很容易衡量，但配置不足的成本更难衡量，但可能同样严重：被拒绝的用户不仅不会产生任何收入，而且可能永远不会回来；[例如，Friendster 相对于竞争对手 Facebook、MySpace 的受欢迎程度下降被认为部分是由于用户对缓慢的响应时间（长达 40 秒）不满意，单击查看图 2c](./img/elasticity.png)：用户将放弃配置不足的服务，直到峰值用户负载等于数据中心的可用容量，此时用户再次获得可接受的服务；另一个简化的例子，假设一个网站的用户分为两类：活跃用户（经常使用该网站的用户）和流失用户（由于性能不佳而放弃该网站的用户）。进一步假设，由于配置不足而获得较差服务的用户中有 10% 永远失去了机会（成为流失用户），即那些本来可以成为常客并获得更好体验的用户。该网站最初配置为处理预期峰值 400,000 个用户（每台服务器 1,000 个用户乘以 400 台服务器），但意外的正面报道在第一个小时内吸引了 500,000 名用户，在 100,000 名被拒之门外或获得不良服务的用户中，根据我们的假设，其中 10,000 人永久流失，剩下 390,000 个活跃用户。下一个小时将看到 250,000 个新的独立用户，前 10,000 名用户表现良好，但该网站仍然超负荷 240,000 个用户。这导致24000名用户流失，留下376000名永久用户。如果这种模式持续下去，经过lg(500000)或19小时后，新增用户数量将接近0，站点将达到稳定状态。显然，服务运营商在这19个小时内获得的稳定收入还不到400000名用户的收入，但这再次证明了利用率不足的论点，更不用说来自不满用户的坏名声了。
这种情况在实践中真的会发生吗？当我们通过Facebook提供服务时，它经历了需求激增，导致服务器数量在3天内从50台增加到3500台。即使每台服务器的平均利用率很低，也没有人能预见到资源需求会在3天内每12小时突然翻一番。峰值消退后，流量会下降到更低的水平。因此，在这个现实世界的例子中，扩展弹性使稳定状态的支出能够更紧密地匹配稳定状态的工作负荷。
### 云计算的 10 大障碍和机遇
[点击查看](./img/top-ten-obstacles-and-opportunities-for-cloud-computing.png)，前 3 个影响采用，接下来的 5 个影响增长，后 2 个是政策和业务障碍，每个障碍都与克服该障碍的机会配对，从产品开发到研究项目。
### The NIST Definition of Cloud Computing
### [NISTdef2011](https://www.nist.gov/publications/nist-definition-cloud-computing)
### NIST -National Institute of Standards and Technology Definition of Cloud Computing
cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources(such as networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal mgmt effort or service provider interaction; this cloud model is composed of 5 essential characteristics, 3 service models, and 4 deployment models; 
essential characteristics: #1 on-demand self-service -a consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with each service provider, #2 broad network access -capabilities are available over the network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms(such as mobile phones, tablets, laptops, and workstations), #3 resource pooling -provider's computing resources are pooled to serve multiple consumers using a multitenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand, there is a sense of location independence in that the customer generally has no control or knowledge over the exact location of the provided resources but may be able to specify location at a higher level of abstraction(such as country, state, or datacenter), examples of resources include storage, processing, mem, and network bdwidth, #4 rapid elasticity -capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward commensurate with demand, to the consumer, the capabilities available for provisioning often appear to be unlimited and can be appropriated in any quantity at any time, #5 measured service -cloud systems automatically control and optimize resource use by leveraging a metering capability(*typically this is done on a pay-as-you-go or charge-per-use basis) at some level of abstraction appropriate to type of service(such as storage, processing, bdwidth, and active user accounts), resources usage can be monitored, controlled, and reported, providing transparency for both provider and consumer of the utilized service; 
service models: #1 SaaS -the capability provided to the consumer it to use provider's application running on a cloud infra(*a cloud infra is collection of hw and sw that enables the 5 essential characteristics of cloud computing, the cloud infra can be viewed as containing both a physical layer and an abstraction layer, the physical layer consists of the hw resources that are necessary to support the cloud services being provided and typically includes server, storage, and network components, the abstraction layer consists of sw deployed across the physical layer which manifests the essential cloud chracteristics, conceptually the abstraction layer sits above the physical layer), the applications are accessible from various client services through either a thin client interface, such as a web browser(such as web-based email), or a program interface, consumer does not manage or control th eunderlying cloud infra including network, servers, os, storage, or even individual application capabilities, with the possible exception of limited user-specific application config settings; #2 PaaS -the capability provided to the consumer is to deploy onto the cloud infra consumer-created or acquired applications created using programming langs, libs, services, and tools supported by the provider(*this capability does not necessarily preclude use of compatible programming langs, libs, services, and tools from other sources), the consumer does not manage or control the underlying cloud infra including network, servers, os, or storage, but has control over the deployed applications and possibly config settings for the application-hosting environment; #3 IaaS -the capability provided to consumer is to provision processing, storage, networks, and other fundamental computing resources where consumer is able to deploy and run arbitrary sw, which can include os and applications, consumer does not manage or control the underlying cloud infra but has control over os, storage, and deployed applications, and possibly limited control of select networking components(such as host firewalls); 
deployment models: #1 private cloud -the cloud infra is provisioned for exclusive use by a single org comprising multiple consumers(such as business units), it may be owned, managed, and operated by the org, a third party, or some combinition of them, and it may exist on or off premises; #2 community cloud -the cloud infra is provisioned for exclusive use by a specific community of consumers from orgs that have shared concerns(such as mission, security requirements, policy, and compliance considerations), it may be owned, managed, and operated by one or more of the orgs in the community, a third party, or some combinition of them, and it may exist on or off premises; #3 public cloud -the cloud infra is provisioned for open use by the general public, it may be owned, managed, and operated by a business, academic, or government org, or some combinition of them, it exists on the premises of the cloud provider; #4 hybrid cloud -the cloud infra is a composition of two or more distinct cloud infras that remain unique entities, but are bound together by standardized or proprietary technology that enables data and application portability(such as cloud bursting for load balancing bt clouds).
### Dynamically Scaling Applications in the Cloud
### [Vaquero11](https://dl.acm.org/doi/pdf/10.1145/1925861.1925869)
### Scalability is said to be one of the major advantages brought by the cloud paradigm and, more specifically, the one that makes it different to an advanced outsourcing solution, however, there are some important pending issues before making the dreamed automated scaling for applications come true.
cloud computing is commonly associated to offering of new mechanisms for infra porvisioning including: the illusion of a virtually infinite computing infra, the employment of advanced billing mechanisms allowing a pay-per-use model on shared multitenant resources, the simplified programming mechanisms(platform); among these features, those introduced by adding scalability and automated on-demand self-service are responsible for making any particular service something more than just an outsourced service with a prettier marketing face; 
as a result of its relevance, the wealth of systems dealing with cloud application scalability is slowing gaining weight in the available literature; automation is typically achieved by using a set of service provider-defined rules that govern how the service scales up or down to adapt to a var load, these rules are themselves composed of a condition, which when met triggers some action on infra or platform, the degree of automation, abstraction for the user(service provider) and customization of rules governing the service vary, some systems offer users chance of building rather simple conditions based on fixed infra or platform matrics(such as cpu, mem, etc.), while others employ server-level metrics(such as cost to benefit ration) and allow for more complex conditions(such as arithmetic and logic combinations of simple rules) to be included in the rules, regarding the subsequent actions launched when conditions are met, available efforts focus on service horizontal scaling(i.e. adding new server replicas and load balancers to distribute load among all available replicas) or vertical scaling(i.e. on-th-fly changing of assigned resources to an already running instance, such as letting more physical cpu to a running vm), unfortunately, the most common os do not support on-the-fly(without rebooting) changes on the available cpu or mem to support this vertical scaling; 
beyond mere server scalability, some other elements need to be taken into account that affect the overall application scaling potential, for example, LBs -load balancers need to support aggregation of new servers(typically, but not necessarily, in the form of new vms) in order to distribute load among several servers, Amazon already provides strategies for load balancing your replicated vms via its elastic load balancing capabilities, hence LBs and the algorithms that distribute load to different servers are essential elements in achieving application scalability in the cloud; 
having several servers and the mechanisms to distribute load among them is a definitive step towards scaling a cloud application, however, there is another element of the datacenter infra to be considered towards complete application scalability, network scalability is an often neglected element that should also be considered; in a consolidated datacenter scenario, several vms share the same network, potentially producing a huge increase in the equired bdwidth(potentially collapsing the network), hence it is necessary to extend infra clouds to other kinds of underlying resources beyond servers, LBs, and storage; cloud applications should be able to request not only virtual servers at multiple points in the network, but also bdwidth-provisioned network pipes and other network resources to interconnect them -NaaS; 
clouds that offer simple virtual hw infra such as vms and networks are usually denoted IaaS, vs., PaaS clouds provide sets of online libs and service for supporting application design, implementation, and maintenance, despite being somewhat less flexible than IaaS clouds, PaaS clouds are becoming important elements for building applications in a faster manner and many important it players scuh as Google, Microsoft, have developed new PaaS clouds systems such as Google AppEngine, Microsoft Azure, we discuss scalability in PaaS clouds at 2 different levels -container level and db level; 
[click for an overview of mechanisms handy to accomplish the goal of the whole application scalability](./img/overview-of-mechanisms-handy-to-accomplish-the-goal-of-the-whole-application-scalability.png).
### Server Scalability
most of the available IaaS clouds deal with single vm mgmt primitives(such as elements for adding/removing vms), lacking mechanisms for treating applications as a whole single entity and dealing with relations among different application components, for example, relationships bt vms are often not considered, ordered deployment of vms contaning sw for different tiers of an appliation is not automated(such as the db's ip is only known at deployment time hence the db needs to be deployed first in order to get its ip and configure the web server connecting to it), etc., application providers typically want to deal with their application only, being released from burden of dealing with (virtual) infra terms; 
**towards increased abstraction and automation :the elasticity controller:** such a fine-grained mgmt(vm-based) may come on handy for few services or domestic users, but it may become intractable with a big number of deployed applications composed of several vms each, the problem gets worse if application providers aim at having their application automatically scaled according to load, they would need to monitor every vm for every application in the cloud and make decisions on whether or not every vm should be scaled, its LB re-configured, its network resources resizes, etc., 2 different approaches are possible: increasing the abstraction level of provided APIs and/or advancing towards a higher automation degree; 
automated scaling features are being included by some vendors, but the rules and policies they allow to express still deal with individual vms only, one cannot easily relate the scaling of vms at tier-1 with its load balancers or the scaling of vms at tier-3; 
on the other hand, more abstract frameworks are proposed(they allow users to deal with applications as a whole rather than per individual vm) that also convey automation, unavoidably, any scalability mgmt system(or [elasticity controller, click to view](./img/elasticity-controller.png)) is bound to the underlying cloud API(the problem of discrete actuators), hence one essential task for any application-level elasticity controller is mapping user scaling policies from the appropriate level of abstraction for the user to actual mechanisms provided by IaaS clouds(depending on specific underlying API, in practice, most of the available cloud APIs expose vm-level mgmt capabilities, so that controller-actuator pairs are limited to adding/removing vms); the implementation of elasticity controller can be done in several different ways with regard to the provided abstraction level: (i)a per-tier controller, so that there is a need for coordination and sync among multiple controller-actuator pairs, treating each tier as an independent actuator with its own control policy can cause shifting of the perf bottleneck bt tiers, a tier can only release resources when an interlock is not being held by the other tiers, (ii)a single controller for the whole application(for all tiers), which let users specify how an application should scale in a global manner, for example, application provider could specify(based on its accurate knowledge of application) to scale the application logic tier whenever #incoming reqs at the web tier is beyond a given threshold; 
**expressing how and when to scale :feeding controller with rules and policies:** all the works above rely on traditional control theory in which several sensors feed a decision making module(elasticity controller) with data to operate on an actuator(cloud API), [click to view](./img/elasticity-controller.png), similarly, all the systems above answer the question on how to automate scalability(i.e. how to implement elasticity controller) in a similar manner either for vm-level or for application-level scalability mgmt systems; 
user-defined rules are the chosen mechanism(as opposed to preconfigured equations sets) to express the policies controlling scalability as shown below: *RULE: if CONDITION(s) then ACTION(s) CONDITION: (1..*) (metric.value MODIFIER) ACTION: (*) IaaS cloud-enabled actions(such as deploy new vm)*, a rule is composed of a series of conditions that when met trigger some action over the underlying cloud, every condition itself is composed of a series of metrics or events that may be present in the system, such as money available, authorization received, etc.(either provided by infra or obtained by application provider itself), which are used together with a modifier(either a comparison against a threshold, the presence of such events) to trigger a set of cloud-provided actions; 
if we focus on application-level scalability(rather than dealing with per vm scaling), a mathematical foumulation in which user just configures threshold for replicating storage servers so as to increase the cloud storage capability is proposed(*called proportional thresholding mechanism, which considers the fact that going from 1 to 2 machines can increase capacity by 100% but going from 100 to 101 machines increases capacity by no more than 1%), a leave full control for users to express their rules and use a rule engine as a controller is also proposed, the OVF -open virtual format is extended to define the application, its components, its contextualization needs, and the rules for scaling, this ways, service providers can generate a description of their application components, the way their application behaves with regard to scalability and the relevant metrics that will trigger action expressed in such rules; 
[as shown in fig1](./img/overview-of-mechanisms-handy-to-accomplish-the-goal-of-the-whole-application-scalability.png), in addition to dynamically adding more vm replicas, application behavior could also include many other aspects determining application scalability: adding load balancers to distribute load among vm replicas is also an important point, in an IaaS cloud, #vms balanced by a single LB can hugely increase, hence overloading the balancer(*although we recognize the importance of load balancing algorithms, the wealth of available literature on this matter places them well beyond of the scope of this work); 
LB scalability requires total time taken to forward each req to the corrsponding server to be negligible for small loads and should grow no faster than O(p)(here p being small loads of balanced vms) when the load is big and #balanced vms is large, however, although mechanisms to scale the load balancing tier would benefit any cloud system, they are missing in most current cloud implementations; 
Amazon already provides its elastic load balancer service aimed at delivering users with a single LB to distribute load among vms, however, Amazon does not provide mechanisms to scale LBs themselves; virtualization of load balancing mechanisms offers chance to define scalability rules by using previously presented systems, recently, a rule of thumb procedure to configure presenration-tier(web server) scalability is proposed :for cpu-intensive web applications, it is better to use a LB to split computation among many instances, for network intensive applications, it may be better to use a cpu-powerful standalone instance to maximize network throughput, yet, for even more network intensive applications, it may be necessary to use dns load balancing to get around a single instance bdwidth limitation, the question emerges whether dns-based load balancing can be scalable enough or not, practical experiences with large well-known services such as Google's search engine and recent experimental works on cloud settings seem to point in that direction, also, for many enterprise applications, hw LB is the most common approach.
### Scaling the Network
properly replicated/sized vms led us to think about LBs as a possible bottleneck, assuming this problem is also resolved takes us to think about the link that keeps application elements stuck together, even across different cloud infra services :the network, which is often overlooked in cloud computing; 
networking over virtualized resources is typically done in 2 different manners: ethernet virtualization and overlay networks and tcp/ip virtualization, these techniques are respectively focused in usage of VLAN -virtual local area network tags (L2) to separate traffic or public key infras to build L2/L3 overlays; 
separating users' traffic is not enough for reaching complete application scalability :the need to scale the very network arises in consolidated datacenters hosting several vms per physical machine, this scalability is often achieved by over-provisioning resources to suit this increased demand; this approach is expensive and induces network instability while infra is being updated, also, it is static and does not take into account that not all applications consume all required bdwidth during all time; improved mechanisms taking into account actual network usage are required; on the other hand, one could periodically measure actual network usage per application and let applications momentarily use other applications' allocated bdwidth; on the other hand, applications could request more bdwidth on demand over the same links, instantiate bdwidth-provisioned network resources together with the vms composing the service across several cloud providers is proposed, similar to the OVF extensions mentioned above, these authors employ NDL -network description lang -based ontologies for expressing required network characteristics; these abstract requirements are mapped to the concrete underlying network peculiarities(such as dynamically provisioned circuits vs. ip overlays), a complete architecture to perform this mapping has also been proposed, unfortunately, there is no known production-ready system that fully accomplishes need for dynamically managing the network in synchrony with vms provisioning; 
these techniques to increase utilization of the network by virtually slicing it have been dubbled as NaaS, this *a la cloud* network provision paradigm can be supported by flow control, distirbuted rate limiting, and network slicing techniques, by applying this mechanism the actual bdwidth can be dynamically allocated to applications on demand, which would benefit from a dynamic resource allocation scheme in which all users pay for the actual bdwidth consumption; to optimize network usage statistical multiplexing is used to compute the final bdwidth allocated to each application, statistical multiplexing helps to allocate more bdwidth to some applications while some others are not using it(most system admins usually provision on a worst-case scenario and never use all requested resources), this way, cloud provider can make a more rational use of its network resources, while still meeting applications' needs.
### Scaling the Platform
IaaS clouds are handy for application providers to control resources used by their systems, however, IaaS clouds demand application developers or system admins to install and configure all sw stack the application components need, in contrast, PaaS clouds offer a ready to use exec environment, along with convenient services, for applications, hence when using PaaS clouds developers can focus on programming their components rather than on setting up environment those components require, but as PaaS clouds can be subject to an extensive usage(many concurrent users calling to hosted applications), PaaS providers must be able to scale exec environment accordingly, in this section, we explore how scalability impacts on the 2 core layers of PaaS platforms: the container and the DBMS, as they are the backbone of any PaaS platform :combination of container+db is the chosen stack to implement many networked(such as internet) applications, which are the ones PaaS platforms are oriented to; here, container is the sw platform where users' components will be deployed and run, different PaaS clouds can be based on different platforms, for example, GAE and its open source counterpart AppEngine provide containers for servlets(part of J2EE specification) and python scripts, while Azure, Aneka offer an environment for .NET applications, each platform type can define different lifecycles, services, and APIs for the components it hosts; db provides data persistence support, the db storage service must address demand for data transactions support combined with big availability and scalability requirements, as it is explained later in this section, this can be addressed in different manners; [click for an overview of possible architecture of a PaaS platform](./img/PaaS.png), where both container and db layer achieve scalability through replication of container and dbms(horizontal scaling), this is the scenario this work focuses on, as it is the only one the authors deem feasible in clouds with certain scalability requirements, vs., applying vertical scaling by using more powerful hw would soon fail, as many clouds will typically face loads that one single machine cannot handle whatever its capacity; other services can be offered by a PaaS platform apart from container and db, which also will need to be scaled to adapt to demand, for example, Azure offers services for inter-component communication(bus) or access control, unfortunately, in this work it would only be possible to study scalability issues of all those services in a too superficial manner, instead, a most thorough analysis of the most important services, the container and the db, has been preferred; 
**container-level scalability:** at container level, a better scalability can be achieved by enabling multitenant containers(having the ability to run components belonging to different users), this imposes strong isolation requirements which maybe not all platforms can achieve by default, for example, the standard java platform is known to carry important security limitations that hinder implementation of safe multitenant environments, a more straightforward option is to run each user's components on non-shared containers, which is the approach taken by GAE; 
in both cases scaling is implemented by instantiating/releasig containers where several replicas of the developer components can be run(i.e. it is a form of horizontal scaling), this should automatically be done by the platform, so developers are not forced to constantly monitor their services' state; either automatic scaling IaaS systems(such as those mentioned in section service availability) can be used, or the platform itself can scale up and down the resources, the latter approach is the one used by both AppEngine, Aneka, AppEngine can run in any Xen based cloud, private(built for example with Eucalyptus) or public(EC2), however, it is not clear from if AppEngine supports transparent cloud federation so that a private PaaS cloud could deploy its containers in vms running in third party owned IaaS clouds to face sudden load peaks, in any case AppScale could apply Eucalyptus ability to be integrated with other clouds, on the other hand, Aneka supports cloud federation natively, so federated Aneka clouds can borrow resources among them, or a given Aneka cloud can use containers hosted in vms running in different IaaS clouds; 
automatic scaling of containers has several implications for developers regarding component design, if the PaaS platform can stop container replicas at any moment(such as due to low load), components could be designed to be as stateless as possible(as GAE recommends for the applications it hosts); in order to ease application dev, support for stateful components should be offered by the platform, in this case with stateful components, LB and container replica mgmt modules must be aware of state, this way, LBs know which container replicas hold session data to forward reqs accordingly, and container instances are not removed as long as they hold some session; 
if more than one container instance holds data from the same session(for better scalability and failure resilience), then some mechanism is necessary that allows to keep the different data replicas updated; transparent session data(such as shopping cart) replication, usually denoted soft state replication can be offered through systems such as Tempest toll or SSM; it is also possible to use distributed cache systems such as memcached for explicit data sharing among component replicas; each solution will have a different impact on component dev, roughly speaking, distributed caches work at application level i.e. they are explicitly accessed by hosted components code to store/retrieve info shared among component replicas, while soft state/session replication systems work in a transparent manner for application developer; 
**db scalability:** PaaS systems must expect very high reqs rates as they can host many applications that demand intense data traffice, 3 mechanisms can be used(and combined) to handle this: distributed caching, nosql dbs, and db clustering, here, (i)caching systems, such as memcached, are used to store intermediate data to speed up frequent data queries, a req for some data item will first check cache to see if the data is present, and will query db only if the data is not in cache(or it has expired), distributed caching systems provide the same cache across several nodes, which is useful in clustered applications, for exmaple, GAE offers [memcache](http://code.google.com/appengine/docs/java/memcache/) as a service for application developers; (ii)nosql refers to a wide family of storage solutions for structured data that are different from traditional relational fully sql-compliant dbs, nosql systems offer high scalability and availability, which seems a good fit in cloud environments with potentially many applications hosted under high demand, on the other hand, the replica mgmt mechanisms they use, provide less guarantees than traditional systems, usually, updates on data copies are not immediately done after each write op, they will be eventually done at some point in the future, this causes 3 systems to be unable to implement support for transparent and fully acid-complaint transactions, hence imposing some limitations on how transactions can be used by developers, besides, the fact that they only support(the equivalent to) a subset of sql can be a hurdle for some applications, such as BigTable used by GAE to provide its object oriented data storage service, HBase an open source implementation of BigTable used by AppEngine; (iii)finally, if fully relational and sql-complaint dbs are to be provided(as in the case of Microsoft's Azure), clusters can be built to provide better scalability, availability, and fault tolerance to typical dbms systems, unfortunately, 3 clusters must be built so several or all nodes contain a replica of each data item, which is known to compromise perf even for moderate loads when transactions are supported, present db replication systems from every major relational dbms have several limitations, and further research is needed to achieve desired perf, the major problem comes from the fact that transactions require protecting data involved while transaction lasts, often making that data unavailable to other transactions, the more transactions running at the same time, the more conflicts will be raised with the corrsponding impact on the application perf; 
yet, some db replication solutions exist that offer some degree of scalability, these can be part of the dbms itself(in-core) or be implemented as a mw layer bt db and application, most of the mw based solutions use a proxy driver used by client applications to access db, this proxy redirects reqs to the replication mw, which forwards them to db, the mw layer handles reqs and transforms then in db ops to ensure that all data copies are updated, also, it takes care of load balancing tasks, exmaples of such solution are C-JDBC, Middle-R, DBFarm; 
it can be concluded from this section that replication of dbs/components is the most important issue to consider when scaling a PaaS platform, [click for main replication ideas presented](./img/replication.png), at container level, the same component can be run on different container instances to achieve better scalability and failure tolerance, but then the platform should make available some mechanism for consistent data replication among components, at db level, copies of application data can be hosted in different dbms instances again for better scalability and failure tolerance, unfortunately, keeping consistency can lead to transaction conflicts; 
[click for summary of most relevant works related to holistic application scaling in cloud environments at the 3 different levels: server, network, and platform level, the most relevant features are highlighted and appropriate references are given](./img/summary-of-most-relevant-works-related-to-holistic-application-scaling-in-cloud-environments-at-the-three-different-levels.png).
****
### Building a Carnegie Mellon Cloud and Openstack
### Virtual Infrastructure Management in Private and Hybrid Clouds
### [Sotomayor2009](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5233608)
### One of the many definitions of cloud is that of an IaaS system, which it infra is deployed in a provider's datacenter as vms, with IaaS clouds' growing popularity, tools and technologies are emerging that can transform an org's existing infra into a private or hybrid cloud; OpenNebula is an open source virtual infra manager that deploys virtualized servers on both a local pool of resources and external IaaS clouds; Haizea is a resource lease manager that can act as a scheduling backend for OpenNebula, providing features not found in other cloud sw or virtualization-based datacenter mgmt sw.
cloud computing is to use a cloud-inspired pun, a nebulously defined term, however, it was arguably first popularized in 2006 by Amazon's [EC2](www.amazon.com/ec2/) -elastic compute cloud, which started offering vms for $0.10 an hr using both a simple web interface and a programmer-friendly API, although not the first to propose a utility computing model, EC2 contributed to popularizing IaaS paradigm, which became closely tied to the notion of cloud computing; an IaaS cloud enables on-demand provisioning of computational resources in the form of vms deployed in a cloud provider's datacenter(such as Amazon's), minimizing or even eliminating associated capital costs for cloud consumers and letting those consumers add or remove capacity from their it infra to meet peak or fluctuating service demands while paying only for the actual capacity load; 
overtime, an ecosystem of providers, users, and technologies has coalesced around this IaaS cloud model, more IaaS cloud providers such as GoGrid, FlexiScale, ElasticHosts, etc., have emerged, and a strong number of companies base their it strategy on cloud-based resources, spending little or no capital to manage their own it infra([click for more examples](https://aws.amazon.com/solutions/case-studies/)); some providers such as Elastra, RightScale, focus on deploying and managing services on top of IaaS clouds, including web and db servers that benefit from such clouds' elastic capacity, and let their clients provision services directly instead of having no provision and set up infra themselves; other providers offer products that facilitate working with IaaS clouds, such as [rPath's rBuilder](www.rpath.org), which enables users to dynamically create sw environments to run on a cloud; 
although this ecosystem has evolved around public clouds -commercial cloud providers that offer a publicly accessible remote interface for creating and managing vm instances within their proprietary infra -internet is growing in open source cloud computing tools that let orgs build their own IaaS clouds using their internal infras, these private cloud deployments' primary aim is not to sell capacity over internet through publicly accessible interfaces but to give local users a flexible and agile private infra to run service wkloads within their administrative domains, private clouds can also support a hybrid cloud model by supplementing local infra with computing capacity from an external public cloud, private and hybrid clouds are not exclusive with being public clouds, a priavte or hybrid cloud can allow remote access to its resources over internet using remote interfaces, such as web services that Amazon EC2 uses, here, we look at 2 open source projects that facilitate mgmt of such private or hybrid cloud models.
### Virtual Infra Mgmt
to provide users with the same features found in commercial public clouds, private or hybrid cloud sw must: (i)provide a uniform and homogeneous view of virtualized resources, regardless of underlying virtualization platform(such as Xen, KVM -kernel-based vm, VMware, etc.), (ii)manage a vm's full life cycle, including setting up networks dynamically for groups of vms and managing their storage requirements, such as vm disk image deployment or on-the-fly sw environment creation, (iii)support configurable resource allocation policies to meet org's specific goals(high availability, server consolidation to minimize power usage, etc.), (iv)adapt to org's changing resource needs, including peaks in which local resource are insufficient, and changing resources including addition or failure of physical resources; 
hence a key component in private or hybrid clouds will be VI -virtual infra mgmt, the dynamic orchestration of vms that meets the requirements we have just outlined, here, we discuss vi mgmt's relevance not just for creating private or hybrid clouds but also within the emerging cloud ecosystem; our 2 open source projects, [OpenNebula](http://www.opennebula.org/), [Haizea](http://haizea.cs.uchicago.edu/), are complementary and can be used to manag vis in private or hybrid clouds, here, OpenNebula is a vi manager that orgs can use to deploy and manage vms, either individually or in groups that must be coscheduled on local resources or external public clouds, it automates vm setup(preparing disk images, setting up networking, etc.) regardless of underlying virtualization layer(Xen, KVM, VMware) or external cloud(EC2, ElasticHosts), Haizea is a resource lease manager that can act as a scheduling backend for OpenNebula, providing leasing capabilities not found in other cloud systems, such as ARs -advance researvations, and resource preemption, which are particularly relevant for private clouds.
### The Cloud Ecosystem
vi mgmt tools for datacenters have been around since before cloud computing became industry's new buzzword, several of these, such as the [Platform VM Orchestrator](https://www.platform.com/Products/platform-vm-orchestrator), [VMware vSphere](http://www.vmware.com/products/vsphere/), [Ovirt](http://ovirt.org/), meet many of the vi mgmt requirements we outlined earlier, providing features such as dynamic placement and vm mgmt on a pool of physical resources, automatic load balancing, server consolidation, and dynamic infra resizing and partitioning; although creating what we now call a private cloud was already possible with existing tools, these tools lack other features that are relevant for building IaaS clouds, such as public cloud-like interfaces, mechanisms for adding such interfaces easily, and the ability to deploy vms on external clouds; 
on the other hand, projects such as [Globus Nimbus](http://workspace.globus.org), [Eucalyptus](www.eucalyptus.com), which we term cloud toolkits, can help transform existing infra into an IaaS cloud with cloud-like interfaces, here, Eucalyptus is compatible with Amazon EC2 interface and is designed to support additional client-side interfaces, Globus Nimbus exposes EC2 and WSRF -web services resource framework interfaces and offers self-configuring virtual cluster support, however, although these tools are fully functional with respect to providing cloud-like interfaces and higher-level functionality for security, contextualization, and vm disk image mgmt, their vi mgmt capabilities are limited and lack features of solutions that specialize in vi mgmt; hence [an ecosystem of cloud tools is starting to form, click to view](./img/ecosystem-of-cloud-tools.png), in which cloud toolkits attempt to span both cloud mgmt and vi mgmt but, by focusing on the former, do not deliver the same functionality as sw written specifically for vi mgmt; although integrating cloud mgmt solutions with existing vi managers would seem like obvious solution, this is complicated by lack of open and standard interfaces bt the 2 layers, and lack of certain key features in existing vi managers, hence our aim is to produce a vi mgmt solution with a flexible and open architecture that orgs can employ to build private or hybrid clouds; with this goal in mind, we started developing OpenNebula and continue to enhance it as part of [EU's Reservoir porject](http://www.reservoir-fp7.eu/), which aims to develop open source technologies to enable deployment and mgmt of complex it services across different administrative domains; OpenNebula provides similar functionality to that found in existing vi managers but also aims to over those solutions' shortcomings, namely: (i)the inability to scale to external clouds, (ii)monolithic and closed architectures that are hard to extend or interface with other sw, not allowing seamless integration with existing storage and network mgmt solutions deployed in datacenters, (iii)a limited choice of preconfigured placement policies(first fit, round robin, etc.), (iv)a lack of support for scheduling, deploying, and configuring groups of vms(such as a group of vms representing a cluster where all the nodes either deploy entirely or do not deploy at all and where some vms' config depends on others' such as the head-worker replationship in compute clusters), [click for a more detailed comparison bt OpenNebula and several well-known vi managers, including cloud toolkits that perform vi mgmt](./img/detailed-comparison-between-OpenNebula-and-several-well-known-virtual-infrastructure-managers.png); 
a key feature of OpenNebula's architecture, which we describe more in the next section, is its highly modular design, which facilitates integration with any virtualization platform and third-party component in the cloud ecosystem, such as cloud toolkits, virtual image managers, service managers, and vm schedulers, for example, it specifies all actions pretaining to setting up a vm disk image(transferring the image, installing sw on it, etc.) in terms of well-defined hooks, although OpenNebula includes a default transfer manager that uses these hooks, it can also leverage contextualizers just by writing code that interfaces bt hooks and thir-party sw; 
Haizea which we developed independently from OpenNebula, was the first to leverage such an architecture in a way that was beneficial to both projects, Haizea originally cloud simulate vm scheduling only for research purposes, but we modified it to act as a drop-in replacement for OpenNebula's default scheduler, with few changes required in Haizea code and none in the OpenNebula code, by working together, OpenNebula could offer resource leases as a fundamental provisioning abstraction, and Haizea could operate with real hw through OpenNebula; 
in fact, integraing OpenNebula and Haizea provides the only vi mgmt solution offering advance reservation of capacity, [as tabel1 shows](./img/detailed-comparison-between-OpenNebula-and-several-well-known-virtual-infrastructure-managers.png), other vi managers use immediate provisioning, or best-effort provisioning, which we discuss in more detail later, however, private clouds -specifically those with limited resources in which not all reqs are satisfiable immediately owing to lack of resources -stand to benefit from more sophisticated vm placement strategies supporting queues, priorities, and ars; additionally, service provisioning clouds, such as the one being developed in Reservoir project, have requirements that are insupportable with only an immediate provisioning model -for example, they need capacity reservations at specific times to meet SLAs -service-level agreements, or peak capacity requirements.
### The OpenNebula Architecture
[click to view](.//img/opennebula.png), encompasses several components specialized in different aspects of vi mgmt; to control a vm's life cycle, OpenNebula core orchestrates 3 different mgmt areas including: image and storage technologies(i.e. virtual appliance tools or distributed file systems) for preparing disk images for vms, network fabric(such as DHCP -dynamic host config protocol servers, firewalls, or switches) for providing vms with a virtual network environment, and underlying hypervisors for creating and controlling vms; the core performs specific storage, network, or virtualization ops through pluggable drivers, hence OpenNebula is not tied to any specific environment, providing a uniform mgmt layer regardless of underlying infra; 
besides managing individual vm's life cycle, we also designed the core to support services deployment, such services typically include a set of interrelated components(such as a web server and db backend) requiring several vms, hence we can treat a group of related vms as a first-class entity in OpenNebula; besides managing vms as a unit, the core also handles delivery of context info(such as web server's ip addr, digital certificates, and sw licenses) to the vms; 
a separate scheduler component makes vm placement decisions, more specifically, the scheduler has access to info on all reqs OpenNebula receives and, based on these reqs, keeps track of current and future allocations, creating and updating a resource schedule and sending appropriate deployment commands to OpenNebula core; OpenNebula default scheduler provides a rank scheduling policy that places vms on physical resources according to a ranking algorithm that the administrator can configure, it relies on realtime data from both the running vms and available physical resources; 
OpenNebula offers mgmt interfaces to integrate core's functionality within other datacenter mgmt tools, such as accounting or monitoring frameworks, to this end, OpenNebula implements the [libvirt API](http://libvirt.org/), an open interface for vm mgmt, as well as a CLI, adiitionally, a subset of this functionality is exposed to external users through a cloud interface; 
finally, OpenNebula can support a hybrid cloud model by using cloud drivers to interface with external clouds, this lets orgs supplement local infra with computing capacity from a public cloud to meet peak demands, better serve their access reqs(such as by moving the service closer to user), or implement high availablity strategies; OpenNebula currently includes an EC2 driver, which can submit reqs to EC2, Eucalyptus, as well as an ElasticHosts driver.
### Haizea Lease Manager
Haizea is an open source lease manager and can act as a vm scheduler for OpenNebula or be used on its own as a simulator to evaluate different scheduling strategies' perf over time; the fundamental resource provisioning abstraction in Haizea is the lease, intuitively, a lease is a form of contract in which one party agrees to provide a set of resources to another, when a user wants to request computational resources from Haizea, it does so in the form of a lease, leases are then implemented as vms managed by OpenNebula, the lease terms Haizea supports include hw resources, sw environments, and the period during which hw and sw resources must be avialable; currently, Haizea supports ar leases which resources must be available at a specific time; best-effort leases, in which resources are provisioned as soon as possible, and reqs are placed in a queue, if necessary; immediate leases, in which resources are provisioned when requested or not at all; 
to satisfy use cases where resources must be guaranteed to be available at certain times, various researchers have studied advance reservation of computational resources in the context of parallel computing; in the absence of suspension/resumption capabilities, this approach produces resource underutilization due to need to vacate resources before an ar starts; by using vms to implement leases, an org can support ars more efficiently through resource preemption, suspending vms of lower-priority leases before a reservation starts, resuming them after reservation ends, and potentially migrating them to other available nodes or even other clouds, we can do this without having to make the applications inside the vm aware that they are going to be suspended, resumed, or even migrated, however, using vms introduces runtime overhead, which poses additional scheduling challenges, as does the preparation overhead of deploying the vm disk images that the lease needs, such overheads can noticeably affect perf if they are not adequately managed; Haizea's approach is to separately schedule preparation overhead rather than assuming it should just be deducted from a user's allocation, however, this is complicated when Haizea must support multiple lease types with conflicting requirements that it has to reconcile -for example, transfers for a lease starting at 2pm could require delaying transfers for best-effort leases, resulting in longer wait times, Haizea uses several optimizations, such as reusing disk images across leases, to minimize preparation overhead's impact, similarly, Haizea also schedules runtime overhead of suspending, resuming, and migrating vms; 
Haizea bases its scheduling on a resource slot table that represents all physical nodes it manages over time, it schedules best-effort leases using a first-come-first-serve queue with backfilling(a common optimization in queue-based systems), whereas ar leases use a greedy algorithm to select physical resources that minimize #preemptions, although the resource selection algorithm is currently hardcoded, future versions will include a policy decision module to let developers specify their own resource selection policies(such as policies to prioritize leases based on user, group, project, etc.), this policy decision module will also specify the conditions under which Haizea should accept or reject a lease.
****
### Encapsulating Computation (Intel Labs, guest)
### Xen and the Art of Virtualization
### [Barham03](https://dl.acm.org/doi/pdf/10.1145/1165389.945462)
### [Project Page](http://www.cl.cam.ac.uk/netos/xen)
numerous systems have been designed which use virtualization to subdivide ample resources of a modern computer, some require specialized hw, or cannot support commodity os, some target 100% binary compatibility at the expense of perf, others sacrifice security of functionality for speed, few offer resource isolation or perf guarantees, most provide only best-effort provisioning, risking denial of service; 
we present Xen, an x86 vm monitor which allows multiple commodity os to share conventional hw in a safe and resource managed fashion, but without sacrificing their perf or functionality, this is achieved by providing an idealized vm abstraction to which os such as linux, bsd, windows xp, can be ported with minimal effort; 
our design is targeted at hosting up to 100 vm instances simultaneously on a modern server, the virtualization approach taken by Xen is extremely efficient -we allow os such as linux, windows xp, to be hosted simultaneously for a negligible perf overhead -at most a few percent compared with the unvirtualized case, we considerably outperform competing commercial and freely available solutions in a range of microbenchmarks and system-wide tests. 
modern computers are sufficiently powerful to use virtualization to present illusion of many smaller vms, each running a separate os instance, this has lead to a resurgence of interest in vm technology, here we present Xen, a high perf resource-managed VMM -vm monitor which enables applications such as server consolidation, co-located hosting facilities, distributed web services, secure computing platforms, and application mobility; 
successful partitioning of a machine to support concurrent exec of multiple os poses several challenges including: (i)vms must be isolated from one another -it is not acceptable for exec of one to adversely affect perf of another, this is particularly true when vms are owned by mutually untrusting users, (ii)it is necessary to support a variety of different os to accomodate the heterogeneity of popular applications, (iii)the perf overhead introduced by virtualization should be small; 
Xen hosts commodity os, albeit with some source modifications, the prototype described and evaluated here can support multiple concurrent instances of our XenoLinux guest os, each instance exports an application binary interface identical to a non-virtualized linux2.4, our port of windows xp to Xen is not yet complete but is capable of running simple user-space processes, work is also progressing in porting NetBSD; 
Xen enables users to dynamically instantiate an os to execute whether they desire, in the XenoServer project we are deploying Xen on standard server hw at economically strategic locations within ISPs or at internet exchanges, we perform admission control when starting new vms and expect each vm to pay in some fashion for resources it requires, we discuss our ideas and approach in this direction elsewhere, here we focus on vmm; 
there are a number of ways to build a system to host multiple applications and servers on a shared machine, perhaps the simplest is to deploy one or more hosts running a standard os such as linux, windows, and then to allow users to install files and start processes -protection bt applications being provided by conventional os techniques, experience shows that system administration can quickly become a time-consuming task due to complex config interactions bt supposedly disjoint applications; 
more importantly, such systems do not adequately support perf isolation; the scheduling policy, mem demand, network traffic, and disk accesses of one process impact perf of others, this may be acceptable when there is adequate provisioning and a closed user group(such as in the case of computational grids, or the experimental PlanetLab platform), but not when resources are oversubscribed, or users uncooperative; one way to address this problem is to retrofit support for perf isolation to os, this has been demonstrated to a greater or lesser degree with resource containers, linux/rk, qlinux, and silk, one difficulty with such approaches is ensuring that all resource usage is accounted to the correct process -consider, for example, the complext interactions bt applications due to buffer cache or page replacement algorithms, this is effectively the problem of QoS crosstalk within the os, performing multiplexing at a low level can mitigate this problem, as demonstrated by the Exokernel and Nemesis os, unintentional or undesired interactions bt tasks are minimized; 
we use this same basic approach to build Xen, which multiplexes physical resources at the granularity of an entire os and is able to provide perf isolation bt them, in contrast to process-levlel multiplexing this also allows a range of guest os to gracefully coexist rather than mandating a specific application binary interface, there is a price to pay for this flexibility -running a full os is more heavyweight than running a process, both in terms of initialization(such as booting or resuming versus fork and exec), and in terms of resource consumption; 
for our target of up to 100 hosted os instances, we believe this price is worth paying -it allows individual users to run unmodified binaries, or collections of binaries, in a resource controlled fashion(such as an Apache server along with a PostgreSQL backend), furthermore it provides an extremely high level of flexibility since the user can dynamically create the precise exec environment their sw requires, unfortunate config interactions bt various services and applications are avoided(such as each windows instance maintains its own strategy).
### Xen :Approach and Overview
in a traditional vmm the virtual hw exposed is functionally identical to underlying machine, although full virtualization has obvious benefits of allowing unmodified os to be hosted, it also has a number of drawbacks, this is particularly true for the prevalent IA-32, x86, architecture; 
support for full virtualization was never part of the x86 architectural design, certain supervisor instructions must be handled by vmm for correct virtualization, but executing these with insufficient privilege fails silently rather than causing a convenient trap, efficiently virtualizing the x86 mmu is also difficult, these problems can be solved but only at the cost of increased complexity and reduced perf; VMware's ESX Server dynamically rewrites portions of hosted machine code to insert traps wherever vmm intervention might be required, this translation is applied to the entire guest os kernel(with associated translation, exec, and caching costs) since all non-trapping priviledged instructions must be caught and handled, ESX Server implements shadow versions of system structures such as page tables and maintains consistency with the virtual tables by trapping every update attempt -this approach has a high cost for update-intensive ops such as creating a new application process; 
notwithstanding the intricacies of x86, there are other arguments against full virtualization, in particular, there are situations in which it is desirable for hosted os to see real as well as virtual resources -providing both real and virtual time allows a guest os to better support time-sensitive tasks, and to correctly handle tcp timeouts and rtt estimates, while exposing real machine addr allows a guest os to improve perf by using superpages or page coloring; 
we avoid drawbacks of full virtualization by presenting a vm abstraction that is similar but not identical to the underlying hw -an approach which has been dubbed paravirtualization, this promises improved perf, although it does require modifications to the guest os, however, we do not require changes to the ABI -application binary interface, and hence no modifications are required to guest applications, we distill the discussion so far into a set of design principles including: (i)suport for unmodified application binaries is essential, or users will not transition to Xen, hence we must virtualize all architectural features required by existing standard ABIs, (ii)supporting full multiapplication os is important, as this allows complex server configs to be virtualized within a single guest os instance, (iii)paravirtualization is necessary to obtain high perf and strong resource isolation on uncooperative machine architestures such as x86, (iv)even on cooperative machine structures, completely hiding effects of resource virtualization from guest os risks both correctness and perf; 
note that our paravirtualized x86 abstraction is quite different from that proposed by the recent Denali project, Denali is designed to support thousands of vms running network services, the vast majority of which are small-scale and unpopular, in contrast, Xen is intended to scale to approximately 100 vms running industry standard applications and services, given these very different goals, it is instructive to contrast Denali's design choices with our own principles including: (i)Denali does not target existing ABIs, and so can elide certain architectural features from their vm interface, for example, Denali does not fully support x86 segmentation although it is exported(and widely used, such as segments are frequently used by thread libs to address thread-local data) in the ABIs of NetBSD, linux, windows xp; (ii)Denali implementation does not address the problem of supporting application multiplexing, nor multiple addr spaces, within a single guest os, rather, applications are linked explicitly against an instance of the Ilwaco guest os in a manner rather reminiscent of a libOS in the Exokernel, hence each vm essentially hosts a single-user single-application unprotected os, by contrast, in Xen a single vm hosts a real os which may itself securely multiplex thousands of unmodified user-level processes, although a prototype virtual mmu has been developed which may help Denali in this area, we are unaware of any published technical details or evaluation; (iii)in the Denali archiecture the vmm performs all paging to and from disk, this is perhaps related to lack of mem-mgmt support at virtualization layer, paging within vmm is contrary to our goal of perf isolation -malicious vms can encourage thrashing behaviour, unfairly depriving others of cpu time and disk bdwidth, in Xen we expect each guest os to perform its own paging using its own guaranteed mem reservation and disk allocation(an idea previously exploited by self-paging); (iv) Denali virtualizes the namespaces of all machine resources, taking the view that no vm can access the resource allocations of another vm if it cannot name them(such as vms have no knowledge of hw addrs but only the virtual addrs created for them by Denali), in contrast, we believe that secure access control within the hypervisor is sufficient to ensure protection -furthermore, as discussed previously, there are strong correctness and perf arguments for making physical resources directly visible to guest os; in the following section we describe vm abstraction exported by Xen and discuss how a guest os must be modified to conform to this, note that here we reserve the term guest os to refer to one of the os that Xen can host and we use the term domain to refer to a running vm within which a guest os executes, the distinction is analogous to that bt a program and a process in a conventional system, we call Xen itself the hypervisor since it operates at a higher privilege level than the supervisor code of the guest os that it hosts; 
**vm interface:** [click for an overview of the paravirtualized x86 interface](./img/paravirtualized-x86-interface.png). factored into 3 broad aspects of the system: mem mgmt, cpu, device io, in the following we address each machine subsystem in turn, and discuss how each is presented in our paravirtualized architecture, note that although certain parts of our implementation, such as mem mgmt, are specific to x86, many aspects(such as our virtual cpu and io devices) can be readily applied to other machine architectures, furthermore, x86 represents a worst case in the areas where it differs significantly from risc-style processors, such as efficiently virtualizing hw page tables is more difficult than virtualizing a sw-managed tlb; 
**mem mgmt:** virtualizing mem is undoubtedly the most difficult part of paravirtualizing an architecture, both in terms of mechanisms required in the hypervisor and modifications required to port each guest os, the task is easier if architecture provides a sw-managed tlb as these can be efficiently virtualized in a simple manner, a tagged tlb is another useful feature supported by most server-class risc architectures including: Alpha, MIPS, SPARC, associating an addr-space identifier tag with each tlb entry allows the hypervisor and each guest os to efficiently coexist in separate addr spaces because there is no need to flush the entire tlb when transferring exec; unfortunately, x86 does not have a sw-managed tlb, instead, tlb misses are serviced automatically by the processor by walking the page table structure in hw, hence to achieve the best possible perf, all valid page translators for the current addr space should be present in the hw-accessible page table, moreover, because tlb is not targeted, addr space switches typically require a complete tlb flush, given these limitations, we made 2 decisions: (i)guest os are responsible for allocating and managing the hw page tables, with minimal involvement from Xen to ensure safety and isolation, (ii)Xen exists in a 64MB section at the top of every addr space, hence avoiding a tlb flush when entering and leaving the hypervisor; each time a guest os requires a new page table, perhaps because a new process is being created, it allocates and initializes a page from its own mem reservation and registers it with Xen, at this point os must relinquish direct write privileges to the page-table mem, all subsequent updates must be validated by Xen, this restrictes updates in a number of ways, including only allowing an os to map pages that it owns and disallowing writable mappings of page tables; guest os may batch update reqs to amortize overhead of entering the hypervisor; the top 64MB region of each addr space, which is reserved for Xen, is not accessible or remappable by guest os, however, this addr region is not used by any of the common x86 ABIs, hence this restriction does not break application compatibility; segmentation is virtualized in a similar way, by validating updates to hw segment descriptor tables, the only restrictions on x86 segment descriptors are: (i)they must have lower privilege than Xen, (ii)they may not allow any access to Xen-reserved portion of the addr space; 
**cpu:** virtualizing cpu has several implications for guest os, principally, insertion of a hypervisor below os violates usual assumption that os is the most privileged entity in the system, in order to protect the hypervisor from os misbehavior(and domains from one another), guest os must be modified to run at a lower privilege level; many processor archiectures only provide 2 privilege levels, in these cases guest os would share the lower privilege level with applications, guest os would then protect itself by running in a separate addr space from its applications, and indirectly pass control to and from applications via hypervisor to set the virtual privilege level and change current addr space, again, if processor's tlb supports addr-space tags then expensive tlb flushes can be avoided; efficient virtualization of privilege levels is possible on x86 because it supports 4 distinct privilege levels in hw, the x86 privilege levels are generally described as rings, and are numbered from 0(most privileged) to 3(least privileged), os code typically executes in ring0 because no other ring can execute privileged instructions, while ring3 is generally used for application code, to our knowledge, rings1&2 have not been used by any well-known x86 os since os/2, any os which follows this common arrangement can be ported to Xen by modifying it to execute in ring1, this prevents guest os from directly executing privileged instructions, yet it remains safely isolated from applications running in ring3; privileged instructions are paravirtualized by requiring them to be validated and executed within Xen -this applies to ops such as installing a new page table, or yielding the processor when idle(rather than attempting to hlt it), any guest os attempt to directly execute a privileged instruction is failed by the processor, either silently or by taking a fault, since only Xen executes at a sufficiently privileged level; exceptions, including mem faults and sw traps, are virtualized on x86 very straightforwardly, a table describing the handler for each type of exception is registered with Xen for validation, the handlers specified in this table are generally identical to those for real x86 hw, this is possible because the exception stack frames are unmodified in our paravirtualized architecture, the sole modifications is to the page fault handler, which would normally read the faulting addr from a privileged processor register(CR2), since this is not possible, we write it into an extended stack frame(*in hindsight, writing the value into a pre-agreed shared mem location rather than modifying the stack frame would have simplified the xp port), when an exception occurs while executing outside ring0, Xen's handler creates a copy of the exception stack frame on guest os stack and returns control to the appropriate registered handler; typically only 2 types of exception occur frequently enough to affect system perf -system calls(which are usually implemented via a sw exception) and page faults, we improve the perf of system calls by allowing each guest os to register a fast exception handler which is accessed directly by the processor without indirecting via ring0, this handler is validated before installing it in the hw exception table, unfortunately it is not possible to apply the same technique to the page fault handler because only code executing in ring0 can read the faulting addr from reg CR2, page faults must therefore always be delivered via Xen so that this reg value can be saved for access in ring1; safely is ensured by validating exception handlers when they are presented to Xen, the only required check is that the handler's code segment does not specify exec in ring0, since no guest os can create such a segment it suffices to compare the specified segment selector to a small number of static values which are reserved by Xen, apart from this, any other handler problems are fixed up during exception propagation -for example, if the handler's code segment is not present or if the handler is not paged into mem then an appropriate fault will be taken when Xen executes the iret instruction which returns to the handler, Xen detects these double faults by checking the faulting program counter value -if addr resides within the exception-virtualizing code then the offending guest os is terminated; note that this lazy checking is safe even for the direct system-call handler -access faults will occur when cpu attempts to directly jump to guest os handler, in this case, the faulting addr will be outside Xen(since Xen will never execute a guest os system call) and so the fault is virtualized in the normal way, if propagation of the fault causes a further double fault then guest os is terminated as described above; 
**device io:** rather than emulating existing hw devices, as is typically done in fully virtualized environments, Xen exposes a set of clean and simple device abstractions, this allows us to design an interface that is both efficient and satisfies our requirements for protection and isolation, to this end, io data is transferred to and from each domain via Xen, using shared-mem, asynchronous buffer-descriptor rings, these provide a high-perf comm mechanism for passing buffer info vertically through the system, while allowing Xen to efficiently perform validation checks(such as checking that buffers are contained within a domain's mem reservation); similar to hw interrupts, Xen supports a lightweight event-delivery mechanism which is used for sending asynchronous notifications to a domain, these notifications are made by updating a bitmap of pending event types and, optionally, by calling an event handler specified of guest os(such as to avoid extra costs incurred by frequent wake-up notifications).
### The Cost of Porting an OS to Xen
[click for cost, in lines of code, of porting commodity os to Xen's paravirtualized x86 environment](./img/cost-of-porting-commodity-operating-systems-to-Xen-paravirtualized-x86-environment.png), note that our NetBSD port is at a very early stage, and hence we report no figures here, the xp port is more advanced, but still in progress, it can execute a number of user-space applications from a ram disk, but it currently lacks any virtual io drivers, for this reason, figures for xp's virtual device drivers are not presented, however, as with linux, we expect these drivers to be small and simple due to the idealized hw abstraction presented by Xen; 
windows xp required a surprising number of modifications to its architecture independent os code because it uses a variety of structures and unions for accessing PTEs -page-table entries, each page-table access had to be separately modified, although some of this process was automated with scripts, in contrast, linux needed far fewer modifications to its generic mem system as it uses pre-processor macros to access PTEs -the macro definitions provide a convenient place to add translation and hypervisor calls required by paravirtualization; in both os, the architecture-specific sections are effectively a port of x86 code to our paravirtualized architecture, this involved rewriting routines which used privileged instructions, and removing a large amount of low-level system initialization code, again, more changes were required in windows xp, mainly due to presence of legacy 16-bit emulation code and need for a somewhat different boot-loading mechanism, note that the x86-specific code base in xp is substantially larger than in linux and hence a larger porting effort should be expected.
### Control and Mgmt
throughout the design and implementation of Xen, a goal has been to separate policy from mechanism wherever possible, although the hypervisor must be involved in data-path aspects(such as scheduling cpu bt domains, filtering network packets before transmission, or enforcing access control when reading data blocks), there is no need for it to be involved in, or even aware of, higher level issues such as how cpu is be to shared, or which kinds of packet each domain may transmit; 
the resulting architecture is one in which the hypervisor itself provides only basic control ops, these are exported through an interface accessible from authorized domains, potentially complex policy decisions such as admission control are best performed by mgmt sw running over a guest os rather than in privileged hypervisor code; 
[click for the overall system structure](./img/overall-system-structure-of-a-machine-running-the-xen-hypervisor.png), note that a domain is created at boot time which is permitted to use the control interface, the initial domain, termed Domain0, is responsible for hosting the application-level mgmt sw, the control interface provides the ability to create and terminate other domains and to control their associated scheduling params, physical mem allocations, and the access they are given to the machine's physical disks and network devices; 
in addition to processor and mem resources, the control interface supports creation and delection of VIFs -virtual network interfaces and VBDs -virtual block devices, these virtual io devices have associated access-control info which determines which domains can access them, and with what restrictions(such as a read-only vbd may be created, or a vif may filter ip packets to prevent source-addr spoofing); 
this control interface, together with profiling stats on current state of the system, is exported to a suite of application-level mgmt sw running in Domain0, this complement of administrative tools allow convenient mgmt of the entire server -current tools can create and destroy domains, set network filters and routing rules, monitor per-domain network activity at packet and flow granularity, and create and delete virtual network interfaces and virtual block devices, we anticipate dev of higher-level tools to further automate the application of administrative policy.
### Detailed Design
in this section we introduce design of the major subsystems that make up a Xen-based server, in each case we present both Xen and guest os functionality for clarity of exposition, the current discussion of guest os focuses on XenoLinux as this is the most mature, nonetheless our ongoing porting of windows xp and netbsd gives us confidence that Xen is guest os agnostic.
### Control Transfer :Hypercalls and Events
2 mechanisms exist for control interactions bt Xen and an overlying domain :synchronous calls from a domain to Xen may be made using a hypercall, while notifications are delivered to domains from Xen using an asynchronous event mechanism; 
the hypercall interface allows domains to perform a synchronous sw trap into hypervisor to perform a privileged op, analogous to use of system calls in conventional os, an example use of a hypercall is to request a set of page-table updates, in which Xen validates and applies a list of updates, returning control to the calling domain when this is completed; 
comm from Xen to a domain is provided through an asynchronous event mechanism, which replaces usual delivery mechanisms for device interrupts and allows lightweight notification of important events such as domain-termination reqs, akin to traditionl unix signals, there are only a small number of events, each acting to flag a particular type of occurence, for example, events are used to indicate that new data has been received over the network, or that a virtual disk req has completed; 
pending events are stored in a per-domain bitmask which is updated by Xen before invoking an event-callback handler specified by guest os, the callback handler is responsible for resetting the set of pending events, and responding to the notifications in an appropriate manner, a domain may explicitly defer event handling by setting a Xen-readable sw flag :this is analogous to disabling interrupts on a real processor.
### Data Transfer :I/O Rings
the presence of a hypervisor means there is an additional protection domain bt guest os and io devices, so it is crucial that a data transfer mechanism be provided that allows data to move vertically through the system with as little overhead as possible; 
2 main factors have shaped the design of our io-transfer mechanism :resource mgmt and event notification, for resource accountability, we attempt to minimize the work required to demultiplex data to a specific domain when an interrupt is received from a device -the overhead of managing buffers is carried out later where computation may be accounted to the appropriate domain, similarly, mem committed to device io is provided by relevant domains wherever possible to prevent crosstalk inherent in shared buffer pools, io buffers are protected during data transfer by pinning underlying page frames within Xen; 
[click for structure of our io descriptor rings](./img/structure-of-asynchronous-io-rings.png), here a ring is a circular queue of descriptors allocated by a domain but accessible from within Xen, descriptors do not directly contain io data, instead, io data buffers are allocated out-of-band by guest os and indirectly referenced by io descriptors, access to each ring is based around 2 pairs of producer-consumer pointers :domains place reqs on a ring, advancing a req producer pointer, and Xen removes these reqs for handling, advancing an associated req consumer pointer, responses are placed back on the ring similarly, save with Xen as the producer and guest os as the consumer, there is no requirement that reqs be processed in order :guest os associates a unique identifier with each req which is reproduced in the associated response, this allows Xen to unambiguously reorder io ops due to scheduling or priority considerations; 
this structure is sufficiently generic to support a number of different device paradigms, for example, a set of reqs can provide buffers for network packet reception, subsequent responses then signal arrival of packets into these buffers, reordering is useful when dealing with disk reqs as it allows them to be scheduled within Xen for efficiency, and use of descriptors with out-of-band buffers makes implementing zero-copy transfer easy; 
we decouple production of reqs or responses from the notification of the other party -in the case of reqs, a domain may enqueue multiple entries before invoking a hypercall to alert Xen, in the case of responses, a domain can defer delivery of a notificatioon event by specifying a threshold number of responses, this allows each domain to trade-off latency and throughput requirements, similarly to the flow-aware interrupt dispatch in the ArseNIC Gigabit Ethernet interface.
### Building a New Domain
the task of building the initial guest os structures for a new domain is mostly delegated to Domain0 which uses its privileged control interfaces(see section Control and Mgmt) to access the new domain's mem and inform Xen of initial reg state, this approach has a number of advantages compared with building a domain entirely within Xen, including reduced hypervisor complexity and improved rebustness(accesses to privileged interface are sanity checked which allowed us to catch many bugs during initial deployment); however, most important is the case with which the building process can be extended and specialized to cope with new guest os, for example, the boot-time addr space assumed by the linux kernel is considerably simpler than that expected by windows xp, it would be possible to specify a fixed initial mem layout for all guest os, but this would require additional bootstrap code within every guest os to lay things out as required by the rest of os, unfortunately this type of code is tricky to implement correctly, for simplicity and robustness it is therefore better to implement it within Domain0 which can provide much richer diagnostics and debugging support than a bootstrap environment.
****
### Programming Models and Frameworks
### MapReuce :Simplified Data Processing on Large Clusters
### [Dean2004](https://www.usenix.org/legacy/events/osdi04/tech/full_papers/dean/dean.pdf)
MapReduce is a programming model and an associated implementation for processing and generating large datasets, users specify a map function that processes a key-value pair to generate a set of immediate key-value pairs, and a reduce function that merges all intermediate values associated with the same immediate key, many real world tasks are expressible in this model; 
programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines, the runtime system takes care of details of partitioning input data, scheduling the program's exec across a set of machines, handling machine failures, and managing required inter-machine comm, this allows programmers without any experience with parallel and distributed systems to easily utilize resources of a large distributed system; 
our implementation of MapReduce runs on a large cluster of commidity machines and it is highly scalable :a typical MapReduce computation processes many terabytes of data on thousands of machines, programmers find the system easy to use :hundreds of MapReduce programs have been implemented and 1000+ MapReduce jobs are executed on Google's clusters everyday. 
over the past 5 years, Google have implemented hundreds of special-purpose computations that process large amounts of raw data, such as crawled documents, web req logs, etc., to compute various kinds of derived data, such as inverted indices, various implementations of the graph structure of web documents, the summaries of #pages crawled per host, the set of most frequent queries in a given day, etc., most such computations are conceptually straightforward, however, input data is usually large and computatins have to be distributed across hundreds of thousands of machines in order to finish in a reasonable amount of time, the issues of how to parallelize computation, distribute data, and handle failures conspire to obscure original simple computation with large amounts of complex code to deal with these issues; 
as a reaction to this complexity, we designed a new abstraction that allows us to express the simple computations we were trying to perform but hides the messy details of parallelization, fault-tolerance, data distribution, and load balancing in a lib, our abstraction is inspired by map, reduce primitives present in Lisp and many other functional langs, we realized that most of our computations involved applying a map op to each logical record in our input in order to compute a set of intermediate key-value pairs, and then applying a reduce op to all values that shared the same key in order to combine derived data appropriately, our use of a functional model with user-specified map, reduce ops allows us to parallelize large computations easily and to use re-exec as the primary mechanism for fault tolerance; 
the major contributions of this work are a simple and powerful interface that enables automatic parallelization and distribution of large-scale computations, combined with an implementation of this interface that achieves high perf on large clusters of commidity PCs.
### Programming Model
the computation takes a set of input key-value pairs, and produces a set of output key-value pairs, the user of MapReduce lib expresses the computation as 2 functions :Map, Reduce, here, Map, written by user takes an input pair and produces a set of intermediate key-value pairs, the MapReduce lib groups together all intermediate values associated with the same intermediate key *I* and passes them to the Reduce function, Reduce, also written by user accepts an intermediate key *I* and and a set of values for that key, it merges together these values to form a possibly smaller set of values, typically just 0 or 1 output value is produced per Reduce invocation, the intermediate values are supplied to user's reduce function via an iterator, this allows us to handle lists of values that are too large to fit in mem. 
**example:** consider the problem of counting #occurrences of each word in a large collection of documents, user would write code similar to the following pseudo-code:
```
map(String key, String value):
  // key: document time
  // value: document contents
  for each word w in value:
    EmitIntermediate(w, "1");
reduce(String key, Iterator values):
  // key: a word
  // values: a list of counts
  int result = 0;
  for each v in values:
    result += ParseInt(v);
  Emit(AsString(result));
```
here, map function emits each word plus an associated count of occurrences(just 1 in this simple example), reduce function sums together all counts emitted for a particular word; 
in addition, user writes code to fill a mapreduce specification object with names of input and output files, and optional tuning params, user then invokes MapReduce function, passing it the specification obejct; user's code is linked together with MapReduce lib(implemented in C++);
**types:** even though the previous pseudo-code is written in terms of string inputs and outputs, conecptually the map and reduce functions supplied by user have associated types:
```
map    (k1,v1)       ->list(k2,v2)
reduce (k2,list(v2)) ->list(v2)
```
i.e., the input keys and values are drawn from a different domain than the output keys and values, furthermore, the intermediate keys and values are from the same domains as the output keys and values; 
our C++ implementation passes strings to and from user-defined functions and leaves it to user code to convert bt strings and appropriate types;
**more examples(that can be easily expressed as MapReduce computations):** distributed grep -map function emits a line if it matches a supplied pattern, reduce function is an identity function that just copies supplied intermediate data to the output; count of url access freq -map function processes logs of web page reqs and outputs <URL,1>, reduce function adds together all values for the same url and emits a <URL,totalcount> pair; reverse web-link graph -map function outputs <target,source> pairs for each link to a target url found in a page named source, reduce function concatenates the list of all source urls associated with a given target url and emits the pair <target,list(source)>; term-vector per host -a term vector summarizes the most important words that occur in a document or a set of documents as a list of <word,frequency> pairs, map function emits a <hostname,termvector> pair for each input document(where hostname is extracted from the url of the document), reduce function is passed all per-document term vectors for a given host, it adds these term vectors together, throwing away infrequent terms, and then emits a final <hostname,termvector> pair; inverted index -map function parses each document and emits a sequence of <word,documentID> pairs, reduce function accepts all pairs for a given word and sorts the corresponding document ids and emits a <word,list(documentID)> pair, the set of all output pairs forms a simple inverted index, it is easy to agument this computation to keep track of word positions; distributed sort -map function extracts the key from each record and emits a <key,record> pair, reduce function emits all pairs unchanges.
### Implementation
many different implementations of MapReduce interface are possible, the right choice depends on the environment, for example, one implementation may be suitable for a small shared-mem machine, another for a large numa multiprocessor, and yet another for an even larger collection of networked machines; 
this section describes an implementation targeted to the computing environment in wide use at Google -large clustes of commodity PCs connected together with switched ethernet, it is: (i)machines are typically dualprocessor x86 processers running linux, with 2-4GB of mem per machine, (ii)commodity networking hw is used -typically either 100 megabits/sec or 1 gigabit/sec at the machine level, but averaging considerably less in overall bisection bdwidth, (iii)a cluster consists of hundreds or thousands of machines, and therefore machine failures are common, (iv)storage is provided by inexpensive ide disks attached directly to individual machines, a distributed file system developed in-house is used to manage the data stored on these disks, the file system users replication to provide availability and reliability on top of unreliable hw, (v)users submit jobs to a scheduling system, each job consists of a set of tasks, and is mapped by scheduler to a set of available machines within a cluster; 
**exec overview:** Map invocations are distributed across multiple machines by automatically partitionalling input data into a set of *M* splits, input splits can be processed in parallel by different machines, Reduce invocations are distributed by partitioning the intermediate key space into *R* pieces using a partitioning function(such as *hash(key)* **mod** *R*), #partition(*R*) and the partitioning function are specified by user; [click for overall flow a MapReduce op in our implementation](./img/mapreduce.png), when our user program calls MapReduce function, the following sequence of actions occur(here the numbered labels correspond to the numbers in the list below): (i)MapReduce lib in the user program first splits input files into *M* pieces of typically 16MB to 64MB per piece(controllable by user via an optional param), it then starts up many copies of the program on a cluster of machines, (ii)one of the copies of the program is special -the master, the rest are workers that are assigned work by master, there are *M* map tasks and *R* reduce tasks to assign, master picks idle workers and assigns each one a map task or a reduce task, (iii)a worker who is assigned a map task reads the contents of the corresponding input split, it parses key-value pairs out of input data and passes each pair to user-defined Map function, the intermediate key-value pairs produced by Map function are buffered in mem, (iv)periodically, buffered pairs are written to local disk, partitioned into *R* regions by the partitioning function, locations of these buffered pairs on local disk are passed back to master, who is responsible for forwarding these locations to the reduce workers, (v)when a reduce worker is notified by master about these locations, it uses rpcs to read buffered data from local disks of the map workers, when a reduce worker has read all intermediate data, it sorts it by the intermediate keys so that all occurrences of the same key are grouped together, the sorting is needed because typically many different keys map to the same reduce taks, if the amount of intermediate data is too large to fit in mem, an external sort is uses, (vi)the reduce worker iterates over the sorted intermediate data and for each unique intermediate key encountered, it passes the key and the corrsponding set of intermediate values to user's Reduce function, output of Reduce function is appended to a final output file for this reduce partition, (vii) when all map tasks and reduce tasks have been completed, master wakes up user program, at this point, MapReduce call in the user program returns back to user code; 
after successful completion, output of the mapreduce exec is avialable in the *R* output files(1 per reduce task, with file names as specified by user), typically, users do not need to combine these *R* output files into 1 file -they often pass these files as input to another MapReduce call, or use them from another distributed application that is able to deal with input that is partitioned into multiple files; 
**master data structures:** master keeps several data structures, for each map task and reduce task, it stores the state(idle, in-progress, or complete), and the identity of worker machine(for non-idle tasks); master is the conduit through which the location of intermediate file regions is propagated from map tasks to reduce tasks, hence for each completed map taks, master stores locations and sizes of the *R* intermediate file regions produced by map task, updates to this location and size info are received as map tasks are completed, the info is pushed incrementally to workers that have in-progress reduce tasks; 
**fault tolerance:** since MapReduce lib is designed to help process very large amounts of data using hundreds of thousands of machines, the lib must tolerate machine failures gracefully: (i) worker failure: master pings every worker periodically, if no response is received from a worker in a certain amount of time, master marks the worker as failed, any map tasks completed by worker are reset back to their initial idel state, hence become eligible for scheduling on other workers, similarly, any map task or reduce task in-progress on a failed worker is also reset to idle and becomes eligible for rescheduling; completed map tasks are re-executed on a failure because their output is stored on local disks of the failed machine and is therefore inaccessible, completed reduce tasks do not need to be re-executed since their output is stored in a global file system; when a map task is executed first by worker *A* and then later executed by worker *B*(because *A* failed), all workers executing reduce tasks are notified of the exec, any reduce task that has not already read the data from worker *A* will read data from worker *B*; MapReduce is resilient to large-scale worker failures, for example, during 1 MapReduce op, network maintenance on a running cluster was causing groups of 80 machines at a time to become unreachable for several mins, MapReduce master simply re-executed the work done by unreachable worker machines, and continued to make forward progress, eventually completing MapReduce op; 
(ii) master failure: it is easy to make master write periodic checkpoints of master data structures described above, if master task dies, a new copy can be stored from the last checkpointed state, however, given that there is only a single master, its failure is unlikely, hence our current implementation aborts MapReduce computation if master fails, clients can check for this condition and retry MapReduce op if they desire; 
(iii) semantics in presence of failures: when the user-supplied map and reduce operators are deterministic functions of their input values, our distributed implementation produces the same output as would have been produced by a non-faulting sequential exec of the entire program; we rely on atomic commits of map and reduce task outputs to achieve this property, each in-progress tasks writes its output to private temporary files, a reduce task produces 1 such file, and a map task produces *R* such files(1 per reduce task), when a map task completes, worker sends a msg to master and includes names of the *R* temporary files in the msg, if master receives a completion msg for an already completed map task, it ignores the msg, otherwise, it records names of *R* files in a master data structure; when a reduce task completes, reduce worker atomically renames its temporary output file to the final output file, if the same reduce task is executed on multiple machines, multiple rename calls will be executed for the same final output file, we reply on the atomic rename op provided by underlying file system to guarantee that the final file system state contains just the data produced by 1 exec of the reduce task; the vast majority of our map and reduce operators are deterministic, and the fact that out semantics are equivalent to a sequential exec in this case makes it very easy for programmers to reason about their program's behavior, when map and/or reduce operators are non-deterministic, we provide weaker but still reasonable semantics, in the presence of non-deterministic operators, output of a particular reduce task *R1* is equivalent to output for *R1* produced by a sequential exec of the non-deterministic program, however, output for a different reduce task *R2* may correspond to output for *R2* produced by a different sequential exec of the non-deterministic program; consider map task *M* and reduce tasks *R1, R2*, let *e(Ri)* be the exec of *Ri* that committed(there is exactly 1 such exec), the weaker semantics arise because *e(R1)* may have read output produced by 1 exec of *M* and *e(R2)* may have read output produced by a different exec of *M*; 
**locality:** network bdwidth is a relatively scarce resource in our computing environment, we conserve network bdwidth by taking advantage of the fact that input data(managed by GFS) is stored on local disks of the machines that make up our cluster, GFS delivers each file into 64MB blocks, and stores several copies of each block(typically 3 copies) on different machines; MapReduce master takes location info of input files into account and attempts to schedule a map task on a machine that contains a replica of the corrsponding input data; failing that, it attempts to schedule a map task near a replica of that task's input data(such as on a worker machine that is on the same network switch as the machine containing the data); when running large MapReduce ops on a significant fraction of the workers in a cluster, most input data is read locally and consumes no network bdwidth; 
**task granularity:** we subdivide the map phase into *M* pieces and the reduce phase into *R* pieces, ideally, *M, R* should be much larger than #worker machines; having each worker perform many different tasks improves dynamic load balancing, and also speeds up recovery when a worker fails :the many map tasks it has completed can be spread out across all other worker machines; there are practical bounds on how large *M, R* can be in our implementation, since master must make *O(M+R)* scheduling decisions and keeps *O(M\*R)* state in mem(the constant factors for mem usage are small however :the *O(M\*R)* piece for the state consists of approximately 1 byte of data per map task-reduce task pair); furthermore, *R* is often constrained by users because output of each reduce task ends up in a separate output file, in practice, we tend to choose *M* so that each individual task is roughly 16-64MB of input data(so that the locality optimization described above is most effective), and we make *R* a small multiple of #worker machines we expect to use, we often perform MapReduce computations with *M=200000, R=5000, using 2000 worker machines*; 
**backup tasks:** one of the common causes that lengthens local time taken for a MapReduce op is a straggler :a machine that takes an unusually long time to complete one of the last few map or reduce tasks in the computation; straggler can arise for a whole host of reasons, for example, a machine with a bad disk may experience frequent correctable errors that slow its read perf from 30MB/sec to 1MB/sec, the cluster scheduling system may have scheduled other tasks on the machine, causing it to execute MapReduce code more slowly due to competition for cpu, mem, local disk, or network bdwidth, a recent problem we experienced was a bug in machine initialization code that caused processor caches to be disabled :computations on affected machines slowed down by over a factor of 100; we have a general mechanism to alleviate the problem of stragglers, when a MapReduce op is close to completion, master schedules backup exec of the remaining in-progress tasks, the task is marked as completed whenever either the primary or the backup exec completes, we have tuned this mechanism so that it typically increases the computational resources used by the op by no more than a few percent, we have found that this significally reduces the time to complete large MapReduce ops.
### Spark :Cluster Computing with Working Sets
### [Zaharia10](https://www.usenix.org/legacy/event/hotcloud10/tech/full_papers/Zaharia.pdf)
MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters, however, most of these systems are built around an acylic data flow model that is not suitable for other popular applications, here we focus on one such class of applications :those that reuse a working set of data across multiple parallel ops, this includes many iterative ml algorithms as well as interactive data analysis tools, we propose a new framework called Spark that supports these applications while retaining the scalability and fault tolerance of MapReduce, to achieve these goals, Spark introduces an abstraction called RDDs -resilient distributed datasets, an rdd is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost, Spark can outperform Hadoop by 10x in iterative ml jobs and can be used to interactively query a 39GB dataset with sub-second response time. 
a new model of cluster computing has been widely popular, in which data-parallel computations are executed on clusters of unreliable machines by systems that automatically provide locality-aware scheduling, fault tolerance, and load balancing; MapReduce pioneered this model while system like Dryad, MapReudceMerge generalized types of data flows supported, these systems achieve their scalability and fault tolerance by providing a programming model where user creates acyclic data flow graphs to pass input data through a set of operators, this allows underlying system to manage scheduling and to react to faults without user intervention; 
while this data flow programming model is useful for a large class of applications, there are applications that cannot be expressed efficiently as acyclic data flows, here we focus on one such class of applications :those that reuse a working set of data across multiple parallel ops, this includes 2 use cases where we have seen Hadoop users report that MapReduce is deficient: (i)iterative jobs -many common ml algorithms apply a function repeatedly to the same dataset to optimize a param(such as through gradient descent), while each iteration can be expressed as a MapReduce/Dryad job, each job must reload the data from disk, incurring a significant perf penalty, (ii)interactive analytics -Hadoop is often used to run ad-hoc exploratory queries on large datasets, through sql interfaces such as Pig, Hive, ideally, a user would be able to load a dataset of interest into mem across a number of machines and query it repeatedly, however, with Hadoop each query incurs significant latency(tens of secs) because it runs as a separate MapReduce job and reads data from disk; 
here we present a new cluster computing framework called Spark, which supports applications with working sets while providing similar scalability and fault tolerance properties to MapReduce; 
the main abstraction in Spark is that of a rdd, which represents a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost; users can explicitly cache an rdd in mem across a set of machines and reuse it in multiple MapReduce-like parallel ops; rdds achieve fault tolerance through a notion of lineage :if a partition of an rdd is lost, the rdd has enough info about how it was was derived from other rdds to be able to rebuild just that partition; although rdds are not a general shared mem abstraction, trhey represent a sweet-spot bt expressively on the one hand and scalability and reliablity on the other hand, and we have found them well-suited for a variety of applications; 
Spark is implemented in Scala, a statically typed high-level programming lang for java vm, and exposes a functional programming interface similar to DryadLINQ, in addition, Spark can be used interactively from a modified version of the Scala interpreter, which allows user to define rdds, functions, vars, and classes, and use them in parallel ops on a cluster, we believe that Spark is the first system to allow an efficient, general-purpose programming lang to be used interactively to process large datasets on a cluster; 
although our implementation of Spark is still a prototype, early experience with the system is encouraging, we show that Spark can outperform Hadoop by 10x in iterative ml wkloads and can be used interactively to scan a 39GB dataset with sub-second latency.
### Programming Model
to use Spark, developers write a driver program that implements the high-level control flow of their application and launches various ops in parallel, Spark provides 2 main abstractions for parallel programming :rdds and parallel ops, on these datasets(invoked by passing a function to apply on a dataset), in addition, Spark supports 2 restricted types of a shared vars that can be used in functions running on the cluster, which we shall explain later; 
**rdds:** a rdd is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost, elements of an rdd need to exist in physical storage, instead, a handle to an rdd contains enough info to compute the rdd starting from data in reliable storage, this means rdds can always be reconstructed if nodes fail; in Spark, each rdd is represented by a Scala object, Spark lets programmers construct rdds in 4 ways including: (i)from a file in a shared file system such as HDFS -Hadoop distributed file system, (ii)by parallelizing a Scala collection(such as an array) in the driver program which means dividing it into a number of slices that will be sent to multiple nodes, (iii)by transforming an existing rdd, a dataset with elements of type A can be transformed into a dataset with elements of type B using an op called flatMap, which passes each element through a user-provided function of type *A->List\[B\]*(*faltMap has the same semantics as the map in MapReduce, but map is usually used to refer to a one2one function of type *A->B* in Scala), other transformations can be expressed using faltMap, including map(passes elements through a function of type *A->B*) and filter(pick elements matching a predicate), (iv)by changing the persistence of an existing rdd, by default, rdds are lazy and ephemeral i.e. partitions of a dataset are materialized on demand when they are used in a parallel op(such as by passing a block of a file through a map function), and are discarded from mem after use(*this is how distributed collections function in DryadLINQ), however, a user can alter the persistence of an rdd through 2 actions :the cache action leaves the dataset lazy but hints that it shoudl be kept in mem after the first time it is computed because it will be reused, the save action evaluates the dataset and writes it to a distributed file system such as hdfs the saved version is used in future ops on it; we note that our cache action is only a hint :if there is not enough mem in the cluster to cache all partitions of a dataset, Spark will recompute them when they are used, we chose this design so that Spark programs keep working(at reduced perf) if nodes fail or if a dataset is too big, this idea is loosely analogous to vm; we also plan to extend Spark to support other levels of persistence(such as in-mem replication across multiple nodes), our goal is to let users trade off bt the cost of storing an rdd, the speed of accessing it, the prob of losing part of it, and the cost of recomputing it; 
**parallel ops:** several parallel ops can be performed on rdds including: (i)reduce -combines dataset elements using an associative function to produce a result at the driver program, (ii)collect -sends all elements of the dataset to the driver program such as an easy way to update an array in parallel is to parallelize, map, and collect the array, (iii)foreach -passes each element through a user provided functions, this is only done for the side effects of the function(which might be to copy data to another system or to update a shared var as explained below); we note that Spark does not currently support a grouped reduce op as in MapReduce, results are only collected at 1 process(the driver, however, local functions are first performed at each node), we plan to support grouped reductions in the future using a shuffle transformation on distributed datasets, however, even using a single reducer is enough to express a variety of useful algorithms such as a recent paper on MapReduce for ml on multicore systems implemented 10 learning algorithms without supporting parallel reduction; 
**shared vars:** programmers invoke ops like map, filter, and reduce by passing closures(functions) to Spark, as is typical in functional programming, these clusters can refer to vars in the scope where they are created; normally, when Spark runs a closure on a worker node, these vars are copied to the worker, however, Spark also lets programmers create 2 restricted types of shared vars to support 2 simple but common usage patterns including: (i)broadcast vars -if a large read-only piece of data(such as a lookup table) is used in multiple parallel ops, it is preferable to distribute it to workers only once instead of packaging it with every closure, Spark lets programmers create a broadcast var object that wraps the value and ensures that it is only copied to each worker once, (ii)accumulators -these are vars that workers can only add to using an associative op, and that only the driver can read, they can be used to implement counters as in MapReduce and to provide a more imperative syntax for parallel sums, accumulators can be defined for any type that has an add op and a 0 value, due to their add-only semantics, they are easy to make fault-tolerant.
### Examples
we now show some sample Spark programs, note that we omit var types because Scala supports type inference; 
**text search:** suppose that we wish to count lines containing errors in a large log file stored in hdfs, this can be implemented by starting with a file dataset object as follows:
```
val file = spark.textFile("hdfs://...")
val errs = file.filter(_.contains("ERROR"))
val ones = errs.map(_ => 1)
val count = ones.reduce(_+_)
```
we first create a distributed dataset called file that represents the hdfs file as a collection of lines, we transform this dataset to create the set of lines containing ERROR(errs), and then amp each line a 1 and add up these 1s using reduce, the arguments to filter, map, and reduce are Scala syntax for function literals; note that errs and ones are lazy rdds that are never materialized, instead, when reduce is called, each worker node scans input blocks in a streaming manner to evaluate ones, adds these to perform a local reduce, and sends its local count to the driver, when used with lazy datasets in this manner, Spark closely emulates MapReduce; where Spark differs from other frameworks is that it can make some of the intermediate datasets persist across operations, such as if wanted to reuse the errs dataset, we could create a cached rdd from it as follows:
```
val cachedErrs = errs.cache()
```
we would now be able to invoke parallel ops on cachedErrs or on datasets derived from it as usual, but nodes would cache partitions of cachedErrs in mem after the first time they compute them, greatly speeding up subsequent ops on it; 
**logistic regression:** the following program implements logistic regression, an iterative classification algorithm that attempts to find hyperplane *w* that best separates 2 sets of points, the algorithm performs gradient descent :it starts *w* at a random value, and on each iteration, it sums a function of *w* over the data to move *w* in a direction that improves it, it hence benifits from caching the data in mem across iterations:
```
// read points from a text file and cache them
val points = spark.textFile(...).map(parsePoint).cache()
// initiate w to random d-dim vector
var w = Vector.random(D)
// run multiple iterations to update w
for (i <- 1 to ITERATIONS) {
  val grad = spark.accumulator(new Vector(D))
  for (p <- points) { // runs in parallel
    val s = (1/(1+exp(-p.y*(w dot p.x)))-1*p.y)
    grad += s*p.x
  }
  w -= grad.value
}
```
first although we create an rdd called points, we process it by running a for loop over it, the for keyword in Scala is syntactic sugar for invoking the foreach method of a collection with the loop body as a closure i.e. the code *for(p <- points){body}* is equivalent to *points.foreach(p => {body})*, hence we are invoking Spark's parallel foreach op, second to sum up the gradient, we use an accumulator var called gradient(with a value of type Vector), note that the loop adds to gradient using an overloaded += operator, the combination of accumulators and for syntax allows Spark porgrams to look much like imperative serial programs, indeed, this example differs from a serial version of logistic regression in only 3 lines; 
**ALS -alternating least squares:** used for collaborative filtering problems such as predicting users' ratings for movies that they have not seen based on their movie rating history(as in the Netflix Challenge), unlike our previous examples, als is cpu-intensive rather than data-intensive; we briefly sketch als -suppose that we wanted to predict the ratings of *u* users for *m* movies and that we had a partially filled matrix *R* containing known ratings for some user-movie pairs, als models *R* as the product of 2 matrices *M, U* of dims *mtimesk, ktimesu* respectively i.e. each user and each movie has a *k*-dim feature vector describing its characteristics, and a user's rating for a movie is the dot product of its feature vector and the movie's, als solves for *M, U* using the known ratings and then computes *MtimesU* to predict the unknown ones, this is done using the following iterative process: 1.initialize *M* to a random value, 2.optimize *U* given *M* to minimize error on *R*, 3.optimize *M* given *U* to minimize error on *R*, 4.repeat steps2&3 until convergence; als can be parallelized by updating different users/movies on each node in steps2&3, however, because all of the steps use *R*, it is helpful to make *R* a broadcast var so that it does not get re-sent to each node on each step, note that we parallelize the collection 0 until u(a Scala range object) and collect it to update each array:
```
val Rb = spark.broadcast(R)
for (i <- 1 to ITERATIONS) {
  U = spark.parallelize(0 until u).map(j => updateUser(j, Rb, M)).collect()
  M = spark.parallelize(0 until m).map(j => updateUser(j, Rb, U)).collect()
}
```
### Implementation
Spark is built on top of Mesos, a cluster os that lets multiple parallel applications share a cluster in a fine-grained manner and provides an api for applications to launch tasks on a cluster, this allows Spark to run alongside existing cluster computing frameworks such as Mesos ports of Hadoop and MPI, and share data with them, in addition, building on Mesos greatly reduced the programming effort that had to go into Spark; 
the core of Spark is the implementation of rdds, for example, suppose that we define a cached dataset called cachedErrs representing error msgs in a log file, and that we count its elements using map and reduce as in section text search, [these datasets will be stored as a chain of objects capturing the lineage of each rdd, click to view](./img/resilient-distributed-datasets-linear-chain.png), each dataset object contains a pointer to its parent and info about how the parent was transformed; 
internally, each rdd object implementats the same simple interface, which consists of 3 ops including: getPartitions -which returns a list of partition IDs, getIterator(partition) -which iterates over a partition, getPreferredLocations(partition) -which is used for task scheduling to achieve data locality; when a parallel op is invoked on a dataset, Spark creates a task to process each partition of the dataset and sends these tasks to worker nodes, we try to send each task to one of its preferred locations using a technique called delay scheduling, once launched on a worker, each task calls getIterator to start reading its partition; 
the different types of rdds differ only in how they implement the rdd interface, for example, for a HDFS-Textfile, the partitions are block IDs in hdfs, their preferred locations are the block locations, and getIterator opens a stream to read a block, vs., in a MappedDataset, the partitions and preferred locations are the same as for the parent, but the iterator applies the map function to elements of the parent, vs., in a CachedDataset, the getIterator method looks for a locally cached copy of a transformed partition, and each partition's preferred locations start out equal to parent's preferred locations, but get updated after the partition is cached on some node to prefer using that node, this design makes faults easy to handle :if a node fails, its partitions are re-read from their parent datasets andd eventually cached on other nodes; 
shipping tasks to worker requires shipping closures to them -both the closures used to define a distributed dataset, and closures passed to ops such as reduce, to achieve this, we rely on the fact that Scala closures are java objects and can be serialized using java serialization, this is a feature of Scala that makes it relatively straightforward to send a computation to another machine; Scala's built-in closure implementation is not ideal, however, because we have found cases where a closure object references vars in the closure's outer scope that are not actually used in its body, we have filed a bug report about this, but in the meantime, we have solved the issue by performing a static analysis of closure classes' bytecode to detect these unused vars and set the corresponding fields in the closure object to null; 
**shared vars:** the 2 types of shared vars in Spark, broadcast vars and accumulators, are implemented using classes with custom serialization formats, when one creates a broadcast var b with a value v, v is saved to a file in a shared file system, the serialized form of b is a path to this file, when b's value is required on a worker node, Spark first checks whether v is in a local cache, and reads it from the file system if it is not, we initially used hdfs to broadcast vars, but we are developing a more efficient streaming broadcast system; accumulators are implemented using a different serialization trick, each accumulator is given a unique ID when it is created, when the accumulator is saved, its serialized form contains its ID and the 0 value for its type, on the workers, a separate copy of the accumulator is created for each thread that runs a task using thread-local vars, and is set to 0 when a task begins, after each task runs, worker sends a msg to the driver program containing the updates it made to various accumulators, the driver applies updates from each partition of each op only once to prevent double-counting when tasks are re-executed due to failures; 
**interpreter integration:** here we only sketch how we have integrated Spark into Scala interpreter, the Scala interpreter normally operates by compiling a class for each line typed by user, this class includes a singleton object that contains vars or functions on that line and runs the line's code in its constructor, for example, if user types *var x=5* followed by *println(x)*, the interpreter defines a class(say *Linel*) containing *x* and causes the second line to compile to *print(Linel.getInstance().x)*, these classes are loaded into jvm to run each line, to make the interpreter work with Spark, we made 2 changes including: (i)we made the interpreter output the classes it defines to a shared filesystem, from which they can be loaded by workers using a custom java class loader, (ii)we changed the generated code so that the singleton object for each line references the singleton objects for previous lines directly rather than going through the static *getInstance* methods, this allows closures to capture current state of the singletons they reference whenever they are serialized to be sent to a worker, if we had not done this, then updates to the singleton objects(such as a line setting *x=7* above) would not propagate to workers.
### Scaling Distributed Machine Learning with the Parameter Server
### [Li14](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf)
we propose a param server framework for distributed ml problems, both data and wkloads are distributed over worker nodes, while the server nodes maintain globally shared params, represented as dense or sparse vectors and matrices, the framework manages asynchronous data comm bt nodes, and supports flexible consistency models, elastic scalability, and continuous fault tolerance; to demonstrate scalability of the proposed framework we show experimental results on petabytes of real data with billions of examples and params on problems ranging from Sparse Logistic Regression to Latent Dirichlet Allocation and Distributed Sketching. 
distributed optimization and inference is becoming a prerequisite for solving large scale ml problems, at scale, no single machine can solve these problems sufficiently rapidly, due to growth of data and the resulting model complexity, often manifesting itself in an increased number of params, however, implementing an efficient distributed algorithms is not easy, both intensive computational wkloads and the volumne of data comm demand careful system design; 
realistic quantities of training data can range bt 1TB-1PB, this allows one to create powerful and complex models with 10^9-10^12 params, these models are often shared globally by all worker nodes, which must frequently accesses shared params as they perform computation to refine it, sharing imposes 3 challenges including: accessing the params requires an enormous amount of network bdwidth; many ml algorithms are sequential, the resulting barriers hurt perf when cost of synchronization and machine latency is high; at scale, fault tolerance is critical, learning tasks are often performed in a cloud environment where machines can be unreliable and jobs can be preempted; to illustrate the last point, we collected all job logs for a 3-month period from one cluster at a large internet company, [click for stats of batch ml tasks serving a production environment](./img/statistics-of-machine-learning-jobs.png), here task failure is mostly due to being preempted or losing machines without necessary fault tolerance mechanisms; unlike in many research settings where jobs run exclusively on a cluster without contention, fault tolerance is a necessity in real world deployments; 
**contributions:** since its introduction, the param server framework has proliferated in academia and industry, here we describe a 3rd-gen open source implementation of a param server that focuses on the systems aspects of distributed inference, it confers 2 advantages to developers :first by factoring out commonly required components of ml systems, it enables application-specific code to remain concise, at the same time, as a shared platform to target for sysytems-level optimizations, it provides a robust versatile and high-perf implementation capable of handling a diverse array of algorithms from sparse logistic regression to topic models and distributed sketching; our design decisions were guided by wkloads found in real systems, our param server provides 5 key features including: (i)efficient comm -the asynchronous comm models does not block computation(unless requested), it is optimized for ml tasks to reduce network traffic and overhead, (ii)flexible consistency models -relaxed consistency further hides synchronization cost and latency, we allow the algorithm designer to balance algorithmic convergence rate and system efficiency, the best trade-off depends on data, algorithm, and hw, (iii)elastic scalability -new nodes can be added without restarting the running framework, (iv)fault tolerance and durability -recovery from and repair of non-catastrophic machine failures within 1sec, without interrupting computation, vector clocks ensure well-defined behavior after network partition and failure, (v)ease of use -the globally shared params are represented as (potentially sparse) vectors and matrices to facilitate dev of ml applications, the linear algebra data types come with high-perf multithreaded libs; the novelty of proposed system lies in the synergy achieved by picking the right systems techinuqes, adapting them to ml algorithms, and modifying the ml algorithms to be more systems-friendly, in particular, we can relax a number of otherwise hard systems constraints since the associated ml algorithms are quite tolerant to perturbations, the consequence is the first general purpose ml system capable of scaling to industrial scale sizes; 
**engineering challenges:** when solving distributed data analysis problems, the issue of reading and updating params shared bt different worker nodes is ubiquitous, the param server framework provides an efficient mechanism for aggregating and synchronizing model params and stats bt workers, each param server node maintains only a part of the params, and each worker node typically requires only a subset of these params when operating, 2 key challenges arise in constructing a high perf param server system: (i) comm: while params could be updated as key-value pairs in a conventional datastore, using this abstraction natively is inefficient :values are typically small(floats or ints), and the overhead of sending each update as a key value op is high; our insight to improve this situation comes from the observation that many learning algorithms represent params as structured mathematical objects such as vectors, matrices, or tensors, at each logical time(or an iteration) typically a part of the object is updated, i.e. workers usually send a segment of a vector, or an entire row of the matrix, this provides an opportunity to automatically batch both comm of updates and their processing on the param server, and allows the consistency tracking to be implemented efficiently; 
(ii) fault tolerance: as noted earlier, is critical at scale, and for efficient op it must not require a full restart of a long-running computation, live replication of params bt servers supports hot failover, failover and self-repair in turn support dynamic scaling by treating machine removal or addition as failure or repair respectively; [click for an overview of the scale of the largest supervised and unsupervised ml experiments performed on a number of systems](./img/comparison-of-the-public-largest-machine-learning-experiments-each-system-performed.png), when possible, we confirmed the scaling limits with authors of each of these systems(data current as of 4/2014), as is evident, we are able to cover orders of magnitude more data on orders of magnitude more processors than any other published system, furthermore, [click for an overview of main characteristics of several ml systems](./img/attributes-of-distributed-data-analysis-systems.png), our param server offers the greatest degree of flexibility in terms of consistency, it is the only system offering continuous fault tolerance, its native data types make it particularly friendly for data analysis.
### Machine Learning
ml systems are widely used in web search, spam detection, recommendation systems, computational advertising, and document analysis, these systems automatically learn models from examples, termed training data, and typically consist of 3 components including: feature extraction, objective function, and learning; here, feature extraction processes raw data such as documents, images, and user query logs, to obtain feature vectors, where each feature captures an attribute of the training data, preprocessing can be executed efficiently by existing frameworks such as MapReduce; 
**goals:** goal of many ml algorithms can be expressed via a object function, this function captures properties of the learned model, such as low error in the case of classifying emails into ham and spam, how well the data is explained in the context of estimating topics in documents, or a concise summary of counts in the context of sketching data; the learning algorithm typically minimizes this objective function to obtain the model, in general, there is no closed-form solution, instead, learning starts from an initial model, it iteratively refines this model by processing training data, possibly multiple times, to approach the solution, it stops when a (near) optimal solution is found or the model is considered to be converged; the training data may be extremely large, for example, a larget internet company using 1yr of an ad impression log to train an ad click predictor would have trillions of trainig examples, each training example is typically represented as a possibly very high-dim feature vector, hence training data may consist of trillions of trillion-length feature vectors, iteratively processing such large scale data requires enormous computing and bdwidth resources, moreover, billions of new ad impressions may arrive daily, adding this data into the system often improves both prediction accuracy and coverage, but it also requires the learning algorithm to run daily, possibly in real time. efficient exec of these algorithms is the main focus; to motivate design decisions in our system, next we briefly outline the 2 widely used ml technologies that we will use to demonstrate the efficacy of our param server; 
**risk minimization:** the most intuitive variant of ml problems is that of risk minimization, the risk is roughly a measure of prediction error such as if we were to predict tomorrow's stock price the risk might be the deviation bt prediction and the actual value of stock; training data consists of *n* examples, *xi* is the *i*-th such example and is often a vector of length *d*, as noted earlier both *n, d* may be on the order of billions to trillions of examples and dims respectively, in many cases, each training example *xi* is associated with a label *yi*, for example, in ad click prediction, *yi* might be 1 for clicked or -1 for not clicked; risk minimization learns a model that can predict the value *y* of a feature example *x*, the model consists of params *w*, in the simplest example, the model params might be the clickiness of each feature in an ad impression, to predict whether a new impression would be clicked, the system might simply suns its clickiness based upon features present in the impression, namely *xTw := _sum{j=1}{d}xjwj*, and then decide based on the sign; in any learning algorithm, there is an important relationship bt the amount of training data and model size, a more detailed model typically improves accuracy, but only up to a point :if there is too little training data, a highly-detailed model will overfit and become merely a system that uniquely memorizes every item in the training set, on the other hand, a too-small model will fail to capture interesting and relevant attributes of the data that are important to making a correct decision; regularized risk minimization is a method to find a model that balances model complexity and training error, it does so by minimizing sum of 2 terms :a loss *l(x,y,w)* representing prediction error on training data, and a regularizater $\Omega$\[*w*\] penalizing model complexity, a good model is one with low error and low complexity, consequently we strive to minimize: *F(w) = sigma_{i=1}{n}l(xi,yi,w)+*$\Omega$(*w*); the specific loss and regularizer functions used are important to the prediction perf of the ml algorithm, but relatively unimportant for the purpose of this paper :algorithms we present can be used with all of the most popular loss functions and regularizers; in future section Sparse Logistic Regression, we use a high-perf distributed learning algorithm to evaluate the param server, for the sake of simplicity we describe a much simpler model called [distributed subgradient descent](./img/distributed-subgradient-descent.png)(*the unfamiliar reader could read this as gradient descent, the subgradient aspect is simply a generalization to loss functions and regularizers that need not be continuously differentiate, such as |w| at w=0), here, training data is partitioned among all of the workers, which jointly learn the param vecor *w*, the algorithm operates iteratively, in each iteration, every worker independently uses its own training data to determine what changes should be made to *w* in order to get closer to an optimal value, because each worker's updates reflect only its own training data, the system needs a mechanism to allow these updates to mix, it does so by expressing updates as a subgradient -a direction in which the param vector *w* should be shifted -and aggregates all subgradients before applying them to *w*, these gradients are typically scaled down, with considerable attention paid in algorithm design to the right lr $\eta$ that should be applied in order to ensure that the algorithm converges quickly; here the most expensive step is computing the subgradient to update *w*, this task is divided among all of the workers, each of which execute WORKERITERATE, as part of this, workers compute *wTx_{i_k}* which should be infeasible for very high-dim *w*, fortunately, a worker needs to know a coordinate of *w* iff some of its training data references that entry; for example, in ad click prediction one of the key features are the words in ad, if only very few advertisements contain the phrase OSDI 2014, then most workers will not generate any updates to the corresponding entry in *w*, and hence do not require this entry, while the total size of *w* may exceed capacity of a single machine, the working set of entries needed by a particular worker can be trivially cached locally, to illustrate this, we randomly assigned data to workers and then counted the average working set size per worker on the dataset that is used in future section Sparse Logistic Regression, [click for for 100 workers each worker only needs 7.8% of the total params, with 10000 workers this reduces to 0.15%](./img/each-workers-set-of-parameters-shrinks-as-more-workers-are-used.png); 
**generative models:** in a second major of class of ml algorithms, the label is to be applied to training examples is unknown, such settings call for unsupervised algorithms(for labeled training data one can use supervised or semi-supervised algorithms), they attempt to capture underlying structure of the data, for example, a common problem in this area is topic modeling :given a collection of documents, infer the topics contained in each document; when run on for example the SOSP'13 proceedings, an algorithm might generate topics such as "distributed systems", "machine learnig", and "performance", the algorithms infer these topics from the content of the documents themselves, not an external topic list, in practical settings such as content personalization for recommendation systems, the scale of these problems is huge -hundreds of millions of users and billions of documents, making it cirtical to parallelize the algorithms across large clusters; because of their scale and data volumes, these algorithms only became commercially applicable following the introduction of 1st-gen param servers, a key challenge in topic models is that params describing current estimate of how documents are supposed to be generated must be shared; a popular topic modeling approach is LDA -Latent Dirichlet Allocation, while the statistical model is quite different, the resulting algorithm for learning it is very similar to Distributed Subgradient Descent(*the specific algorithm we use in the evaluation is a parallelized variant of a stochastic variational sampler with an update strategy similar to that used in YahooLDA), however, the key difference is that the update step is not a gradient computation, but an estimate of how well the document can be explained by current model, this computation requires access to auxiliary metadata for each document that is updated each time a document is accessed, because of #documents, metadata is typically read from and written back to disk whenever document is processed; here, this auxiliary data is the set of topics assigned to each word of a document, and the param *w* being learned consists of the relative freq of occurrence of a word; as before, each worker needs to store only the params for words occuring in the documents it processes, distributing documents across workers has the same effect as in the previous section -we can process much bigger models than a single worker may load.
### Architecture
an instance of the param server can run more than one algorithm simultaneously, param server nodes are grouped into a server group and several worker groups as shown in [fig4](./img/architecture-of-a-parameter-server-communication-with-several-groups-of-servers.png), a server node in the server group maintains a partition of the globally shared params, server nodes communicate with each other to replicate and/or to migrate params for reliability and scaling, a server manager node maintains a consistent view of the metadata of servers such as node liveness and the assignment of param partitions; 
each worker group runs an application, a worker typically stores locally a portion of the training data to compute local stats such as gradients, workers communicate only with the server nodes(not among themselves), updating and retrieving shared params, there is a scheduler node for each worker group, it assigns tasks to workers and monitors their progress, if workers are added or removed, it reschedules unfinished tasks; 
the param server supports independent param namespaces, this allows a worker group to isolate its set of shared params from others, several worker groups may also share the same namespace :we may use more than one worker group to solve the same dl application to increase parallelization; another example is that of a model being actively queried by some nodes such as online services consuming this model, simultaneously the model is updated by a different group of worker nodes as new training data arrives; 
the param server is designed to simplify developing distributed ml applications such as those discussed in section Machine Learning, the shared params are represented as (key,value) vectors to facilitate linear algebra ops(see future section (key,value) vectors), they are distributed across a grooup of server nodes(see future section consistent hashing), any node can both push out its local params and pull params from remote nodes(see future section range push and pull), by default, wkloads or tasks are executed by worker nodes, however, they can also be assigned to server nodes via user defined functions(see future section user-defined functions on the server), tasks are asynchronous and run in parallel(see future section asynchronous tasks and dependency), the param server provides the algorithm designer with flexibility in choosing a consistency model via task dependency graph(see future section flexible consistency) and predicates to communicate a subset of params(see future section user-defined filters); 
**(key,value) vectors:** the model shared among nodes can be represented as a set of (key,value) pairs, for example, in a loss minimization problem, the pair is a feature ID and its weight, for LDA, the pair is a combination of word ID and topic ID, and a count, each entry of the model can be read and written locally or remotely by its key, this (key,value) abstraction is widely adopted by existing approaches; our param server improves upon this basic approach by acknowledging underlying meaning of these key value items -ml algorithms typically treat the model as a linear algebra object, for example, *w* is used as a vector for both objective function and optimization in Distributed Subgradient Descent by risk minimization, by treating these objects as sparse linear algebra objects, the param server can provide the same functionality as the (key,value) abstraction, but admits important optimized ops such as vector addition *w+u*, multiplication *Xw*, finding the 2-norm ||w||2, and other more sophisticated ops; to support these optimizations, we assume that the keys are ordered, this lets us treat params as (key,value) pairs while endowing them with vector and matrix semantics, where non-existing keys are associated with 0s, this helps with linear algebra in ml, it reduces the programming effort to implement optimization algorithms, beyond convenience, this interface design leads to efficient code by leveraging cpu-efficient multithreaded self-tuning linear algebra libs such as BLAS, LAPACK, ATLAS; 
**range push and pull:** data is sent bt nodes using push and pull ops, in Distributed Subgradient Descent each worker pushes its entries local gradient into servers and then pulls updated weight back, the more advanced algorithm described in [Delayed Block Proximal Gradient](./img/delayed-block-proximal-gradient.png) uses the same pattern except that only a range of keys is communicated each time; the param server optimizes these updates for programmer convenience as well as computational and network bdwidth efficiency by supporting range-based push and pull -if *R* is a key range, then *w.push(R,dest)* sends all existing entries of *w* in key range *R* to destination, which can be either a particular node or a node group such as the server group, similarly, *w.pull(R,dest)* reads all existing entries of *w* in key range *R* from destination, if we set *R* to be the whole key range then the whole vector *w* will be communicated, if we set *R* to include a single key then only an individual entry will be sent; this interface can be extended to communicate any local data structures that share the same keys as *w*, for example, in Distributed Subgradient Descent a worker pushes its temporary local gradient *g* globally shared, however, note that *g* shares the keys of the worker's working set *w*, hence the programmer can use *w.push(R,g,dest)* for local gradients to save mem and also enjoy the optimization discussed in the following sections; 
**user-defined functions on the server:** beyond aggregating data from workers, server nodes can execute user-defined functions, it is beneficial because server nodes often have more complete or up-to-date info about the shared params, in Distributed Subgradient Descent server nodes evaluate subgradients of the regularizer $\Omega$ in order to update *w*, at the same time in Delayed Block Proximal Gradient a more complicated proximal operator is solved by servers to update the model; 
**asynchronous tasks and dependency:** a tasks is issued by a rpc, it can be a push or a pull that a worker issues to servers, it can also be a user-defined function that the scheduler issues to any node, tasks may include any number of subtasks such as the task WorkerIterate in Distributed Subgradient Descent contains 1 push and 1 pull; tasks are executed asynchronously :the caller can perform further computation immediately after issuing a task, the caller marks a task as finished only once it receives the callee's reply, a reply could be the function return of a user-defined function, the (key,value) pairs requested by the pull or an empty acknowledgement, the callee marks a task as finished only if the call of the task is returned and all subtasks issued by this call are finished; by default, callees execute tasks in parallel for best perf, a caller that wishes to serialize task exec can place an exec-after-finished dependency bt tasks, [click for 3 example iterations of WorkerIterate](./img/three-example-iterations-of-workerIterate.png), here Iterations10&11 are independent, but 12 depends on 11, the callee therefore begins iteration 11 immediately after local gradients are computed in 10, however, 12 is postponed until the pull of 11 finishes; task dependencies help implement algorithm logic, for example, the aggregation logic in ServerIterate of Distributed Subgradient Descent updates the weight *w* only after all worker gradients have been aggregated, this can be implemented by having the updating task depend on the push tasks of all workers, the second important use of dependencies is to support the flexible consistency models described next; 
**flexible consistency:** independent tasks improve system efficiency via parallelizing use of cpu, disk, and network bdwidth, however, this may lead to data inconsistency bt nodes, in [fig5](./img/three-example-iterations-of-workerIterate.png) worker *r* starts Iteration11 before *w^(11)* has been pulled back so it uses the old *w_r^(10)* in this iteration and so obtains the same gradient as in 10, namely *g_r^(11)=g_r^(10)*, this inconsistency potentially slows down the convergence progress of Distributed Subgradient Descent, however, some algorithms may be less sensitive to this type of inconsistency, for example, only a segment of *w* is updated each time in Delayed Block Proximal Gradient, hence starting 11 without waiting for 10 causes only a part of *w* to be inconsistent; the best trade-off bt system efficiency and algorithm convergence rate usually depends on a variety of factors including the algorithm's sensitivity to data inconsistency, feature correlation in training data, and capacity difference of hw components, instead of forcing user to adopt 1 particular dependency that may be illsuited to the problem, the param server gives the algorithm designer flexibility in defining consistency models, this is a substantial difference to other ml systems; [we show 3 different models that can be implemented by task dependency, click for their associated directed acyclic graphs](./img/directed-acyclic-graphs-for-different-consistency-models.png), it is: (i)sequential consistency, all tasks are executed one by one, the next task can be started only if the previous one has finished, it produces results identical to the single-thread implementation, and also named Bulk Synchronous Processing, (ii)eventual consistency, is the opposite -all tasks may be started simultaneously, however, this is only recommendable if underlying algorithms are robust with regard to delays, (iii)bounded delay, when a maximal delay time $\tau$ is set, a new task will blocked until all previous tasks $\tau$ times ago have been finished, Delayed Block Proximal Gradient uses such a model, this model provides more flexible controls than the previous two :$\tau$*=0* is the sequential consistency model, and an infinite delay $\tau$*=*$\infty$ becomes the eventual consistency model; note that the dependency graphs may be dynamic such as the scheduler may increase or decrease the maximal delay according to runtime progress to balance system efficiency and convergence of underlying optimization algorithm, in this case caller traverses directed acyclic graph, if the graph is static, caller can send all tasks with directed acyclic graph to callee to reduce synchronization cost; 
**user-friendly filters:** complementary to a scheduler-based flow control, the parameter server supports user-defined filters to selectively synchronize individual (key,value) pairs, allowing fine-grained control of data consistency within a task, the insight is that the optimization algorithm itself usually possesses info on which params are most useful for synchronization, one example is the significantly modified filter which only pushes entries that have changed by more than a threshold since their last synchronization.
### Implementation
servers store params(key value pairs) using consistent hashing(see future section consistent hashing), for fault tolerance entries are replicated using chain replication(see future section replication and consistency), different from prior (key,value) systems the param server is optimized for range-based comm with compression on both data(see future section msgs) and range-based vector clocks(see future section vector clock); 
**vector clock:** given the potentially complex task dependency graph and the need for fast recovery, each (key,value) pair is associated with a vector clock, which records time of each individual node on this (key,value) pair, vector clocks are convenient such as for tracking aggregation status or rejecting doubly sent data, however, a naive implementation of the vector clock requires O(nm) space to handle n nodes and m params, with thousands of nodes and billions of params, this is infeasible in terms of mem and bdwidth; fortunately, many params hare the same timestamp as a result of the range-based comm pattern of the param server :if a node pushes params in a range then the timestamps of the params associated with the node are likely the same, hence they can be compressed into a single range vector clock, more specifically, assume that *vc_i (k)* is the time of key *k* for node *i*, given a key range *R*, the ranged vector clock *vc_i (R)=t* means for any key *k belongstoR, vc_i (k)=t*; initially, there is only 1 range vector clock for each node *i*, it covers the entire param key space as its range with 0 as its initial timestamp, each range set may split the range and create at most 3 new vector clocks, click to view(./img/set-vector-clock-to-t-for-range-R-and-node-i.png), let *k* be the total #unique ranges communicated by the algorithm, *m* is #nodes, then there are at most O(mk) vector clocks, here, *k* is typically much smaller than the total #params, this significantly reduces the space required for range vector clocks(*ranges can be also merged to reduce #fragments, however, in practice both *m, k* are small enough to be easily handled); 
**msgs:** nodes may send msgs to individual nodes or node groups, a msg consists of a list of (key,value) pairs in the key range *R* and the associated range vector clock: *\[vc(R),(k1,v1),...,(k_p,v_p)\] k belongstoR and j belongsto{1,...,p}*, this is the basic comm format of the param server not only for shared params but also for tasks, for the latter, a (key,value) pair might assume the form (task ID, arguments or return results); msgs may carry a subset of all available keys within range *R*, the missing keys are assigned the same timestamp without changing their values, a msg can be split by the key range, this happens when a worker sends a msg to the whole server group, or when the key assignment of the receiver node has changed, by doing so, we partition the (key,value) lists and split the range vector clock similar to Set Vector Clock to *t* for Range *R* and Node *i*; because ml problems typically require high bdwidth, msg compression is desirable, training data often remains unchanged bt iterations, a worker might send the same key lists again, hence it is desirable for the receiving node to cache the key lists, later the sender only needs to send a hash of the list rather than the list itself, values, in turn, may contain many zero entries, likewise, [a user-defined filter may also zero out a large fraction of the values, click to view](./img/unique-features-ie-keys-filtered-by-the-Karush-Kuhn-Tucker-filter-as-optimization-proceeds.png), hence we need only send nonzero (key,value) pairs, we use the fast Snappy compression lib to compress msgs, effectively removing the 0s, note that key caching and value-compression can be used jointly; 
**consistent hashing:** the param server partitions keys much as a conventional distributed hash table does :keys and server node IDs are both inserted into the hash ring, each server node manages the key range starting with its insertion point to the next point by other nodes in the counter-clockwise direction, this node is called the master of this key range, a physical server is often represented in the ring via multiple virtual servers to improve load balancing and recovery; we simplify the mgmt by using a direct-mapped DHT design, the server manager handles the ring mgmt, all other nodes cache the key partition locally, this way they can determine directly which server is responsible for a key range and are notified of any changes; 
**replication and consistency:** each server node stores a replica of the *k* counterclockwise neighbor key ranges relative to the one it owns, we refer to nodes holding copies as slaves of the appropriate key range, [click for an example with *k=2*, where server1 replicates the key ranges owned by 2 and 3](./img/server-node-layout.png); worker nodes communicate with the master of a key range for both push and pull, any modification on the master is copied with its timestamp to the slaves, modifications to data are pushed synchronously to the slaves, [click for a case where worker1 pushes *x* into server1, which invokes a user defined function *f* to modify the shared data](./img/replica-generation.png), the push task is completed only once the data modification *f(x)* is copied to the slave; naive replication potentially increases the network traffic by *k* times, this is undesirable for many ml applications that depend on high network bdwidth, the param server framework permits an important optimization for many algorithms :replication after aggregation, server nodes often aggregate data from worker nodes such as summing local gradients, servers may therefore postpone replication until aggregation is complete, here in the righthand side of the diagram, 2 workers push *x, y* to the server respectively, the server first aggregates the push by *x+y*, then applies the modification *f(x+y)* and finally performs the replication, with *n* workers, replication uses only *k/n* bdwidth, here often *k* is a small constant while *n* is hundreds to thousands, while aggregation increases delay of the task reply, it can be hidden by relaxed consistency conditions; 
**server mgmt:** to achieve fault tolerance and dynamic scaling we must support addition and removal of nodes, for convenience we refer to virtual servers below, the following steps happen when a server joins: 1.server manager assigns the new code a key range to server as master, this may cause another key range to split or be removed from a terminated node, 2.the node fetches the range of data to maintain as master and *k* additional ranges to keep as slave, 3.server manager broadcasts the node changes, recipients of the msg may shrink their own data based on key ranges they no longer hold and to resubmit unfinished tasks to the new node; fetching the data in the range *R* from some node *S* proceeds in 2 stages, similar to the Ouroboros protocol, first *S* pre-copies all (key,value) pairs in the range together with associated vector clocks, this may cause a range vector clock to split similar to Set Vector Clock to *t* for Range *R* and Node *i*, if the new node fails at this stage, *S* remains unchanged, at the second stage *S* no longer accepts msgs affecting the key range *R* by dropping msgs without executing and replying, at the same time, *S* sends the new node all changes that occured in *R* during the pre-copy stage; on receiving the node change msg a node *N* first checks if it also maintains the key range *R*, if true and if this key range is no longer to be maintained by *N*, it deletes all associated (key,value) pairs and vector clocks in *R*, next, *N* scans all outgoing msgs that have not received replies yet, if a key range intersects with *R* then the msg will be split and resent; due to delays, failures, and lost acknowledgements *N* may send msgs twice, due to use of vector clocks both the original recipient and the new node are able to reject this msg and it does not affect correctness; the departure of a server node(voluntary or due to failure) is similar to a join, server manager takes a new node with taking the key range of the leaving node, server manager detects node failure by a heartbeat signal, integration with a cluster resource manager such as Yarn, Mesos is left for future work; 
**worker environment:** adding a new worker node *W* is similar but simpler than adding a new server node: 1.task scheduler assigns *W* a range of data, 2.this node loads the range of training data from a network file system or existing workers, training data is often read-only so there is no 2-phase fetch, next, *W* pulls shared params from servers, 3.task scheduler broadcasts the change, possibly causing other workers to free some training dataw; when a worker departs, task scheduler may start a replacement, we give the algorithm designer the option to control recovery for 2 reasons including: if the training data is huge, recovering a worker node be may more expensive than recovering a server node, and losing a small amount of training data during optimization typically affects the model only a little, hence the algorithm designer may prefer to continue without replacing a failed worker, it may even be desirable to terminate the slower workers.
****
### Tail Latency & Interference
### The Tail at Scale
### [Dean13](https://dl.acm.org/doi/pdf/10.1145/2408776.2408794)
### Even rare perf hiccups affect a significant fraction of all reqs in large-scale distribted systems; elliminating all resources of latency variability in large-scale systems is impractical, especially in shared environments; using an approach analogous to fault-tolerant computing, tail-tolerant sw techniques form a predictable whole out of less-predictable parts.
### A simple way to curb latency variability is to issue the same req to multiple replicas and use the results from whichever replica responds first.
systems that respond to user actions quickly(within 100ms) feel more fluid and natural to users than those that take longer, improvements in internet connectivity and rise of warehouse-scale computing systems have enabled web services that provide fluid responsiveness while consulting multiterabyte datasets spanning thousands of servers, for example, the Google search system updates query results interactively as user types, predicting the most likely query based on prefix typed so far, performing the search and showing the results within a few tens of ms; emerging ar devices(such as the Google Glass prototype) will need associated web services with even greater responsiveness in order to guarantee seamless interactivity;
it is challenging for service providers to keep tail of latency distribution short for interactive services as the size and complexity of the system scales up or as overall use increases, temporary high-latency episodes(unimportant in moderate-size systems) may come to dominate overall service perf at large scale, just as fault-tolerant computing aims to create a reliable whole out of less-reliable parts, large online services need to create a predictably responsive whole out of less-predictable parts, we refer to such systems as latency tail-tolerant, or simply tail-tolerant; here, we outline some common causes for high-latency episodes in large online services and describe techniques that reduce their severity or mitigate their effect on whole-system perf, in many cases, tail-tolerant techniques can take advantage of resources already deployed to achieve fault-tolerance, resulting in low additional overhead, we explore how these techniques allow system utilization to be driven higher without lengthening latency tail, hence avoiding wasteful overprovisioning.
### Why Variablity Exists?
variability of response time that leads to high tail latency in individual components of a service can arise for many reasons including: (i)shared resources -machines might be shared by different applications contending for shared resources(such as cpu cores, processor caches, mem bdwidth, and network bdwidth), and within the same application different reqs might contend for resources; (ii)daemons -bg darmons may use only limited resources on average but when scheduled can generate multims hiccups; (iii)global resource sharing -applications running on different machines might contend for global resources(such as network switches and shared file systems); (iv)maintainance activities -bg activities(such as data reconstruction in distributed file systems, periodic log compactions in storage systems like BigTable, and periodic garbage collection in garbage-collected langs) can cause periodic spikes in latency; (v)queueing -multiple layers of queueing in intermediate servers and network switches amplify this variability; (vi)increased variability is also due to several hw trends including: power limits -modern cpus are designed to temporarily run above their average power envelope, mitigating thermal effects by throttling if this activity is sustained for a long period, garbage collection -solid-state storage devices provide very fast random read access, but the need to periodically garbage collect a large #data blocks can increase read latency by a factor of 100 with even a modest level of write activity, energy mgmt -power-saving modes in many types of devices save considerable energy but add additional latency when moving from inactive to active modes.
### Component-Level Variability Amplified by Scale
a common technique for reducing latency in large-scale online services is to parallelize sub-ops across many different machines, where each sub-op is co-located with its portion of a large dataset; parallelization happens by fanning out a req from a root to a large #leaf servers and merging responses via a req-distribution tree, these sub-ops must all complete within a strict deadline for the service to feel responsive; 
variability in the latency distribution of individual components is magnified at the service level, for example, consider a system where each server typically responds in 10ms but with a 99th-percentile latency of 1sec, if a user req is handled on jsut one such server, an user req in 100 will be slow(1sec), [click for an outline of how service-level latency in this hypothetical scenario is affected by very modest fractions of latency outliers](./img/component-level-variability-amplified-by-scale-example.png), if a user req must collect responses from 100 such servers in parallel, then 63% of user reqs will take more than 1sec(marked "x" in the fig), even for services with only one in 10000 reqs experiencing more than 1sec latencies at the single-server level, a service with 2000 such servers will see almost one in five user reqs taking more than 1sec(marked "o" in the fig); table1 lists measurements from a real Google service that is logically similar to this idealized scenario, root servers distribute a req through intermediate servers to a very large #leaf servers, table1 shows the effect of large fan-out on latency distributions, the 99th-percentile latency for a single random req to finish, measured at the root, is 10ms, however, the 99th-percentile latency for all reqs to finish is 140ms, and the 99th-percentile latency for 95% of reqs finishing is 70ms, meaning that waiting for the slowest 5% of reqs to complete is responsible for half of the total 99th-percentile latency, techniques that concentrate on these slow outliers can yield dramatic reductions in overall service perf; 
overprovisioning of resources, careful realtime engineering of sw, and improved reliability can all be used at all levels and in all components to reduce base causes of variability, we next describe general approaches useful for reducing variability in service responsiveness.
### Reducing Component Variability
interactive response-time variability can be reduced by ensuring interactive reqs are serviced in a timely manner through many small engineering decisions including: (i)differentiating service classes and higher-level queueing -differentiated service classes can be used to prefer scheduling reqs for which a user is waiting over non-interactive reqs; keep low-level queues short so higher-level policies take effect more quickly, for example, storage services in Google's cluster-level filesystem sw keep few ops outstandig in os's disk queue, instead maintaining their own priority queues of pending disk reqs, this shallow queue allows servers to issue incoming high-priority interactive reqs before older reqs for latency-intensive batch ops are served; (ii)reducing head-of-line blocking -high-level services can handle reqs with widely varying intrinsic costs, it is sometimes useful for the system to break long-running reqs into a sequence of smaller reqs to allow interleaving of the exec of other short-running reqs, for example, Google's web search system uses such time-slicing to prevent a small #very computationally expensive queries from adding substantial latency to a large #concurrent cheaper queries; (iii)managing bg activities and synchronized disruption -bg tasks can create significant cpu, disk, or network load, examples are log compaction in log-oriented storage systems and garbage-collector activity in garbage-collected langs; a combination of throttling, breaking down heavyweight ops into smaller ops, and triggering such ops at times of lower overall load is often able to reduce effect of bg activities on interactive req latency; for large fan-out services, it is sometimes useful for the system to synchronize the bg activity across many different machines, this synchronization enforces a brief burst of activity on each machine simultaneously, slowing only those interactive reqs being handled during the brief period of bg activity, in contrast, without synchronization a few machines are always doing some bg activity, pushing out the latency tail on all reqs; 
missing in this discussion so far is any reference to caching, while effective caching layers can be useful, even a necessity in some systems, they do not directly address tail latency, aside from configs where it is guaranteed that the entire working set of an application can reside in a cache.
### Living with Latency Variability
the careful engineering techniques in the preceding section are essential for building high-perf interactive services, but the scale and complexity of modern web services make it infeasible to eliminate all latency variability, even if such perfect behavior could be achieved in isolated environments, systems with shared computational resources exhibit perf fluctuations beyond control of application developers, Google has therefore found it advantageous to develop tail-tolerant techniques that mask or work around temporary latency pathologies, instead of trying to eliminate them altogether, we separate these techinques into 2 main classes -the first corresponds to within-req immediate-response techinques that operate at a time scale of tens of ms, before longer-term techniques have a chance to react, the second consists of cross-req long-term adaptations that perform on a time scale of tens of secs to mins and are meant to mask effect of longer-term phenomena.
### Within Req Shrot-Term Adaptations
a broad class of web services deploy multiple replicas of data items to provide additional throughput capacity and maintain availability in the presence of failures, this approach is particularly effective when most reqs operate on largely read-only, loosely consistent datasets, an example is a spelling-correction service that has its model updated once a day while handling thousands of correction reqs per sec, similarly, distributed file systems may have multiple replicas of a given data chunk that can all be used to service read reqs, the techniques here show how replication can also be used to reduce latency variability within a single higher-level req including: 
**hedged reqs:** a simple way to curb latency variability is to issue the same req to multiple replicas and use the results from whichever replica responds first, we term such reqs "hedged reqs" because a client first sends one req to the replica believed to be the most appropriate, but then falls back on sending a secondary req after some brief delay, the client cancels remaining outstanding reqs once the first result is received, although naive implementations of this technique typically add unacceptable additional load, many variations exist that give most of the latency-reduction effects while increasing load only modestly; one such approach is to defer sending a secondary req until the first req has been outstanding for more than the 95th-percentile expected latency for this class of reqs, this approach limites the additional load to approximately 5% while substantially shortening the latency tail, the technique worker because the source of latency is often not inherent in the particular req but rather due to other forms of interference, for example, in a Google benchmark that reads values for 1000 keys stored in a BigTable table distributed across 100 different servers, sending a hedging req after a 10ms delay reduces the 99.9th-percentile latency for retrieving all 1000 values from 1800ms to 74ms while sending just 2% more reqs, the overhead of hedged reqs can be further reduced by tagging them as lower priority than the primary reqs; 
**tied reqs:** the hedged-reqs technique also has a window of vulnerability in which multiple servers can execute the same req unnecessarily, that extra work can be capped by waiting for the 95th-percentile expected latency before issuing the hedged reqs, but this approach limites the benefits to only a small fraction of reqs, permitting more aggressive use of hedged reqs with moderate resource consumption requires faster cancellation of reqs; a common source of variability is queueing delays on the server before a req begins exec, for many services, once a req is actually scheduled and begins exec, the variability of its completion time goes down substantially, it said that allowing a client to choose bt 2 servers based on queue lengths at enqueue time exponentially improves load-balancing perf over a uniform random scheme, we advocate not choosing but rather enqueueing copies of a req in multiple servers simultaneously and allowing servers to communicate upates on the status of these copies to each other, we call reqs where servers perform cross-server status updates "tied reqs", the simplest form of a tied req has the client send the req to 2 different servers, each tagged with the identity of the other server("tied"), when a req begins exec, it sends a cancellation msg to its counterpart, the corresponding req, if still enqueued in the other server, can be aborted immediately or deprioritized substantially; there is a brief window of one average network msg delay where both servers may start executing the req while the cancellation msgs are both in flight to the other server, a common case where this situation can occur is if both server queues are completely empty, it is useful therefore for client to introduce a small delay of 2 times the average network msg delay(1ms or less in modern datacenter networks) bt sending the first req and sending the second req; Google's implementation of this technique is the context of its cluster-level distributed file system is effective at reducing both median and tail latencies, [click for the times for servicing a small read req from a BigTable where the data is not cached in mem but must be read from the underlying file system, each file chunk has 3 replicas on distinct machines](./img/read-latencies-observed-in-a-bigtable-service-benchmark.png), table2 includes read latencies observed with and without tied reqs for 2 scenarios -the first is a cluster in which the benchmark is running in isolation, in which case latency variability is mostly from self-interference and regular cluster-mgmt activities, in it, sending a tied req that does cross-server cancellation to another file system replica following 1ms reduces median latency by 16% and is increasingly effective along the tail of the latency distribution, achieving nearly 40% reduction at the 99.9th-percentile latency, the second scenario is like the first except there is also a large concurrent sorting job running on the same cluster contending for the same disk resources in the shared file system, although overall latencies are somewhat higher due to higher utilization, similar reductions in the latency profile are achieved with the tied-req technique discussed earlier, the latency profile with tied reqs while running a concurrent large sorting job is nearly identical to the latency profile of a mostly idle cluster without tied reqs, tied reqs allow wkloads to be consolidated into a single cluster, resulting in dramatic computing cost reductions, in both table2 scenarios, the overhead of tied reqs in disk utilization is less than 1%, indicating the cancellation strategy is effective at eliminating redundant reads; an alternative to the tied-req and hedged-req schemes is to probe remote queues first, then submit the req to the least-loaded server, it can be beneficial but is less effective than submitting work to 2 queues simultaneously for 3 main reasons including: (i)load levels can change bt probe and req time, (ii)req service times can be difficult to estimate due to underlying system and hw variability, (iii)clients can create temporary hot spots by all clients picking the same(least-loaded) server at the same time; the Distributed Shortest-Positioning Time First system uses another variation in which the req is sent to one server and forwarded to replicas only if the initial server does not have it in its cache and uses cross-server cancellations; worth noting is this technique is not restricted to simple replication but is also applicable in more-complex coding schemes(such as Reed-Solomon) where a primary req is sent to the machine with the desired data block, and, if no response is received following a brief delay, a collection of reqs is issued to a subset of the remaining replication group sufficient to reconstruct the desired data, with the whole ensemble forming a set of tied reqs; note: the class of techniques described here is effective only when the phenomena that causes variability does not tend to simultaneously affect multiple req replicas, we expect such uncorrelated phenomena are rather common in large-scale systems.
### Cross-Req Long-Term Adaptations
here we turn to techniques that are applicable for reducing latency variability caused by coarser-grain phenomena(such as service-time variations and load imbalance), although many systems try to partition data in such a way that the partitions have equal cost, a static assignment of a single partition to each machine is rarely sufficient in practice for 2 reasons -first the perf of the underlying machines is neither uniform nor constant overtime, for reasons(such as thermal throttling and shared wkload interference) mentioned earlier, and, outliers in the assignment of items to partitions can cause data-induced load imbalance(such as when a particular item becomes popular and the load for its partition increases); 
**micro-partitions:** to combat imbalance, many of Google's systems generate many more partitions in the service, then do dynamic assignment and load balancing of these partitions to particular machines, load balancing is then a matter of moving responsibility for one of these small partitions from one machine to another, say, with an average of 20 partitions per machine, the system can shed load in roughly 5% increments and in 1/20th the time it would take if the system simply had a one2one mapping of partitions to machines, the BigTable distributed-storage system stores data in tablets, with each machine managing bt 20-1000 tablets at a time, failure-recovery speed is also improved through micro-partitioning, since many machines pick up one unit of work when a machine failure occurs, this method of using micro-partitions is similar to the virtual servers notion and the virtual-processor-partitioning technique; 
**selective replication:** an enhancement of the micro-partitioning scheme is to detect or even predict certain items that are likely to cause load imbalance and create additional replicas of these items, load-balancing systems can then use additional replicas to spread the load of these hot micro-partitions across multiple machines without having to actually move micro-partitions; Google's web search system uses this approach, making additional components in multiple micro-partitions, at various times in Google's web search system's evolution, it has also created micro-partitions biased toward particular document langs and adjusted replication of these micro-partitions as the mix of query langs changes through the course of a typical day, query mixes can also change abruptly, say, as when an Asian datacenter outage causes a large fraction of Asian-lang queries to be directed to a North America facility, materially changing its working behavior; 
**latency-induced probation:** by observing the latency distribution of responses from the various machines in the system, intermediate servers sometimes detect situations where the system performs better by excluding a particularly slow machine, or putting it on probation, the source of the slowness is frequently temporary phenomena like interference from unrelated networking traffic or a spike in cpu activity for another job on the machine, and the slowness tends to be noticed when the system is under greater load, however, the system continues to issue shadow reqs to these excluded servers, collecting stats on their latency so they can be reincorporated into the service when the problem abates, this situation is somewhat peculiar, as removal of serving capacity from a live system during periods of high load actually improves latency.
### Large IR Systems
in ir -information retrieval systems, speed is more than a perf metric, it is a key quality metric, as returning good results quickly is better than returning the best results slowly, 2 techniques apply to such systems, as well as other to systems that inherently deal with imprecise results: (i)good enough -in large ir systems, once a sufficient fraction of all the leaf servers has responded, user may be best served by being given slightly incomplete("good-enough") results in exchange for better end2end latency, the chance thta a particular leaf server has the best result for the query is less than 1 in 1000 queries, odds further reduced by replicating the most important documents in the corpus into multiple leaf servers, since waiting for exceedingly slow servers might stretch service latency to unacceptable levels, Google's ir systems are tuned to occasionally respond with good-enough results when an acceptable fraction of overall corpus has been searched, while being careful to ensure good-enough results remain rare, in general, good-enough schemes are also used to skip nonessential subsystems to improve responsiveness, for example, results from ads or spelling-correction systems are easily skipped for web searches if they do not respond in time; (ii)canary reqs -another problem that can occur in systems with very high fan-out is that a particular req exercises an untested code path, causing crashes or extremely long delays on thousands of servers simultaneously, to prevent such correlated crash scenarios, some of Google's ir systems employ a technique called canary reqs -rather than initially send a req to thousands of leaf servers, a root server sends it first to 1 or 2 leaf servers, the remaining servers are only queried if the root gets a successful response from the canary in a reasonable period of time, if the server crashes or hangs while the canary req is outstanding, the system flags the req as potentially dangerous and prevents further exec by not sending it to the remaining leaf servers, canary reqs provide a measure of robustness to backends in the face of difficult-to-predict programming errors, as well as malicious denial-of-service attacks; the canary req phase adds only a small amount of overall latency because the system must wait for only a single server to respond, producing much less variability than if it had to wait for all servers to respond for large fan-out reqs, compare the first and last rows in [table1](./img/component-level-variability-amplified-by-scale-example.png), despite the slight increase in latency caused by canary reqs, such reqs tend to be used for every req in all of Google's large fan-out search systems due to the additional safety they provide.
### Mutations
the techniques we have discussed so far are most applicable for ops that do not perform critical mutations of the system's state, which covers a broad range of data-intensive services, tolerating latency variability for ops that mutate state is somewhat easier for a number of reasons including: (i)scale of latency-critical modifications in these services is generally small, (ii)updates can often be performed off the critical path, after responding to user, (iii)many services can be structured to tolerate inconsistent update models for(inherently more latency-tolerant) mutations, (iv)for those services that require consistent updates, the most commonly used techniques are quorum-based algorithms(such as Lamport's Paxos), since these algorithms must commit to only 3-5 replicas, they are inherently tail-tolerant.
****
### Data Lakes & Warehouses
### Lakehouse:A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics
### [Armbrust21](https://www.cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf)
we argue that the data warehouse architecture as we know it today will wither in the coming years and be replaced by a new architectural pattern, the Lakehouse, which will (i)be based on open direct-access data formats, such as Apache Parquet, (ii)have first-class support for mlds, (iii)offer state-of-the-art perf; Lakehouses can help address several major challenges with data warehouses, including data staleness, reliability, total cost of ownership, data lock-in, and limited use case support; we discuss hwo the industry is already moving toward Lakehouses and how this shift may affect work in data mgmt, we also report results from a Lakehouse system using Parquet that is competitive with popular cloud data warehouses on TPC-DS. 
the history of data warehousing started with helping business leaders get analytical insights by collecting data from operational dbs into centralized warehouses, which then could be used for decision support and business intelligence, data in these warehouses would be written with schema-on-write, which ensured that the data model was optimized for downstream BI consumption, we refer to this as the first-gen da platforms; a decade ago, the first-gen systems started to face several challenges including: (i)they typically coupled compute and storage into an on-premises appliance, this forced enterprises to provision and pay for peak of user load and data under mgmt, which became very costly as datasets grew, (ii)not only were datasets growing rapidly, but more and more datasets were completely unstructured, such as video, audio, and text documents, which data warehouses could not store and query at all; to solve these problems, the second-gen da platforms started offloading all raw data into data lakes :low-cost storage systems with a file api that hold data in generic and usually open file formats such as Apache Parquet, this approach started with the Apache Hadoop movement, using the hdfs for cheap storage, the data lake was a schema-on-read architecture that enabled agility of storing any data at low cost, but on the other hand, punted the problem of data quality and governance downstream, in this architecture, a small subset of data in the lake would later be ETLed to a downstream data warehouse such as Teradata for the most important decision support and BI applications, the use of open formats also made data lake data directly accessible to a wide range of other analytics engines such as ml systems; from 2015 onwards, cloud data lakes, such as S3, ADLS, GCS, started replacing hdfs, they have superior durability(often >10 nines), geo-replication, and extremely low cost with the possibility of automatic, even cheaper, archival storage, such as AWS Glacier, the rest of the architecture is largely the same in the cloud as in the second-gen systems, with a downstream data warehouse such as Redshift, Snowflake, this 2-tier data lake + warehouse architecture is now dominant in the industry in our experience(used at virtually all Fortune500 enterprises); this brings us to challenges with current data architectures, while cloud data lake and warehouse architecture is ostensibly cheap due to separate storage(such as S3) and compute(such as Redshift), a 2-tier architecture is highly complex for users :in the first-gen platforms all data was ETLed from operational data systems directly into a warehouse, vs., in today's architectures data is first ETLed into lakes and then again ETLed into warehouses, creating complexity, delays, and new failure modes, moreover, enterprise use cases now include advanced analytics such as ml, for which neither data lakes nor warehouses are ideal, specifically, today's data architectures commonly suffer from 4 problems including: (i) reliability: keeping data lake and warehouse consistent is difficult and costly, continuous engineering is required to ETL data bt the 2 systems and make it available to high-perf decision support and BI, each ETL step also risks incurring failures or introducing bugs that reduce data quality, such as due to subtle difference bt data lake and warehouse engines; 
(ii) data staleness: data in the warehouse is stale compared to that of data lake, with new data frequently taking days to load, this is a step back compared to the first-gen da systems, where new operational data was immediately available for queries, according to a survey by Dimensional Research and Fivetran, 86% of analysts use out-of-date data and 62% report waiting on engineering resources numerous times per month; 
(iii) limited support for advanced analytics: businesses want to ask predictive questions using their warehouse data, such as "which customers should I offer discounts to?" despite much research on the confluence of ml and data mgmt, none of the leading ml systems, such as Tensorflow, Pytorch, and XGBoost, work well on top of warehouses, unlike BI queries, which extract a small amount of data, these systems need to process large datasets using complex nonsql code, reading this data via ODBC/JDBC is inefficient, and there is no way to directly access the internal warehouse proprietary formats, for these use cases, warehouse vendors recommend exporting data to files, which further increases complexity and staleness(adding a third ETL step), alternatively, users can run these systems against data lake data in open formats, however, they then lose rich mgmt features from data warehouses, such as ACID transactions, data versioning and indexing; 
(iv) total cost of ownership: apart from paying for continuous ETL, users pay double the storage cost for data copied to a warehouse, and commercial warehouses lock data into proprietary formats that increase cost of migrating data or wkloads to other systems; 
a straw-man solution that has had limited adoption is to eliminate data lake altogether and store all data in a warehouse that has built-in separation of compute and storage, we will argue that this has limited variability as evidenced by lack of adoption, because it still does not support managing video/audio/text data easily or fast direct access from mlds wkloads; 
here we discuss the following technical question :is it possible to turn data lakes based on standard open data formats such as Parquet into high-perf systems that can provide both perf and mgmt features of data warehouses and fast direct io from advanced analytics wkloads? we argue that this type of system design, which we refer to as a [Lakehouse](./img/lakehouse.png), is both feasible and is already showing evidence of success, in various forms, in the industry, as more business applications start relying on operational data and on advanced analytics, we believe Lakehouse is a compelling design point that can eliminate some of the top challenges with data warehousing; in particular, we believe that time for Lakehouse has come due to recent solutions that address the following key problems including: (i) reliable data mgmt on data lakes: a Lakehouse needs to be able to store raw data, similar to today's data lakes, while simultaneously supporting ETL/ELT processes that curate this data to improve its quality for analysis, traditionally, data lakes have managed data as "just a bunch of files" in semi-structured formats, making it hard to offer some of the key mgmt features that simplify ETL/ELT in data warehouses, such as transactions, rollbacks to old table versions, and zero-copy cloning, however, a recent family of systems such as DeltaLake, Apache Iceberg, provide transactional views of a data lake, and enable these mgmt features, of course, orgs still have to do the hard work of writing ETL/ELT logic to create curated datasets with a Lakehouse, but there are fewer ETL steps overall, and analysts can also easily and performantly query raw data tables if they wish to, much like in first-gen analytics platforms; 
(ii) support for mlds: ml systems' support for direct access from data lake formats already places them in a good position to efficiently access a Lakehouse, in addition, many ml systems have adopted DataFrames as the abstraction for manipulating data, and recent systems have designed declarative DataFrame apis that enable performing query optimizations for data accesses in ml wkloads, these apis enable ml wkloads to directly benefit from many optimizations in Lakehouses; 
(iii) sql perf: Lakehouses will need to provide state-of-the-art sql perf on top of the massive Parquet datasets that have been amassed over the last decade(or in the long term, some other standard format that is exposed for direct access to applications), in contrast, classic data warehouses accept sql and are free to optimize everything under the hood, including proprietary storage formats, nonetheless, we show that a variety of techniques can be used to maintain auxiliary data about Parquet datasets and to optimize data layout within these existing formats to achieve competitive perf, we present results from a sql engine over Parquet(the Databricks Delta Engine) that outperforms leading cloud data warehouses on TPC-DS.
### Motivation :Data Warehousing Challenges
data warehouses are critical for many business processes, but they still regularly furstrate users with incorrect data, staleness, and high costs, we argue that at least part of each these challenges is "accidental complexity" from the way enterprise data platforms are designed, which could be eliminated with a Lakehouse; 
first, the top problem reported by enterprise data users today is usually data quality and reliability, implementing correct data pipelines is intrinsically difficult, but today's 2-tier data architectures with a separate lake and warehouse add extra complexity that exacerbates this problem, for example, data lake and warehouse systems might have different semantics in their supported data types, sql dialects, etc., data may be stored with different schemas in the lake and the warehouse(such as denormalized in one), and the increased #ETL/ELT jobs, spanning multiple systems, increases the prob of failures and bugs; 
second, more and more business applications require up2date data, but today's architectures increase data staleness by having a separate sharing area for incoming data before the warehouse and using periodic ETL/ELT jobs to load it, theoretically, orgs could implement more streaming pipelines to updata data warehouse faster, in the first-gen platforms warehouse users had immediate access to raw data loaded from operational systems in the same environment as derived datasets, business applications such as customer support systems and recommendation engines are simply ineffective with stale data, and even human analysts querying warehouses and their api do not easily support it; 
finally, most orgs are now deploying mlds applications, but these are not well served by data warehouses and lakes, as discussed before, these applications need to process large amounts of data with nonsql code, so they cannot run effectively over ODBC/JDBC, as advanced analytics systems continue to develop, we believe that giving them direct access to data in an open format will be the most effective way to support them, in addition, mlds applications suffer from the same data mgmt problems that classical applications do, such as data quality, consistency, and isolation, so there is immense value in bringing dbms features to their data; 
several currect industry trends give further evidence that customers are unsatisfied with the 2-tier lake + warehouse model including: first, in recent years virtually all major data warehouses have added support for external tables in Parquet format, this allows warehouse users to also query the data lake tables easier to manage and it does not remove ETL complexity, staleness, and advanced analytics challenges for data in the warehouse, in practice, these connectors also often perform poorly because sql engine is mostly optimized for its internal data format, second, there is also broad investment in sql engines that run directly against data lake storage, such as SparkSQL, Presto, Hive, and AWS Athena, however, these engines alone cannot solve all problems with data lakes and replace warehouses :data lakes still lack basic mgmt features such as ACID transactions and efficient access methods such as indexes to match data warehouse perf.
### Lakehouse Architecture
we define a Lakehouse as a data mgmt system based on low-cost and directly-accessible storage that also provides traditional analytical dbms mgmt and perf features such as ACID transactions, data versioning, auditing, indexing, caching, and query optimization, Lakehouses thus combine key benefits of data lakes and data warehouses :low-cost storage in an open format accessible by a variety of systems from the former, and powerful mgmt and optimization features from the latter, they key question is whether one can combine these benefits in an effective way :in particular, Lakehouses' support for direct access means that they give up some aspects of data independence, which has been a cornerstone of relational dbms design; 
we note that Lakehouses are an especially good fit for cloud environments with separate compute and storage :different computing applications can run on-demand on completely separate computing nodes(such as a gpu cluster for ml) while directly accessing the same storage data, however, one coudl also implement a Lakehouse over an on-premise storage system such as hdfs; 
in this section, we sketch one possible design for Lakehouse systems, based on 3 recent technical ideas that have appeared in various forms throughout the industry, we have been building towards a Lakehouse platform based on this design at Databricks through the Delta Lake, Delta Engine, and Databricks ML Runtime projects, other designs may also be visible, however, as are other concrete technical choices in our high-level design(such as our stack at Databricks currently builds on the Parquet storage format, but it is possible to design a better format), we discuss several alternatives and future directions for research; 
**implementing a Lakehouse system:** the first key idea we propose for implementing a Lakehouse is to have the system store data in a low-cost object store(such as Amazon S3) using a standard file format such as Apache Parquet, but implement a transactional metadata layer on top of the object store that defines which objects are part of a table version, this allows the system to implement mgmt features such as ACID transactions or versioning within the metadata layer, while keeping the bulk of data in low-cost object store and allowing clients to directly read objects from this store using a standard file format in most cases, several recent systems including DeltaLake, Apache Iceberg, have successfully added mgmt features to data lakes in this fashion, for example, DeltaLake is now used in about half of Databrick's wkload, by thousands of customers; although a metadata layer adds mgmt capabilities, it is not sufficient to achieve good sql perf, data warehouses use several techniques to get state-of-the-art perf such as storing hot data on fast devices such as SSDs, maintaining stats, building efficient access methods such as indexes, and co-optimizing data format and compute engine; in a Lakehouse based on existing storage formats, it is not possible to change the format, but we show that it is possible to implement other optimizations that leave the data files unchanged, including caching, auxiliary data structures such as indexes and stats, and data layout optimizations; finally, Lakehouses can both speed up advanced analytics wkloads and give them better data mgmt features thanks to the dev of declarative DataFrame APIs, many ml libs such as Tensorflow, Spark MLlib, can already read data lake file formats such as Parquet, hence the simplest way to integrate them with a Lakehouse would be to query the metadata layer to figure out which Parquet files are currently part of a table, and simply pass those to the ml lib, however, most of these systems support a DataFrame API for data preparation that creates more optimization opportunities; DataFrames were popularized by R and Pandas and simply give users a table abstraction with various transformation operators, most of which map to relational algebra, systems such as SparkSQL have made this api declarative by lazily evaluating transformations and passing the resulting operator plan to an optimizer, these apis can thus leverage the new optimization features in a Lakehouse, such as caches and auxiliary data, to further accelerate ml; [click for how these ideas fit together into a Lakehouse system design](./img/lakehouse-system-design.png); 
**metadata layers for data mgmt:** the first component that we believe will enable Lakehouse is metadata layers over data lake storage that can raise its abstraction level to implement ACID transactions and other mgmt features, data lake storage systems such as S3 or hdfs only provide a low-level object store or filesystem interface where even simple ops such as updating a table that spans multiple files, are not atomic; orgs soon began designing richer data mgmt layers over these systems, starting with Apache Hive ACID, which tracks which data files are part of a hive table at a given table version using an OLTP DBMS and allows ops to update this set transactionally; in recent years, new systems have provided even more capabilities and improved scalability, in 2016 Databricks began developing DeltaLake, which stores info about which objects are part of a tabel in the data lake itself as a transaction log in Parquet format, enabling it to scale to billions of objects per table, Apache Iceberg which started at Netflix, uses a similar design and supports Parquet storage, Apache Hudi which started at Uber, is another system in this area focused on simplifying streaming ingest into data lakes, although it does not support concurrent writers; experience with these systems has shown that they generally provide similar or better perf to raw Parquet data lakes, while adding highly useful mgmt features such as transactions, zero-copy coning, and time travel to past versions of a table, in addition, they are easy to adopt for orgs that already have a data lake, for example, DeltaLake can convert an existing dir of Parquet files into a DeltaLake table with zero copies just by adding a transaction log that starts with an entry that references all existing files, as a result, orgs are rapidly adopting these metadata layers, for example, DeltaLake grew to cover half the compute-hrs on Databricks in 3 yrs; in addition, metadata layers are a natural place to implement data quality enforcement features, for example, DeltaLake implements schema enforcement to ensure that data uploaded to a table matches its schema, and constraints api that allows table owners to set constraints on the ingested data(such as country can only be one of a list of values), Delta's client libs will automatically reject records that violate these expectations or quarantine them in a special location, customers have found these simple features very useful to improve quality of data lake based pipelines; finally, metadata layers are a natural place to implement governance features such as access control and audit logging, for example, a metadata layer can check whether a client is allowed to access a table before granting it credentials to read raw data in the table from a cloud object store, and can reliably log all accesses; 
**future directions and alternative design:** because metadata layers for data lakes are a fairly new dev, there are many open questions and alternative designs, for example, we designed DeltaLake to store its transaction log in the same object store that it runs over(such as S3) in order to simplify mgmt(removing the need to run a separate storage system) and offer high availability and high read bdwidth to the log(the same as the object store), however, this limits the rate of transactions per sec it can support due to object stores' high latency, a design using a faster storage system for the metadata may be preferable in some cases, likewise, DeltaLake, Iceberg, and Hudi only support transactions on 1 table at a time, but it should be possible to extend them to support cross-table transactions, optimizing the format of transaction logs and size of objects managed are also open questions; 
**sql perf in a Lakehouse:** perhaps the largest technical question with the Lakehouse approach is how to provide state-of-the-art sql perf while giving up a significant portion of the data independence in a traditional dbms design, the answer clearly depends on a number of factors including: what hw resources we have available(such as can we implement a caching layer on top of the object store), and whether we can change the data object storage format instead of using existing standards such as Parquet(new designs that improve over these formats continue to emerge), however, regardless of the exact design the core challenge is that the data storage format becomes part of the system's public api to allow fast direct access, unlike in a traditional dbms; we propose several techniques to implement sql perf optimizations in a Lakehouse independent of the chosen data format, which can therefore be applied either with existing or future formats, we have also implemented these techniques within the Databricks Delta Engine, and show that they yield competitive perf with popular cloud data warehouses, though there is plenty of room for further perf optimizations, these format-independent optimization are: (i) caching: when using a traditional metadata layer such as DeltaLake, it is safe for a Lakehouse system to cache files from cloud object store on faster storage devices suhc as SSDs and RAM on the processing nodes, running transactions can easily determine when cached files are still valid to read, moreover, cache can be in a transcoded format that is more efficient for the query engine to run on, matching any optimization that would be used in a traditional "closed-world" data warehouse engine, for example, our cache at Databricks partially decompresses the Parquet data it loads; 
(ii) auxiliary data: even though a Lakehouse needs to expose the base table storage format for direct io, it can maintain other data that helps optimize queries in auxiliary files that it has full control over, in DeltaLake and Delta Engine, we maintain column min-max stats for each data file in the table within the same Parquet file used to store the transaction log, which enables data skipping optimizations when the base data is clustered by particular columns, we are also implementing a Bloom filter based index, one can imagine implementing a wide range of auxiliary data structures here, similar to proposals for indexing "raw" data; 
(iii) data layout: data layout plays a large role in access perf, even when we fix a storage formate such as Parquet, there are multiple layout decisions that can be optimized by Lakehouse system, the most obvious is recorder ordering -which records are clustered together and hence easiest to read together, in DeltaLake we support ordering records using individual dims or space-filling curves such as Z-order, Hilbert curves, to provide locality across multiple dims, one can also imagine new formats that support placing columns in different orders within each data file, choosing compression strategies differently for various groups of records, or other strategies; these 3 optimizations work especially well together for the typical access patterns in analytical systems, in typical networks most queries tend to be concentrated against a hot subset of data, which Lakehouse can cache using the same optimized data structures as a closed-world data warehouse to provide competitive perf; for cold data the a cloud object store, the main determinant of perf is likely to be the amount of data read per query, in that case, the combination of data layout optimizations(which cluster co-accessed data) and auxiliary data structures such as zone maps(which let the engine rapidly figure out what ranges of data files to read) can allow a Lakehouse system to minimize io the same way a closed-world proprietary data warehouse would, despite running against a standard open file format; 
(iv) perf results: at Databricks we combined these 3 Lakehouse optimizations with a new C++ exec engine for Apache Spark called Delta Engine, to evaluate the feasibility of Lakehouse architecture, [click for comparing Delta Engine on TPC-DS at scale factor 30000 with 4 widely used cloud data warehouses(from cloud providers as well as third-party companies that run over public clouds)](./img/delta-engine.png), using comparable clusters on AWS, Azure, GCP with 960 vGPUs each and local SSD storage(*we started all systems with data cached on SSDs when applicable, because some of the warehouses we compared with only supported node-attached storage, however, Delta Engine was only 18% slower when starting with a cold cache), we report the time to run all 99 queries as well as total cost for customers in each service's pricing model(Databricks lets users choose spot and on-demand instances, so we show both), Delta Engine provides comparable or better perf than these systems at a lower price point; 
(v) future directions and alternative designs: designing performant yet directly-accessible Lakehouse systems is a rich area for future work, one clear direction that we have not explored yet is designing new data lake storage formats that will work better in this use case, such as formats that provide more flexibility for Lakehouse system to implement data layout optimizations or indexes over or are simply better suited to modern hw, of course, such new formats may take a while for processing engines to adopt, limiting #clients that can read from them, but desiging a high quality directly-accessible open fortmat for next gen wkloads is an important research problem; even without changing data format, there are many types of caching strategies, auxiliary data structures, and data layout strategies to explore for Lakehouses, determining which ones are likely to be most effective for massive datasets in cloud object stores is an open question; finally, another exicting research direction is determining when and how to use serverless computing systems to answer queries and optimizing the storage, metadata layer, and query engine designs to minimize latency in this case; 
**efficient access for advanced analytics:** (i) as we discussed earlier, advanced analytics libs are usually written using imperative code that cannot run as sql, yet the need to access large amount of data, there is an interesting research question in how to design data access layers in these libs to maximize flexibility for the node running on top but still benefit from optimization opportunities in a Lakehouse; one approach that we have had success with is offering a declarative version of DataFrame APIs used in such libs, which maps data preparation computations into SparkSQL query plans and can benefit from optimizations in DeltaLake and Delta Engine, we used this approach in both Spark DataFrames and in Koalas, a new DataFrame API for Spark that offers improved compatibility with Pandas, DataFrames are the main data type used to pass input into the ecosystem of advanced analytics libs for Apache Spark, including MLlib, GraphFrames, SparkR, and many community libs, so all of these wkloads can enjoy accelerated io if we can optimize the DataFrame computation; Spark's query planner pushes selections and projections in the user's DataFrame computation directly into the "data source" plugin class for each data source read, [hence in our implementation of DeltaLake data source, we leverage caching, data skipping, and data layout optimizations described in section sql perf in a Lakehouse to accelerate these reads from DeltaLake and thus accelerate mlds wkloads, click to view](./img/dataframe-api.png); however, ml apis are quickly evolving, and there are also other data access apis, such as Tensorflow's tf.data, that do not attempt to push query semantics into underlying storage system, many of these apis also focus on overlapping data loading on the cpu with cpu2gpu transfers and gpu computation, which has not received much attention in data warehouses, recent systems work has shown that keeping modern accelerators well-utilized, especially for ml inference, can be a difficult problem, so Lakehouse access libs will need to tackle this challenge; 
(ii) future directions and alternative designs: apart from the questions about existing apis and efficiency that we have just discussed, we can explore radically different designs for data access interfaces from ml, for example, recent work has proposed factorized ml frameworks that push ml logic into sql joins, and other query optimizations that can be applied for ml algorithms implemented in sql; finally, we still need standard interfaces to let data scientists take full advantage of powerful data mgmt capabilities in Lakehouse(or even data warehouses), for example at Databricks we have integrated DeltaLake with ml experiment tracking service in MLflow to let data scientists easily track the table versions used in an experiment and reproduce that version of data later; there is also an emerging abstraction of feature stores in the industry as a data mgmt layer to store and update features used in ml application, which would benefit from using the standard dbms functions in a Lakehouse design, such as transactions and data versioning.
### Research Questions and Implications
beyond research challenges that we raised as future directions, Lakehouse raise several other research questions, in addition, the industry trend towards increasingly feature-rich data lakes has implications for other areas of data systems research; 
**are there other ways to achieve Lakehouse goals?** one can imagine other means to achieve primary goals of Lakehouse such as building a massively parallel serving layer for a data warehouse that can support parallel reads from advanced analytics wkloads, however, we believe that such infra will be significantly more expensive to run, harder to manage, and likely less performant than giving wkloads direct access to the object store; we have not seen broad deployment of systems that add this type of serving layer, such as Hive LLAP, moreover, this approach punts the problem of selecting an efficient data format for reads to the serving layer, and this format still needs to be easy to transcode from warehouse's internal format, the main draws of cloud object stores are their low cost, high bdwidth access from elastic wkloads, and extremely high availability, all these get worse with a separate serving layer in front of the object store; beyond the perf, availability, cost, and lock-in challenges with these alternative approaches, there are also important governance reasons why enterprises may prefer to keep their data in an open format, with increasing regulatory requirements about data mgmt, orgs may need to search through old datasets, delete various data, or change their data processing infra on short notice, and standardizing on an open format means that they will always have direct access to data without blocking on a vendor, the long-term trend in sw industry has been towards open data formats, and we believe that this trend will continue for enterprise data; 
**what are the right storage formats and access apis?** the access interface to a Lakehouse includes raw storage format, client libs to directly read this format(such as when reading into Tensorflow), and a high-level sql interface, there are many different ways to place rich functionality across these layers, such as storage schemes that provide more flexibility to the system by asking readers to perform more sophisticated, programmable decoding logic; it remains to be seen which combination of storage formats, metadata layer designs, and access apis works best; 
**how does Lakehouse affect other data mgmt research and trends?** the prevalence of data lakes and the increasing use of rich mgmt interfaces over them, whether they be metadata layers or the full Lakehouse design, has implications for several other areas of data mgmt research including: (i)polystores were designed to solve the difficult problem of querying data across disparate storage engines, this problem will persist in enterprises, but the increasing fraction of data that is available in an open format in a cloud data lake means that many polystore queries could be answered by running directly against the cloud object store, even if the underlying data files are part of logically separate Lakehouse deployments; (ii)data integration and cleaning tools can also be designed to run in place over a Lakehouse with fast parallel access to all the data, which may enable new algorithms such as running large joins and clustering algorithms over many of the datasets in an org; (iii)HTAP systems could perhaps be built as bolt-on layers in front of a Lakehouse by archieving data directly into a Lakehouse system using it transaction mgmt apis, the Lakehouse would be able to query consistent snapshots of the data; (iv)data mgmt for ml may also become simpler and more powerful if implemented over a Lakehouse, today, orgs are building a wide range of ml-specific data versioning and feature store systems that implement standard dbms functionality, it might be simpler to just use a lake house abstraction with dbms mgmt functions built-in to implement feature store functionality, at the same time, declarative ml systems such as factorized ml could likely run well against a Lakehouse; (v)cloud-native dbms designs such as serverless engines will need to integrate with richer metadata mgmt layers such as DeltaLake instead of just scanning over raw files in a data lake, but may be able to achieve increased perf; (vi)finally, there is ongoing discussion in the industry about how to organize data engineering processes and teams, with concepts such as the data mesh, where separate teams own different data products end2end, gaining popularity over traditional central data team approach, Lakehouse designs lend themselves easily to distributed collaboration structures because all datasets are directly accessible from an object store without having to onboard users on the same compute resources, making it straightforward to share data regardless of which teams produce and consume it.
****
### Geo-Replication
### Stronger Consistency and Semantics for Low-Latency Geo-Replication Storage
### [Lloyd13](https://www.cs.princeton.edu/~wlloyd/papers/PhdThesis.pdf)
### Only Chapters 1-3 from Lloyd13.
geo-replicated, distributed data stores that support complex online applications, such as social networks, strive to provide an always-on experience where ops always successfully complete with low latency, today's systems sacrifice strong consistency and expressive semantics to achieve these goals, exposing inconsistencies to their clients and necessitating complex application logic, here we identify and define a consistency model -causal consistency with convergent conflict handling, or causal+ -that is the strongest achieved under these constraints; 
we explore ramifications of and implementation techniques for causal+ consistency through design and implementation of 2 storage systems :COPS, Eiger, here COPS is a key-value store that provides causal+ consistency, a key contribution of COPS is its scalability, which can enforce causal dependencies bt keys stored across an entire cluster, rather than a single server like previous systems, Eiger is a system that provides causal+ consistency for the rich column-family data model, the central approach in COPS, Eiger is tracking and explicitly checking that causal dependencies bt keys are satisfied in the local cluster before exposing writes; 
we also provide stronger semantics, in the form of read-only and write-only transactions, that allow programmers to consistently interact with data spread across many servers, COPS includes the first scalable read-only transactions that can optimistically complete in 1rd of local reads and take at most 2 non-blocking rds of local reads, Eiger includes read-only transactions with the same properties that are more general -they are applicable to non-causal systems -and more fault-tolerant, Eiger also includes scalable write-only transactions that take 3rds of local non-blocking comm and do not block concurrent read-only transactions; 
we implemented COPS, Eiger and desmonstrate their perf experimentally, the later of these systems, Eiger, achieves low latency, has throughput competitive with an eventually-consistent and non-transactional production system, and scales to large clusters almost linearly. 
large-scale data stores are a critical infra component in many of today's largest internet services, these services supoort many millions of concurrent users interacting with many petabytes of data, the extreme scale of the infra needed to support these services provides a new environment to explore distributed storage designs, [in contrast to classical distributed storage systems that focus on local-area op in the small, these services are typically characterized by massive wide-area deployment across a few to tens of datacenters, click to view](./img/the-general-architecture-of-modern-web-services.png); 
in these systems each datacenter stores a full replica of data, i.e. there is a copy of each data item in each datacenter(*extending this work to settings where data may be only replicated over some some subset of datacenters is an interesting avenue of future work), for example, Facebook stores all user profiles, comments, friends lists, and likes at each of its datacenters, replicating data in this way across geographically distinct locations is called geo-replication, geo-replication brings 2 key benefits to web services :fault tolerance and low latency, it provides fault tolerance through redundancy :if one location fails, other locations can continue to provide the service, it provides low latency through proximity :users can de directed to and served by a nearby location to avoid speed-of-light delays associated with cross-country or round-the-globe comm; 
the scale of data stored at each datacenter is far too large to fit on a single physical machine, instead, it must be spread across clusters of 10s-1000s of machines, a typical technique for spreading data is sharding, which puts disjoint subsets of data on different servers, as a primitive example, Server1 might store and serve user profiles for people whose names start with A, Server2 for B, and so on, such sharding allows clusters to scale in capacity and throughput, adding additional machines to a cluster results in a rebalancing of shards across the new larger set of servers, each server in the new cluster is then responsible for a smaller shard of the data, this allows the throughput and capacity of a cluster to increase, or scale, with the addition of more machines, the storage systems that support these large web services are thus doubly distributed systems, replicas are distributed geographically and each is itself distributed across many machines via sharding; 
when users initiate a connection with a service they are directed to a nearby datacenter, within that datacenter users are a connected one to many identical frontend web servers, frontends serve reqs by reading and writing data to and from storage tier nodes(in a 3-tier web service the web server and application logic are considered separate tiers, we describe a 2-tier web service where they are combined in this dissertation, but all discussion applies in the 3-tier model as well with the application logic tier as out client), for the storage system, clients are the frontend web servers that issue read and write ops on behalf of human users, when we say, "a client writes a value", we mean that an application running on a web or application server writes into the storage system; 
in this dissertation, we address the problem of building a geo-replicated data store targeted at applications that demand low latency, such applications are common :Amazon, eBay, Google all claim that a slight increase in user-perceived latency translates into concrete revenue loss, user-perceived latency consists of many components, these include, at a minimum, (i)the network delay to the service instance a user is connected to, (ii)processing time on frontend web server that accepts the user's req, (iii)one or more rds of accesses to the storage system, (iv)additional processig time on frontend web server, (v)the network delay for the response to travel back to the user, (vi)user-side rendering time; perf goals vary across services, but typical targets are in the hundreds or low thousands of ms; 
these low page-load-time targets help demonstrate the latency benefit of geo-replication :when less time is used for user2service&back delay(also called rd trip time or rtt) it easier to meet these targets, [click for some examples rtts from a user in Princeton to servers around the world](./img/rtts-example.png), clearly, the page load time for a Princeton user will be much faster when accessing a service instance in NYC(8ms rtt), than when accessing one in LA(72ms rtt) or Sydney(240ms rtt); 
given the low targets for page load time and the many components that add to it, it is imperative that accesses to the data store have low latency, to this end, many storage systems -including those described in this dissertation -serve all accesses from the local replica of the data store, reads return a current view of data from this replica, they do not examine remote replicas, writes are applied at this replica immediately, they return before being applied at remote replicas; 
the local-replica-only design helps ensure low latency by avoiding slow accesses to remote replicas, which would take at least the rtt bt datacenters, [table1.1](./img/rtts-example.png) is again a good reference point for the magnitude of these rtts that are avoided, however, the local-replica-only design precludes providing the strongest forms of consistency, which restricts exec of ops to make the data store more intuitive for users and programmers, for example, linearizability, a strong consistency model, specifies that a read issued after a write returns must see effects of that write, linearizability is clearly incompatibe with the local-replica-only design because writes return after being written to their local replicas, but a read issued to a different replica instantaneously after that write returns must see its effects even though it is impossible for them to have propagated to the other replicas yet due to speed-of-light delay, theoretical results formalize the trade-off bt low latency and the strongest forms of consistency; 
the strongest forms of consistency are also at odds with other desirable properties of geo-replicated storage systems, the famous CAP theorem shows it is impossible to create a system that is linearizable, always available for reads and writes, and able to continue operating during network partitions, as a result, many modern web services have chosen to embrace availability and partition tolerance at the cost of linearizability, we refer to scalable systems with these 3 properties as ALPS systems, because they provide availability, low latency, partition-tolerance, and high scalability, we define the ALPS properties more rigorously and give an in-depth discussion of trade-offs with consistency in future section ALPS Systems and Tradeoffs; 
given that ALPS systems must sacrifice strong consistency(i.e. linearizability), we seek the strongest consistency model that is achievable under these constraints, such stronger consistency is desirable because it makes systems easier for a programmer to reason about and exposes fewer anomalies to end users of the system, here we consider causal consistency with convergent conflict handling, which we refer to as causal+ consistency, many previous systems believed to impelement the weaker causal consistency, such as Bayou, PRACTI actually implement the more useful causal+ consistency, through none do so in a scalable manner; 
the causal component of causal+ consistency ensures that the data store respects the causal dependencies bt ops, consider a scenario where a user uploads a picture to a website, the picture is saved, and then a reference to it is added to that user's album, the reference depends on the picture being saved, under causal+ consistency, these dependencies are always satisfied, programmers never need to reason about the situation where they can get the reference to the picture but not the picture itself, unlike in systems with weaker guarantees, such as eventual consistency; 
the convergent conflict handling component of causal+ consistency ensures that replicas never permanently diverge and that conflicting updates to the same key are dealt with identically at all sites, when combined with causal consistency, this property ensures that clients see only progressively newer versions of keys, in comparison, eventually consistent systems may expose versions out of order, by combining causal consistency and convergent conflict hanlding, causal+ consistency ensures clients see a causally-correct, conflict-free, and always-progressing data store; 
beyond causal+ consistency, we seek stronger semantics from out data store that give programmers more powerful tools for interacting with data, these stronger semantics include a rich data model, read-only transactions, and write-only transactions; 
key-value storage -perhaps the simplest data model provided by data stores -is used today by many services, however, this data model does not provide the abstractions typically used to build web services such as counters, lists, and graphs, in contrast, the column-family data models offered by systems like BigTable, Cassandra, do provide these abstractions, these rich data models provide hierarchical sorted column-families and numerical counters, column-families are well matched to many services such as Facebook, while counter columns are particularly useful for numerical stats, as used by collaborative filtering(Digg, Reddit), likes(Facebook), or re-tweets(Twitter), here we demonstrate how to provide causal+ consistency for the simple key-value data model and the rich column-family data model; 
in a scalable setting where data is spread across many machines, asynchronous reqs to a consistently-updated data store are insufficient for providing consistency to clients of the system, simultaneously, issued reqs to different servers may arrive at different times, hence returning distinct and potential inconsistent views of data, instead, what is needed are read-only transactions that provide programmers with a consistent and up2date view of data spread across many machines in the system, write-only transaction similarly benefit programmers by enabling them to atomically update data spread across many machines in the system; 
in this dissertation we describe 3 systems that provide causal+ consistency :COPS, COPS-RT, Eiger, each processing system is a step forward for scalable geo-replicated storage, with Eiger supersending our original efforts on COPS, COPS-RT; 
our COPS -cluster of order-preserving servers system provides causal+ consistency and is designed to support complex online applications that are hosted from a small #large-scale datacenters, each of which is composed of frontend servers(clients of COPS) and backend key-value data stores, COPS executes all read and write ops in the local datacenter in a linearizable fashion, and then replicates data across datacenters in a causal+ consistent order in the background; 
COPS-RT(the RT in COPS-RT stands for read-only transactions) builds on COPS by adding read-only transactions that give clients a consistent view of multiple keys spread across many servers, read-only transactions are needed to obtain a consistent view of multiple keys, even in a fully-linearizable system, our read-only transactions require no locks, are non-blocking, and take at most 2 parallel rds of intra-datacenter reqs, these read-only transactions do come at some cost :compared to the regular version of COPS, COPS-RT is less efficient fro cetain wkloads(such as write-heavy) and is less robust to long network partitions and transient datacenter failures; 
our Eiger system continues this line of work by providing stronger semantics for ALPS systems, these stronger semantics include providing causal+ consistency for the rich column-family data model, improved read-only transactions, and write-only transactions, Eiger's read-only and write-only transaction algorithms each represent an advance in the state-of-the-art; COPS introduces a read-only transaction algorithm that normally completes in 1rd of local reads, and 2rds in the worst case, Eiger's read-only transaction algorithm has the same properties, but achieves them using logical time instead of expicit dependencies, not storing explicit dependencies allows Eiger to avoid drawbacks of COPS-RT, Eiger is as efficient as COPS for all wkloads and is robust to long network partitions and transient datacenter failures; 
the scalability requirements for ALPS systems creates the largest distinction bt this work and prior causal+ consistent systems, previous systems required that all data fit on a single machine or that all data that potentially could be accessed together fit on a single machine, in comparision, data stored in COPS, Eiger can be spread across an arbitrarily-sized datacenter, with dependencies, read-only, and write-only transaction that can sketch across many servers in the datacenter, to the best of our knowledge, our work describes the first scalable systems to implement causal+(and thus causal) consistency.
### ALPS Systems and Tradeoffs
distributed storage systems have multiple, sometimes competing goals :avaiability, low latency, and partition tolerance to provide an always on ux, scalability to adpat to increasing load and storage demands, and a sufficiently strong consistency model to simplify programming and provide users with the system behavior that they expect, in slightly more depth, the desirable properties including: (i)availability -all ops issued to the data store completely successfullt, no op can block indefenitely or return an error signifying that data is unavailable, availability is important for data storage because it ensures clients can interact with a functioning service, unresponsive storage results in an unresponsive service that quickly becomes an unused service, our definition of availability also importantly avoids designs that cheat to provide low latency by quickly returning ann error; (ii)low latency -client ops complete quickly, commercial service-level objectives suggest average perf of a few ms and worst-case perf(i.e. 99.9th-percentile) of 10s or 100s of ms, low latency is critical for data storage, it prevents slow page load times that are correlated with lost revenue; (iii)partition tolerance -the data store continues to operate under network partitions, such as 1 separating datacenters in Asia from the US, the failure of a datacenter is equivalent to it being partitioned from rest of the system, and hence a partition tolerant system also continues to operate during failure of a datacenter; partition tolerance and availability are different, but complementary requirements, systems can provide one property but not the other, quorum-based systems are considered partition tolerant because the majority partitions can continue operating, but are not considered available because nodes in the minority partition cannot form a valid quorum and hence are unable to complete ops, a system that assumes away partitions could be described as available, even though that property would not hold in practice if a partition did occur; providing both partition tolerance and availability guarantees that all replicas of a system will continue to operate and be available as long as they have not failed; (iv)high scalability -the data store scales linearly, adding N resources to the system increases aggregate throughput and storage capacity by O(N), scalability is paramount for modern storage systems, the amount of data backing web services continues to grow, systems with all data fitting on a single machine are not longer practical, instead, scalable storage must be used that spreads data across many machines and that can grow along with the data, enabling larger storage systems enables supporting a larger #users, which is an important goal for almost all web services, larger storage also enables an increase in the amount of storage per user that can be used to provide a richer web service, which is also an important goal for many web services; (v)stronger consistency -the data store provides consistency stronger than eventual consistency, which we define in this dissertation as achieving causal+ consistency, we define causal+ consistency in future section Causal+ Consistency; 
an ideal data store would provide linearizability -sometimes called strong consistency -which dictates that ops appear to take effect across the entire system as a single instance in time bt the invocation and completion of the op, in a data store that provides linearizability, as soon as a client completes a write op to an object in 1 datacenter, read ops to the same object in all other datacenters will reflect its newly written state, linearizability simplifies programming -the distributed system provides a single, consistent image -and users experience the storage behavior they expect; weaker, eventual consistency models, common in many large distributed systems, are less intuitive :not only might subsequent reads not reflect the largest value, reads across multiple objects might reflect an incoherent mix of old and new values; 
the CAP theorem proves that a shared-data system that has availability and partition tolerance cannot achieve linearizability, this impossibility result can be explained quite intuitively :if a network partition occurs, availability requires that a write on one side of the partition must be able to complete, while linearizability requires that later reads on the other side of the partition must return the result of that write, which is clearly impossible; 
several forms of strong consistency, such as linearizability, serializability, and sequential consistency have been proven incompatible with low latency -defined as latency for all ops that is less than half of the maximum speed-of-light delay bt replicas, for example, linearizability is easily demonstrated to be incompatible :it requires that a write to a datacenter in Asia be reflected immediately in a read to a US datacenter, to achieve this property, either the write must be propagated to the US site before returning, or the read must query Asia before returning, both approaches are incompatible with the goal of low latency, in general, all consistency models with a total order over ops on multiple data locations have been shown to be incompatible with low latency.
### Causal+ Consistency
a storage system's consistency guarantees can restrict the possible orderings and timing of ops throughout the system, helping to simplify the possible behaviors that a programmer must reason about and the anomalies that clients may see; 
given that the strongest forms of consistency -linearizability, serializability, and sequential consistency -are provably incompatible with our low latency requirement, previous partition-tolerant, low-latency systems settled for the weakest form of consistency, eventual consistency, the commonly agreed upon definition of eventual consistency is that writes to one replica will eventual appear at other replicas and that if all replicas have received the same set of writes, they will have the same values for all data, this weak form of consistency does not restrict ordering of ops on different keys in any way, forcing programmers to reason about all possible orderings and exposing many inconsistencies to users; 
fortunately, causal+ consistency can avoid many such inconvenient orderings while guaranteeing low latency; 
to define causal consistency with convergent conflict handling(causal+ consistency), we first describe the abstract model over which it operates, there are 2 basic ops in our model :read and write, data is stored and retrieved from logical replicas, each of which hosts the entire key space, here a single logical replica corresponds to an entire local cluster of nodes; 
an important concept in our model is the notion of potential causality bt ops, 3 rules define potential causality, denoted -->: (i)thread-of-exec -if a and b are 2 ops in a single thread of exec, then a-->b if op a happens before op b, (ii)reads-from -if a is a write op and b is a read op that returns the value written by a, then a-->b, (iii)transitivity -for ops a,b,c, if a-->b, b-->c, then a-->c, hence the causal relationship bt ops is the transitive closure of the first 2 rules; these rules establish potential causality bt ops within the same exec thread and bt ops whose exec threads interacted through the data store, our model, like many, does not allow threads to communicate directly, requiring instead that all comm occur through the data store; [click for an example exec of the ops in fig3.1(a) is shown in fig3.1(b)](./img/causal-consistency-example.png), here it demonstrates all 3 rules, the thread-of-exec rule gives w1~->w4, the reads-from rule gives w1~->w2, and the transitivity rule gives w1~->w8; 
**definition:** we define causal+ consistency as a combination of 2 properties :causal consistency and convergent conflict handling, we present intuitive definitions here not the formal definitions, previous work describes systems that provide causal+ consistency, our contribution includes identifying difference bt causal and causal+, naming causal+, and explicitly defining causal+; 
causal consistency requires that values returned from read ops at a replica are consistent with the order defined by -->(causality), i.e. it must appeart that the op that writes a value occurs after all ops that causally precede it, for example, in [fig3.1(b)](./img/causal-consistency-example.png) it must appear that w1 happens before w3, which in turn happened before w6, if a client saw w6 but not w1, causal consistency would be violated; 
causal consistency does not order concurrent ops, if a not --> b and b not --> a, then a, b are concurrent, normally, this allows increased  efficiency in an implementation :2 unrelated writes can be replicated in any order, avoiding the need for a serialization point bt them, however, if a, b are both writes to the same key, then they are in conflict; 
conflicts are undesirable for 2 reasons: (i)because they are unordered by causal consistency, conflicts allow replicas to diverge forever, for example, if a writes 1 to location X and b writes 2 to location X, then causal consistency allows one replica to forever return 1 for X and another replica to forever return 2 for X, (ii)conflicts may represent an exceptional condition that requires special handling, for example, in a shopping cart application if 2 people logged in to the same account concurrently add items to their cart, the desired result is to end up with both items in the cart; 
convergent conflict handling requires that all conflicting writes be handles in the same manner at all replicas, using a handler function h, h must be associative and commutative, so that replicas can handle conflicting writes in the same order they receive them and that results of these handlings will converge(such as one replica's h(a,h(b,c)) and another's h(c,h(b,a)) agree); 
one common way to handle conflicting writes in a convergent fashion is the last-writer-wins rule(also called Thomas's write rule), which declares one of the conflicting writes as having occurred later and has it overwrite the earlier write, another common way to handle conflicting writes is to mark them as conflicting and require their resolution by some other means such as through direct user intervention as in Coda, or through a programmed procedure as in Bayou, Dynamo; 
all potential forms of convergent conflict handling avoid the first issue -conflicting updates may continually diverge -by ensuring that replicas reach the same result after exchanging ops, on the other hand, the second issue with conflicts -applications may want special handling of conflicts -is only avoided by use of more explicit conflict resolutio procedures, these explicit procedures provide greater flexibility for applications, but require additional programmer complexity and/or perf overhead, although our system can be configured to detect conflicting updates explicitly and apply some application-defined resolution, the default versions of our systems use the last-writer-wins rule; 
**causal+ vs. other consistency models:** distributed systems literature defines several popular consistency models including: (i)linearizability(or strong consistency) -maintains a global, realtime ordering, (ii)serializability -ensures a global ordering of transactions, (iii)sequential consistency -ensures a global ordering of ops, (iv)causal consistency -ensures partial orderings bt dependent ops, (iv)fifo(pram) consistency -only preserves partial ordering of an exec thread, not bt threads, (v)per-key sequential consistency -ensures for each individual key, all ops have a global order, (vi)eventual consistency, a catch-all term used today to suggest eventual convergence to some type of agreement; [click for the causal+ consistency we introduce falls bt sequential and causal consistency](./img/a-spectrum-of-consistency-models.png), it is weaker than sequential consistency, but sequential consistency is provably not achievable in an ALPS system, it is stronger than causal consistency asnd per-key sequential consistency, and is achievable for ALPS systems; 
to illustrate utility of causal component of the causal+ model, consider an example where a user uploads a photo to an internet service and then adds that photo to an album, under causal and causal+ consistency, the photo will always arrive before the add2album op to appear before the photo, this forces programmers to reason about that scenario :should they drop the add2album op, queue up a callback for when the photo arrives, add a broken reference to the album, or do something else? the generalization of this scenario where op B appears before a proceeding op A maps to many situations where user expectations are not obeyed and/or programmers must reason about out-of-order ops, some of these other situations including: (i)removing a friend's status update (A) then replying to it (B), (ii)adding an item to a shopping cart (A) then checking to see if it is there (B), (iii)a status update (A) followed by trying to hide it (B), (iv)an account op (A) followed by trying to log in (B); 
if the data store was sequentially consistent or linearizable, it would still be possible for there to be 2 simultaneous updates to a key, however, in these stronger models it is possible to implement mutual exclusion algorithms that can be used to avoid creating a conflict altogether; 
**causal+ in practice:** we use 2 abstractions in our systems :versions and dependencies, to help us reason about causal+ consistency, we refer to the different values a data location has as the versions of a location, which we denote loc_version, in our systems, versions are assigned in a manner that ensures that if xi-->yj then i<j, once a replica returns version i of a location, xi, causal+ consistency ensures it will then only return that version or a causally later version(note that handling of a conflict is causally later than conflicting writes it resolves, to see this, condier by contradiction the following scenario :assume a replica first returns xi and then xk, where i not equal to k and xi not --> xk, causal consistency ensures that if xk is returned after xi then xk not --> xi, and so xi, xk conflict, but, if xi, xk conflict, then convergent conflict handling ensures that as soos as both are preset at a replica, their handling h(xi,xk) which is causally after both, will be returned instead of either xi or xk, which contradicts our assumption), hence each replica in our system always returns non-decreasing versions of a key, we refer to this as causal+ consistency's progressing property; 
causal consistency dictates that all ops that causally precede a given ops must appear to take effect before it, i.e. if xi-->yj then xi must be written before yj, we call these preceding values dependencies, more formally, we say yj depends on xi iff write(xi)-->write(yj), these dependencies are the reverse of the causal ordering of writes and are, by definition, the same as the happens-before relationship, [click for a graph of causality bt ops in fig3.3(a)](./img/causal-consistency-example-2.png)(this is the same graph as [fig3.1(b)](./img/causal-consistency-example.png)) and fig3.3(b) shows the corresponding dependencies bt the write ops; 
our systems provide causal+ consistency during replication by writing a version only after all of its dependencies are satisfied, meaning those writes have already been applied in the cluster; 
**scalable causality:** to our knowledge, our work is the first to name and formally define causal+ consistency, interestingly, several previous systems believed to achieve causal consistency in fact achieved the stronger guarantees of causal+ consistency, these systems were not designed to and do not provide scalable causal(or causal+) consistency, however, as they all use a form of log serialization and exchange, all ops at a logical replica are written to a single log in serialized order, commonly marked with a version vector, different replicas then exchange these logs, using version vectors to establish potential causality and detect concurrency bt ops at different replicas; 
log-exchange-based serialization impairs replica scalability, as it relies on a single serialization point in each replica to establish ordering, hence either causal dependencies bt keys are limited to set of keys that can be stored on one node \[12,25,46,59\], or a single node(or replicated state machine) must provide a commit ordering and log for all ops across a cluster; 
our systems achieve scalability by taking a different approach, nodes in each datacenter are responsible for different partitions of the keyspace, but the system can track and enforce dependencies bt keys stored on different nodes, our systems explicitly encode dependencies in metadata associated with each key's version, when keys are replicated remotely, the receiving datacenter performs dependency checks before committing the incoming version, they perform these checks by communicating directly with nodes responsible for storing the other keys, in this way, they provide the first mechanisms fir achieving scalable causal+ replication.
****
### Scheduling & MapReduce Scheduling
### VMware Distributed Resource Management :Design, Implementation, and Lessons Learned
### [Gulati12](https://www.waldspurger.org/carl/papers/drs-vmtj-mar12.pdf)
automated mgmt of physical resources is critical for reducing the operational costs of virtualized environments, an effective resource-mgmt solution must provide perf isolation among vms, handle resource fragmentation across physical hosts, and optimize scheduling for multiple resources, it must also utilize underlying hw infra efficiently, here we present design and implementation of 2 such mgmt solutions :DRS, DPM, we also highlight some key lessosn from production customer deployments over a period of more than 5yrs; 
VMware's DRS -distributed resource scheduler manages allocation of physical resources to a set of vms deployed in a cluster of hosts, each running the VMware ESX hypervisor, DRS maps vms to hosts and performs intelligent load balancing in order to improve perf and to enforce both user-specific policies and system-level constraints, using a variety of experiements, augumented with simulation results, we show that DRS significantly improves overall perf of vms running in a cluster, DRS also supports a what-if mode, making it possible to evaluate the impact of changes in wkloads or cluster config; 
VMware's DPM -distributed power mgmt extends DRS with the ability to reduce power consumption by consolidating vms onto fewer hosts, DPM recommends evacuating andd powering off hosts when cpu and mem resources are lightly utilized, it recommends powering on hosts appropriately as demand increases, or as required to satisfy resource-mgmt policies and constraints, our extensive evaluation shows that in clusters with non-trivial periods of lowered demand, DPM reduces server power consumption significantly. 
initially the rapid adoption of virtualization was fueled by significant cost savings resulting from server consolidation, running several vms on a single physical host improved hw utilization, allowing admins to do more with less and reduce capital expenses, later, more advanced vm capabilities such as cloning, template-based deployment, checkpointing, and live migration of running vms led to more agail it infras, as a result, it became much easier to create and manage vms; 
the ease of deploying wkloads in vms is leading to increasingly large vm installations, moreover, hw technology trends continue to produce more powerful servers with higher core counts and increased mem density, causing consolidation ratios to rise, however, the operational expense of managing vms now represents a significant fraction of overall costs for datacenters using virtualization, ideally, the complexity of managing a virtualize environment should also benefit from consolidation, scaling with #hosts, rather than #vms, otherwise, managing a virtual infra would be as hard -or arguably harder, due to sharing and contention -as managing a physical environment, where each application runs on its own dedicated hw; 
in practice, we observed that a large fraction of the operational costs in a virtualized environment were related to the inherent complexity of determining good vm2host mappings, and deciding when to use vMotion, VMware's live migration technology, to rebalance load by changing those mappings, the difficulty of this problem is exacerbated by the fragmentation of resources across many physical hosts and the need to balance utilization of multiple resources(including cpu and mem) simultaneously; 
we also found that admins needed a reliable way to specify resource-mgmt policies, in consolidated environments, aggregate demand can often exceed supply of physical resources, admins need expressive resource controls to prioritize vms of varying importance, in order to isolate and control perf of diverse wkloads competing for the same physical hw; 
we designed DRS to help reduce the operational complexity of running a virtualized datacenter, DRS enables managing a cluster containing many potentially-heterogeneous hosts as if it were a single pool of resources, in particular, DRS provides several key capabilities including: (i)a cluster abstraction for managing a collection of hosts as a single aggregate entity, with the combined processing and mem resources of its constituent hosts, (ii)a powerful resource pool abstraction, which supports hierarchical resource mgmt among both vms and groups of vms, at each level in the hierarchy, DRS provides a rich set of controls for flexibility expressing policies that manage resource contention, (iii)automatic initial placement, assigning a vm to a specific host within the cluster when it is powered on, (iv)dynamic load balancing of both cpu and mem resources across hosts in response to dynamic fluctuations in vm demands, as well as changes to the physical infra, (v)custom rules to constrain vm placement, including affinity and anti-affinity rules both among vms and bt vms and hosts, (vi)a host maintenance mode for hw changes or sw upgrades, which evaluates running vms from one host to other hosts in the cluster; 
here we discuss design and implementation of DRS and a related technology called DPM, DRS provides automated resource-mgmt capabilities for a cluster of hosts, DPM extends DRS with automated power mgmt, powering off hosts(placing them in standby) during periods of low utilization, and powering them back on when needed; 
our experimental evaluation demonstrates that DRS is able to meet resource-mgmt goals while improving utilization of underlying hw resources, we also show that DPM can save significant power in large configs with many hosts and vms, both of these features have been shipping as VMware prodcucts for more than 5yrs, they have been used by thousands of customers to manage hundreds of thousands of hosts and millions of vms worldwide.
### Resource Model
here we discuss DRS resource model and its associated resoure controls, a resource model explains capabilities and goals of a resource-mgmt solution, DRS offers a powerful resource model and provides a flexible set of resource controls, a wide range of resource-mgmt policies can be specified by using these controls to provide differentiated QoS to groups of vms; 
**basic resource controls:** VMware's resource controls allow admins and users to express allocations in term of either absolute vm allocation or relative vm importance, control knobs for processor and mem allocations are provided at the individual host level by the VMware ESX hypervisor, DRS provides exactly the same controls for a distributed cluster consisting of multiple ESX hosts, allowing them to be managed as a single entity, the basic VMware resource controls including: (i) reservation: specifies a minimum guaranteed amount of a certain resource, i.e. a lower bound that applies even when this resource is over-committed heavily, reservations are expressed in absolute units such as MHz for cpu, MB for mem, admission control during VMpower-on ensures that sum of reservations for a resource does not exceed its total capacity; 
(ii) limit: specifies an upper bound on consumption of a certain resource, even when this resource is under-committed, a vm is prevented from consuming more than its limit, even if that leaves some resources idle, like reservations, limits are expressed in concrete absolute units such as MHz and MB; 
(iii) shares: specify relative importance, and are expressed using abstract numeric values, a vm is entitled to consume resources proportional to its share allocation, it is guaranteed a minimum resource fraction equal to its fraction of total shares when there is contention for a resource, in the literature, this control is sometimes referred to as a weight; 
the ability to express hard bounds on allocations using reservations and limits is extremely important in a virtualized environment, without such guarantees, it is easy for vms to suer from unacceptable or unpredictable perf, meeting perf objectives would require resorting to crude methods such as static partitioning or over-provisioning of physical hw, negating the advantages of server consolidation, this motivated the original implementation of reservations and limits in ESX, as well as their inclusion in DRS; although DRS focuses on cpu and mem resources, similar controls for io resources have been validated by a research prototype, VMware also offers shares and limit controls for network and storage bdwidth, a new StorageDRS functionality for virtual disk placement and load-balancing across storage devices; 
**resource pools:** in addition to the basic, per-vm resource controls, admins and users can specify flexible resource-mgmt policies for groups of vms, this is facilitated by introducing concept of a logical resource pool -a container that species an aggregate resource allocation fro a set of vms, a resource pool is a named object with associated settings for each managed resource -the same familiar shares, reservation, and limit controls used for vms, admission control is performed at the pool level :sum of reservations for a pool's children must not exceed pool's own reservation; resource pools may be configured in a flexible hierarchical org :each pool has an enclosing parent pool, and children that may be vms or sub-pools, resource pools are useful for dividing or sharing aggregate capacity among groups of users or vms, for example, admins often use resource-pool hierarchies to mirror human organizational structures, resource pools also provide direct support for delegated administration :an admin can allocate resources in bulk to sub-admins using sub-pools; [click for a resource pool structure defined by an example org](./img/resource-pool-structure.png), here resources are first split across 2 groups :business, testing, business is further sub-divided into it, sales groups, while testing is a flat collection of vms, separate per-pool allocation provide both isolation bt pools and sharing within pools; for example, if some vms within the sales pool are idle, their unused allocation will be reallocated preferentially to other vms within the same pool, any remaining spare allocation flows preferentially to other vms within business, its enclosing parent pool, then to Org, its ancestor, note that in a resource-pool hierarchy, shares are meaningful only with respect to siblings :each pool effectively defines a scope(similar to a currency) within which share values are interpreted; a distinguished root resource pool for a cluster represents the physical capacity of the entire cluster, which is divided up among its children, all resource pools and vms in a cluster are descendants of the root resource pool; 
**resource pool divvy:** a resource pool represents an aggregate resource allocation that may be consumed by its children, we refer to the process of computing the entitled reservation, limit, and shares of its child sub-pools and VMs as divvying; divvying is performed in a hierarchical manner, dividing resources associated with a parent pool among its children, divvying starts from root of the resource pool hierarchy, and ultimately ends with vms at its leaves, DRS uses resulting entitlements to update host-level settings properly, ensuring that controls such as reservations and limits are enforced strictly by ESX hypervisor, DRS also uses host-level entitlement imbalances to inform vm migration decisions; during divvying, DRS computes resource entitlements for individual pools and vms, divvying computations incorporate user-sepcified resource controls settings, as well as per-vm and aggregate wkload demands, since pool-level resource allocations reflect vm demands, DRS allows resources to flow among vms as demands change, this enables convenient multiplexing of resources among groups of vms, without the need to set any per-vm resource controls; divvying must handle several cases in order to maintain the resource pool abstraction, for example, it is possible for the total reservation of a parent pool to be greater than sum of its children's reservations, similarly, the limit at a parent pool can be smaller than sum of its children's limit values, finally, a parent's shares need to be distributed among its children in proportion to each child's shares value, in all such cases, parent values need to be divided among children based on the user-set values of reservation, limit, shares, and actual runtime demands of children; these divvy ops are defined :reservation-divvy, limit-divvy, share-divvy, named for their respective resource controls, DRS carries out these divvy ops periodically(by defult, every 5mins), reflecting current vm demands, DRS also initiates divvy ops in response to changes in resource allocation settings; the divvy algorithm works in 2 phases: (i)in the first bottom-up phase, divvying starts with demand values of individual vms, which are leaf nodes, it then accumulates aggregate demands up the resource pool tree; vm demands are computed by ESX hypervisor for both cpu and mem, a vm's cpu demand is computed as its actual cpu consumption CPU_used plus a scaled portion of CPU_ready, the time it was ready to execute but queued due to contention is: *CPU_demand = CPU_used + (CPU_run/(CPU_run+CPU_sleep))timesCPU_ready*; a vm's mem demand is computed by tracking a set of randomly-selected pages in the vm's physical addr space, and computing how many of them are touched within a certain time interval, for example, if 40% of the sampled pages are touched for a vm with 16GB mem allocation, its active mem is estimated to be 0.4times16=6.4GB; once the per-vm demand values have been computed, they are aggregated up the tree, demand values are updated to always be no less than the reservation and no more than the limit value, hence at each node, the following adjustments are made: *demand=MAX(demand, reservation) demand=MIN(demand, limit)*; (ii)the second divvying phase proceeds in a top-down manner, reservation and limit values at each parent are used to compute respective resource settings for its children such that the following constraints are met including: (i)child allocations are in proportion to their shares, (ii)each child is allocated at least its own reservation, (iii)no child is allocated more than its own limit, in order to incorporate demand info, if sum of the children's demands is larger than the quantity(reservation or limit) that is being parceled out, we replace limit values of children by their demand values: *limit=MIN(demand, limit)*, this allows reservation and limit settings at a parent to flow among its children based on their actual demands; conceptually, the divvy algorithm at a single level in the tree can be implemented by first giving reservation to every child, then the extra reservation or limit, depending on what is being divvied out, can be allocated in small chunks to children, by giving a chunk to the child with minimum value of current allocation/shares, if a child's current allocation hits its limit, that child can be taken out from further consideration and capped at the limit; the share-divvy is performed by dividing a parent's shares among its children, in proportion to shares of each child, demands do not play any role in the share-divvy op; we illustrate divvy ops using a simple example with a 2-level resource pool tree, [click for a resource pool tree](./img/resource-pool-tree.png) with cpu reservation R=10GHz, limit L=20GHz, and shares S=1000 at the root node, shares are unitless and the other two settings are in absolute cpu units(GHz), the user-set resource control values are denoted by U, and the final divvied values are denoted by D, although this example shows only cpu divvying, a similar computation is performed for mem, here in fig2, there are 2 resource pools under the root, RP1, RP2, each with 2 child vms, first, the bottom-up divvying phase aggregates demands up the tree, starting with individual vm demands, next, the top-down divvying phase starts at the root node, doing reservation and limit divvy recursively until reaching the leaf nodes; the 10GHz reservation at the root is greater than the 5GHz sum of its children's reservations, hence the reservation is divvied based on shares, while meeting all divvying constraints, in this case, shares-based allocation of 8GHz and 2GHz satisfies the reservation and limit values of the 2 resource pools, next, the reservation of 8GHz at RP1 is divvying among vm1, vm2, since 8GHz is smaller than aggregate demand of 10GHz from vm1 and vm2 we replace limit values of vm1 and vm2 by their demand values of 3GHz and 7GHz respectively, as a result, even though shares of these 2 vms are equal, the divvied reservations are 3GHz and 5GHz(instead of 4GHz each), this illustrates how demands are considered while allocating a parent's reservation; note that vm demands do not act as an upper bound while divvying limit values here, since sum of the children's demands is smaller than the limit at the parent, as a result, the actual limit values of the children are used in the divvying, shares are simply distributed in proportion to shares of children, although we have not walked through every computation in this resource pool tree, one can verify that both reservation and limit values can be divvied in a top-down pass starting from root, while considering user-set values and current demands.
### DRS Overview and Design
DRS is designed to enforce resource-mgmt policies accurately, delivering physical resources to each vm based on the resource model described in the previous section, [click for DRS runs as part of the vCenter Server centralized mgmt sw](./img/drs-runs-part-of-the-vcenter-server-centralized-management-software.png), it manages resources for a cluster of ESX hosts as well as vms running on them, more specifically, DRS performs 4 key resource-mgmt ops including: (i)it computes the amount of resources to which each vm is entitled based on the reservation, limit, and shares values and the runtime demands for all related vms and resource pools, (ii)it recommends and performs migration of powered-on vms to balance load across hosts in a dynamic environment where vm's resource demands may change over time, (iii)it optioanlly saves power by invoking DPM, (iv)it performs initial placement of vms onto hosts, so that a user does not have to make manual placement decisions;
DRS load balancing is inboked periodically(by default, every 5mins) to satisfy cluster constraints and ensure delivery of entitled resources, it is also invoked on demand when user makes cluster config changes, such as adding a host to the cluster or requesting that a host enter maintenance mode, when DRS is invoked, it performs the first 3 resource-mgmt ops listed above, along with a pass to correct cluster constraint violations, for example, constraint correction evacuates vms from hosts that user has requested to enter maintenance or standy mode;
DRS initial placement assigns a vm to a host within a cluster when vm is powered-on, resumed from a suspended state, or migrated into cluster manually, DRS initial placement shares code with DRS load balancing to ensure that placement recommendations respect constraints and resource entitlements;
**load balancing:** we first examine DRS load-balancing metric and algorithm, we then consider in more detail how DRS analyzes possible load-balancing moves in terms of their impact on addressing imbalance, their costs and benefits, and their interaction with pending and dependent actions, including: (i) load balancing metric: is dynamic entitlement, which differs from the more commonly used metric of host utilization in that it reflects resource delivery in accordance with both needs and importance of vms, dynamic entitlement is computed based on overall cluster capacity, resource controls, and the actual demand for cpu and mem resources from each vm; a vm's entitlement for a resource is higher than its reservation and lower than its limit :the actual value depends on cluster capacity and total demand; dynamic entitlement is equivalent to demand when demands of all vms in the cluster can be met, otherwise, it is a scaled-down demand value with the scaling dependent on cluster capacity, the demands of other vms, the vm's place in the resource pool hierarchy, and its shares, reservation, and limit; dynamic entitlement is computed by running the divvy algorithm over the resource pool hierarchy tree, for entitlement computation, we use the cluster capacity at the root as the quantity that is divvied out, this is done for both cpu and mem resources separately; DRS currently uses normalzied entitlement as its core per-host load metric, reflecting host capacity as well as entitlements of running vms, for a host *h*, normalized entitlement *Nb* is defined as sum of the per-vm entitlements *Ei* for all vms running on *h*, divided by host capacity *Ch* available to vms: *Nh=(sum_Ei)/Ch*, if *Nh<=1* then host *h* is deemed to have insufficient resources to meet entitlements of all its vms, and as a result, vms on that host would be treated unfairly as compared to vms running on hosts whose normalized entitlements were not above 1; after calculating *Nh* for each host, DRS computes the cluster-wide imbalance *Ic* which is defined as the standard deviation over all *Nh* values, the cluster-wide imbalance considers both cpu and mem imbalance using a weighted sum, in which weights depend on resource contention, if mem is highly contended, i.e. its max normalized entitlement on any host is above 1, then it is weighted more heavily than cpu, if cpu is highly contended, then it is weighted moe heavily than mem, equal weights are used if neither resource is highly contended, the ration 3:1 derived by experimentation, is used when one resource is weighted more heavily than the other;
(ii) [load balancing algorithm, click to view](./img/drs-load-balancing-algorithm.png): here this overview of DRS algorithm has been greatly simplified to focus on its core load-balancing metric, the actual load-balancing algorithm considers many other factors, including the impact of move on imbalance and risk-adjusted benefit of each move; DRS load-balancing algorithm uses a greedy hill-climbing technique, this approach, as opposed to an exhaustive, offline approach that would try to find the best target balance, is driven by practical considerations, the live migration ops used to improve load-balancing have a cost, and vm demand is changing overtime, so optimizing for a particular dynamic situation is not worthwhile; DRS aims too minimize cluster-wide imbalance *Ic* by evaluating all possible single-VMmigrations, many filtered quickly in practice, and selecting the move that would reduce *Ic* the most, the selected move is applied to the algorithm's current internal cluster state so that it reflects the state that would result when migration completes, this move-selection step is repeated until no additional beneficial moves remain, there are enough moves for this pass, or the cluster imbalance is at or below threshold *T* specified by DRS admin, after the algorithm completes, an exec engine performs the recommended migrations, optionally requiring user-approval;
(iii) minimum goodness: DRS load balancing rejects a move if it does not produce enough benefit in terms of improvement in the standard deviation value representing imbalance, the threshold used for this filtering is computed dynamically based on #hosts and vms, the threshold si reduced significantly when imbalance is very high and moves to correct it are filtered by the normal threshold, so that many low-impact moves can be used to correct high imbalance;
(iv) cost-benefit analysis: the main check DRS uses to filter unstable moves is cost-benefit analysis, it considers various costs involved in the move by modeling the vMotion costs as well as cost to the other vms on the destination host which will have an additional vm competing for resources, the benefit is computed as how much the vms on the source and the migrating vm will benefit from the move, cost-benefit analysis also considers risk of the move by predicting how the wkload might change on both the source and destination and if this move still makes sense when the wkload changes; 
