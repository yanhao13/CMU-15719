# CMU-15719
### Introduction, Use cases, and Elasticity
### A View of Cloud Computing
### [Armbrust2010](https://dl.acm.org/doi/10.1145/1721654.1721672)
### Clearing the clouds away from the true potential and obstacles posed by this computing capability.
### Oracle's CEO Larry Ellison vent his frustration: "The interesting thing about cloud computing is that we've redefined cloud computing to include exerything that we already do.... I don't understand what we would do differently in the light of cloud computing other than change the wording of some of our ads."
### Just as large ISPs use multiple network providers so that failure by a single company will not take them off the air, we believe the only plausible solution to very high availability is multiple cloud computing providers.
cloud computing, the long-held dream of computing as a utility, has the potential to transform a large part of it industry, making sw even more attractive aaS and shaping the way it hw is designed and purchased; 
developers with innovative ideas for new internet services no longer require the large capital outlays in hw to deploy their service or the human expense to operate it, they need not be concerned about overprovisioning for a service whose popularity does not meet their predictions, hus wasting costly resources, or underprovisioning for one that becomes wildly popular, thus missing potential customers and revenue; 
moreover, companies with large batch-oriented tasks can get results as quickly as their programs can scale, since using 1000 servers for 1hr costs no more than using 1 server for 1000hrs, this elasticity of resources, without paying a premium for large scale, is unprecedented in history of it.
### Defining Cloud Computing
cloud computing refers to both the applications delivered aaS over the internet and the hw and systems sw in the datacenters that provide those services, the services themselves have long been referred to as SaaS(*here we use the term SaaS to mean applications delivered over the internet, the broadest definition would encompass any on demand sw, including those that run sw locally but control use via remote sw licensing), some vendors use terms such as IaaS, PaaS to describe their products, but we eschew these because accepted definitions for them still vary widely, the line bt low-level infra and a higher-level platform is not crisp, we believe the two are more alike than different, and we consider them together; similarly, the related term grid computing, from the hp computing community, suggests protocols to offer shared computation and storage over long distances, but those protocols did not lead to a sw environment that grew beyond its community; 
the datacenter hw and sw is what we will call a **cloud**, when a cloud is made available in a pay-as-you-go manner to the general public, we call it a **public cloud**, the service being sold is **utility computing**, we use the term **private cloud** to refer to internal datacenters of a business or other org, not made available to the general public, when they are large enough to benefit from the advantages of cloud computing that we discuss here; thus cloud computing is sum of SaaS and utility computing, but does not include small or medium sized datacenters, even if these rely on virtualization for mgmt, people can be users or providers of SaaS, or users or providers of utility computing; we focus on SaaS providers(cloud users) and cloud providers, which have received less attention than SaaS users, [click for making provider-user relationships clear](./img/making-provider-user-relationships-clear.png), in some cases, the same actor can play multiple roles, for example, a cloud provider might also host its own customer-facing services on cloud infra; 
from a hw provisioning and pricing point of view, 3 aspects are new in cloud computing including: the appearance of infinite computing resources available on demand, quickly enough to follow load surges, thereby eliminating need for cloud computing users to plan far ahead for provisioning; elimination of an up-front commitment by cloud users, thereby allowing companies to start small and increase hw resources only when there is an increase in their needs(*note, however, that upfront commitments can still be used to reduce per-usage charges, for exmaple, AWS also offers long-term rental of servers, which they call reserved instances); the ability to pay for use of computing resources on a short-term basis as needed(such as processors by the hr and storage by the day) and release them as needed, thereby rewarding conservation by letting machines and storage go when they are no longer useful; 
we argue that construction and operation of extremely large scale, commodity-computer datacenters at low cost locations was the key necessary enabler of cloud computing, for they uncovered factors of 5-7 decrease in cost of electricity, network bdwidth, ops, sw, and hw available at these very large economies of scale; these factors, combined with statistical multiplexing to increase utilization compared to traditional datacenters, meant that cloud computing could offer services below the costs of a medium sized datacenter and yet still make a good profit; 
out proposed definition allows us to clearly identify certain installations as examples and non-examples of cloud computing, consider a public-facing internet service hosted on an ISP who can allocate more machines to the service given 4hrs notice, since load surges on the public internet can happen much more quickly than that(we saw its load to double every 12 hrs for nearly 3 days), this is not cloud computing, in contrast, consider an internal enterprise datacenter whose applications are modified only with significant advance notice to admins, in this scenario large load surges on the scale of mins are highly unlikely, so as long as allocation can track expected load increases, this scenario fulfills one of the necessary conditions for operating as a cloud, the enterprise datacenter may still fail to meet other conditions for being a cloud, however, such as the appearance of infinite resources for fine-grained billing; a private datacenter may also not benefit from economies of scale that make public clouds financially attractive; 
omitting private clouds from cloud computing has led to considerable debate in the blogosphere, we believe the confusion and skepticism illustrated by Larry Ellison's quote occurs when advantages of public clouds are also claimed for medium sized datacenters, except for extremely large datacenters of hundreds of thousands of machines, such as those that might be operated by Google, Microsoft, [most datacenters enjoy only a subset of the potential advantages of public clouds, click to view](./img/most-data-centers-enjoy-only-a-subset-of-the-potential-advantages-of-public-clouds.png), hence we believe that including traditional datacenters in the definition of cloud computing will lead to exaggerated claims for smaller so-called private clouds, which is why we exclude them, however, here we describe how so-called private clouds can get more of the benefits of public clouds through surge computing, hybrid cloud computing.
### Classes of Utility Computing
any application needs a model of computation, a model of storage, and a model of comm, the statistical multiplexing necessary to achieve elasticity and the appearance of infinite capacity available on demand requires automatic allocation and mgmt, in practice, this is done with virtualization of some sort, we argue that different utility computing offerings will be distinguished based on the cloud sytem sw's level of abstraction and on the level of mgmt of resources; 
Amazon EC2 is at one end of the spectrum, an EC2 instance looks much like physical hw, and users can control nearly the entire sw stack, from the kernel upward, this low level makes it inherently difficult for Amazon to offer automatic scalability and failover because the semantics associated with replication and other state mgmt issues are highly application-dependent; at the other extreme of the spectrum are application domain-specific platforms such as Google AppEngine, which is targeted exclusively at traditional web applications, enforcing an application structure of clean separation bt a stateless computation tier and a stateful storage tier, AppEngine's impressive automatic scaling and high-availability mechanisms, and the proprietary MegaStore data storage available to AppEngine applications, all rely on these constraints; applications for Microsoft's Azure are written using .NET libs, and compiled to the Common Language Runtime, a lang-independent managed environment, the framework is significantly more flexible than AppEngine's, but still constrains user's choice of storage model and application structure, hence Azure is intermediate bt hw vms like EC2 and application frameworks like AppEngine.
### Cloud Computing Economics
we see 3 particularly compelling use cases that favor utility computing over conventional hosting, including: (i)when demand for a service varies with time, for example, provisioning a datacenter for the peak load it must sustain a few days per month leads to underutilization at other times, instead, cloud computing lets an org pay by hr of computing resources, potentially leading to cost savings even if the hourly rate to rent a machine from a cloud provider is higher than the rate to own one; (ii)when demand is unknown in advance, for example, a web startup will need to support a spike in demand when it becomes popular, followed potentially by a reduction once some visitors turn away; (iii)orgs that perform batch analytics can use the cost associativity of cloud computing to finish computations faster :using 1000 EC2 machines for 1hr costs the same as using 1 machine for 1000hrs; 
although the economic appeal of cloud computing is often described as CapEx2OpEx -converting capital expenses to operating expenses, we believe the phrase pay-as-you-go more directly captures the economic benefit to the buyer, hrs purchased via cloud computing can be distributed non-uniformly in time(for example, use 100 server hrs today and no server hrs tomorrow, and still pay only for 100), in the networking community, this way of selling bdwidth is already known as usage-based pricing(*usage-based pricing is not renting, renting a resources involves paying a negotiated cost to have the resource over some time period, whether or not you use the resource, vs., pay-as-you-go involves metering usage and charging based on actual use, independently of the time period over which the usage occurs); in addition, absence of upfront capital expense allows capital to be rediected to core business investment; 
hence, even if Amazon's pay-as-you-go pricing was more expensive than buying and depreciating a comparable server over the same period, we argue that the cost is outwrighed by the extremely important cloud computing economic benefits of elasticity and transference of risk, esp. risks of overprovisioning(underutilization) and underprovisioning(saturation); 
we start with elasticity, key observation is that cloud computing's ability to add or remove resources at a fine grain(1 server at a time with EC2) and with a lead time of mins rather than weeks allows matching resources to wkload much more closely, real world estimates of average server utilization in datacenters range from 5%-20%, this may sound shockingly low, but it is consistent with the observation that for many services the peak wkload exceeds the average by factors of 2-10, since few users deliberately provision for less than the expected peak, resources are idle at nonpeak times, the more pronounced the variation, the more the waste, [click for example in figure2a](./img/elasticity.png), here, we assume our service has a predictable demand where the peak requires 500 servers at noon but the trough requires only 100 servers at midnight, as long as the average utilization over a whole day is 300 servers, the actual cost per day(area under the curve) is 300times24=7200 server hrs, but since we must provision to the peak of 500 servers, we pay for 500times24=12000 server hrs, a factor of 1.7 more; hence, as long as the pay-as-you-go cost per server hr over 3 years(typically amortization time) is less than 1.7 times cost of buying the server, utility computing is cheaper; in fact, this example underestimates benefits of elasticity, because in addition to simple diurnal patterns, most services also experience seasonal or other periodic demand variation(such as e-commerce in December and photo sharing sites after holidays) and some unexpected demand bursts due to external events(such as news events), since it can take weeks to acquire and rack new equipment, to handle such spikes you must provision for them in advance, we already saw that even if service operators predict the spike sizes correctly, capacity is wasted, and if they overestimate the spike they provision for, it is even worse; 
[they may also underestimate spike, click to view in figure2b](./img/elasticity.png), however, accidentally turning away excess users, while cost of overprovisioning is easily measured, cost of underprovisioning is more difficult to measure yet potentially equally serious :not only do rejected users generate 0 revenue, they may never come back; [for example, Friendster's decline in popularity relative to competitors Facebook, MySpace, is believed to have resulted partly from user dissatisfaction with slow repsonse times(up to 40 secs), click to view in figure2c](./img/elasticity.png) :users will desert an underprovisioned service until peak user load equals datacenter's usable capability, at which point users again receive acceptable service; for another simplified example, assume that users of a hypothetical site fall into 2 classes :active users(those who use the site regularly) and defectors(those who abandon the site due to poor perf), further, suppose that 10% of active users who receive poor service due to underprovisioning are permanently lost opportunities(become defectors), i.e., users who would have remained regular visitors with a better experience, the site is initially provisioned to handle an expected peak of 400000 users(1000 users per server times 400 servers), but unexpected positive press drives 500000 users in the first hr, of the 100000 who are turned away or receive bad service, by our assumption 10000 of them are permanently lost, leaving an active user base of 390000, the next hr sees 250000 new unique users, the first 10000 do fine, but the site is still overcapacity by 240000 users, this results in 24000 additional defections, leaving 376000 permanent users, if this pattern continues, after lg(500000) or 19hrs, #new users will approach 0 and the site will be at capacity in steady state, clearly, the service operator has collected less than 400000 users' worth of steady revenue during those 19hrs, however, again illustrating the underutilization argument, to say nothing of the bad reputation from disgruntled users; 
do such scenarios really occur in practice? when we made our service available via Facebook, it experienced a demand surge that resulted in growing from 50 servers to 3500 servers in 3 days, even if the average utilization of each server was low, no one could have foreseen that resource needs would suddenly double every 12hrs for 3 days, after the peak subsided, traffic fell to a lower level, so in this real world example, scale-up elasticity allowed the steady-state expenditure to more closely match the steady-state wkload.
### Top 10 Obstacles and Opportunities for Cloud Computing
[click to view](./img/top-ten-obstacles-and-opportunities-for-cloud-computing.png), the first 3 affect adoption, the next 5 affect growth, the last 2 are policy and business obstacles, each obstacle is paired with an opportunity to overcome that obstacle, ranging from product dev to research projects.
### 拨开云雾，展现云计算的真正潜力，并消除其带来的障碍。
### Oracle 首席执行官 Larry Ellison 表达了他的不满：“云计算的有趣之处在于，我们重新定义了云计算，将我们已经在做的一切都涵盖其中……除了修改一些广告措辞之外，我不知道在云计算的背景下，我们还能做什么不同的事情。”
云计算，作为计算作为一种实用工具的长期梦想，有可能改变 IT 行业的很大一部分，使软件即服务 (aaS) 更具吸引力，并塑造硬件的设计和购买方式；
拥有创新互联网服务理念的开发人员不再需要投入大量的硬件资金来部署他们的服务，也不再需要投入大量的人力来运营它，他们不必担心为受欢迎程度未达预期的服务过度配置资源，从而浪费昂贵的资源，也不必担心为已经非常流行的服务配置资源不足，从而错失潜在客户和收入；
此外，拥有大量批处理任务的公司能够像其程序扩展一样快速获得结果，因为使用 1000 台服务器 1 小时的成本不超过使用 1 台服务器 1000 小时的成本，这种资源弹性在历史上是前所未有的，无需为大规模支付额外费用。
### 定义云计算
云计算既指通过互联网以“即服务”（aaS）形式交付的应用程序，也指数据中心中提供这些服务的硬件和系统软件。这些服务本身长期以来被称为 SaaS（*此处我们使用 SaaS 一词表示通过互联网交付的应用程序，最广泛的定义涵盖任何按需软件，包括在本地运行软件但通过远程软件许可控制使用的软件）。一些供应商使用 IaaS、PaaS 等术语来描述他们的产品，但我们避免使用这些术语，因为它们的公认定义仍然差异很大，低级基础设施和高级平台之间的界限并不明确，我们认为两者的相同之处多于不同之处，我们认为将它们结合在一起；同样，来自惠普计算社区的相关术语“网格计算”提出了提供远距离共享计算和存储的协议，但这些协议并没有催生出一个超越其社区的软件环境；数据中心硬件和软件就是我们所说的**云**，当云以按需付费的方式向公众开放时，我们称之为**公共云**，所出售的服务是**效用计算**，我们使用术语**私有云**来指代企业或其他组织的内部数据中心，这些数据中心不向公众开放，但当它们规模足够大到能够从我们在此讨论的云计算优势中受益时；因此，云计算是 SaaS 和效用计算的总和，但不包括中小型数据中心，即使这些数据中心依赖虚拟化进行管理，人们可以是 SaaS 的用户或提供商，也可以是效用计算的用户或提供商；我们关注的是 SaaS 提供商（云用户）和云提供商，这两者的关注度低于 SaaS 用户。[点击查看提供商-用户关系](./img/making-provider-user-relationships-clear.png)，在某些情况下，同一个参与者可以扮演多个角色，例如，云提供商可能还会在云端基础设施上托管自己的面向客户的服务；
从硬件配置和定价的角度来看，云计算有三个新特点：无限计算资源的出现，可以按需提供，并且能够快速应对负载激增，从而消除了云计算用户提前规划配置的需要；取消了云用户的前期承诺，从而允许公司从小规模开始，仅在需求增加时才增加硬件资源（*但请注意，前期承诺仍然可以用来降低每次使用的费用，例如，AWS 也提供服务器的长期租赁，他们称之为预留实例）；能够根据需要短期支付计算资源的使用费用（例如按小时支付处理器费用和按天支付存储费用），并根据需要释放这些资源，从而通过在不再使用时释放机器和存储来奖励节约；
我们认为，在低成本地点建设和运营超大规模商用计算机数据中心是云计算的关键必要因素，因为它们揭示了在这些非常大的规模经济下，电力、网络带宽、操作、软件和硬件成本降低 5 至 7 倍的因素；这些因素与统计复用相结合，提高了与传统数据中心相比的利用率，意味着云计算可以提供低于中型数据中心成本的服务，同时仍然获得良好的利润；
我们提出的定义使我们能够清楚地识别某些安装作为示例和非云计算的例子，考虑一个托管在 ISP 上的面向公众的互联网服务，该 ISP 可以在提前 4 小时通知的情况下为该服务分配更多机器，因为公共互联网上的负载激增可能发生得更快（我们看到它的负载每 12 小时翻一番，持续了近 3 天），这不是云计算。相反，考虑一个内部企业数据中心，其应用程序只有在提前通知管理员的情况下才会进行修改，在这种情况下，分钟级的大规模负载激增极不可能发生，因此只要分配可以跟踪预期的负载增长，这种情况就满足了作为云运营的必要条件之一。然而，企业数据中心可能仍然无法满足成为云的其他条件，例如出现无限资源以实现细粒度计费；私有数据中心也可能无法从使公共云具有经济吸引力的规模经济中受益；
将私有云从云计算中剔除在博客圈引发了相当大的争议，我们认为，拉里·埃里森的引言所体现的困惑和怀疑，发生在公有云的优势也适用于中型数据中心的情况下，除了拥有数十万台机器的超大型数据中心，例如谷歌、微软运营的那些。[大多数数据中心只享有公有云的部分潜在优势，点击查看](./img/most-data-centers-enjoy-only-a-subset-of-the-potential-advantages-of-public-clouds.png)，因此，我们认为将传统数据中心纳入云计算的定义会导致对规模较小的所谓私有云的夸大宣传，这就是我们将其排除在外的原因。然而，我们在这里描述了所谓的私有云如何通过激增计算、混合云计算获得更多公有云的优势。
### 效用计算的类别
任何应用程序都需要一个计算模型、一个存储模型和一个comm，实现弹性和按需无限容量所需的统计复用需要自动分配和管理，实际上，这是通过某种虚拟化实现的，我们认为不同的效用计算产品将根据云系统软件的抽象级别和资源管理级别进行区分；
Amazon EC2 处于一个极端，EC2 实例看起来很像物理硬件，用户可以控制几乎整个软件堆栈，从内核向上。这种低级别使得 Amazon 很难提供自动可扩展性和故障转移，因为与复制和其他状态管理问题相关的语义高度依赖于应用程序；另一个极端是应用程序领域特定平台，例如 Google AppEngine，它专门针对传统的 Web 应用程序，强制执行无状态计算层和有状态存储层干净分离的应用程序结构，AppEngine 令人印象深刻的自动扩展和高可用性机制，以及 AppEngine 应用程序可用的专有 MegaStore 数据存储，都依赖于这些约束；微软 Azure 的应用程序使用 .NET 库编写，并编译为公共语言运行时（Common Language Runtime，一个独立于语言的托管环境）。该框架比 AppEngine 的框架灵活得多，但仍然限制了用户对存储模型和应用程序结构的选择，因此 Azure 介于 EC2 等硬件虚拟机和 AppEngine 等应用程序框架之间。
### 云计算经济学
我们发现 3 个特别引人注目的用例表明，效用计算优于传统托管，包括：(i) 当服务需求随时间变化时，例如，为数据中心配置每月必须维持几天的峰值负载，会导致其他时间利用率不足。相反，云计算允许组织按小时支付计算资源费用，即使从云提供商租用机器的小时费率高于拥有机器的费率，也有可能节省成本； (ii) 当需求事先未知时，例如，一家网络初创公司需要支持其在流行时出现的需求激增，而一旦一些访问者流失，需求可能会减少；(iii) 执行批量分析的组织可以利用云计算的成本关联性来更快地完成计算：使用 1000 台 EC2 机器 1 小时的成本与使用 1 台机器 1000 小时的成本相同；
虽然云计算的经济吸引力通常被描述为资本支出转化为运营支出（CapEx2OpEx），即将资本支出转化为运营支出，但我们认为“按需付费”这一说法更直接地体现了买方的经济效益，通过云计算购买的服务器小时数可以不均匀地分配（例如，今天使用 100 小时服务器时间，明天不使用，仍然只需支付 100 小时的费用），在网络社区中，这种销售带宽的方式被称为基于使用量的定价（*基于使用量的定价不是租用，租用资源涉及支付协商好的费用以在一段时间内拥有资源，无论是否使用资源；而按需付费则涉及计量使用量并根据实际使用情况收费，与使用发生的时间段无关）；此外，由于没有前期资本支出，资本可以重新用于核心业务投资；因此，即使亚马逊的按需付费定价比在同一时期购买和折旧一台同类服务器的成本更高，我们认为，其成本也被云计算极其重要的经济效益所抵消，即弹性和风险转移，尤其是过度配置（利用不足）和配置不足（饱和）的风险；
我们从弹性开始，关键的观察是云计算能够以细粒度添加或删除资源（使用 EC2 一次添加或删除一台服务器），并且只需几分钟而不是几周的准备时间，就可以更紧密地匹配资源以进行负载均衡，现实世界中数据中心的平均服务器利用率估计在 5% 到 20% 之间，这听起来可能低得惊人，但这与以下观察结果一致：对于许多服务而言，峰值负载是平均值的 2 到 10 倍，因为很少有用户故意提供低于预期峰值的资源，资源在非峰值时段处于闲置状态，变化越明显，浪费就越多，[单击图 2a 中的示例](./img/elasticity.png)，在这里，我们假设我们的服务具有可预测的需求，峰值在中午需要 500 台服务器，但低谷在午夜只需要 100 台服务器，只要全天的平均利用率为 300 台服务器，则每天的实际成本（曲线下面积）为300x24=7200 服务器小时，但由于我们必须为 500 台服务器的峰值进行配置，因此我们需要支付 500x24=12000 服务器小时的费用，比峰值高出 1.7 倍；因此，只要 3 年内（通常是摊销期）每服务器小时的即用即付成本小于购买服务器成本的 1.7 倍，效用计算就会更便宜；事实上，这个例子低估了弹性的优势，因为除了简单的昼夜模式外，大多数服务还会经历季节性或其他周期性需求变化（例如 12 月的电子商务和节假日后的照片分享网站）以及一些由于外部事件（例如新闻事件）导致的意外需求爆发，因为购买和上架新设备可能需要数周时间，为了应对这样的峰值，您必须提前做好准备，我们已经看到，即使服务运营商正确预测了峰值规模，容量也会被浪费，如果他们高估了他们所准备的峰值，情况会更糟；
[他们也可能低估峰值，单击查看图 2b](./img/elasticity.png)，然而，意外地拒绝了多余的用户，虽然过度配置的成本很容易衡量，但配置不足的成本更难衡量，但可能同样严重：被拒绝的用户不仅不会产生任何收入，而且可能永远不会回来；[例如，Friendster 相对于竞争对手 Facebook、MySpace 的受欢迎程度下降被认为部分是由于用户对缓慢的响应时间（长达 40 秒）不满意，单击查看图 2c](./img/elasticity.png)：用户将放弃配置不足的服务，直到峰值用户负载等于数据中心的可用容量，此时用户再次获得可接受的服务；另一个简化的例子，假设一个网站的用户分为两类：活跃用户（经常使用该网站的用户）和流失用户（由于性能不佳而放弃该网站的用户）。进一步假设，由于配置不足而获得较差服务的用户中有 10% 永远失去了机会（成为流失用户），即那些本来可以成为常客并获得更好体验的用户。该网站最初配置为处理预期峰值 400,000 个用户（每台服务器 1,000 个用户乘以 400 台服务器），但意外的正面报道在第一个小时内吸引了 500,000 名用户，在 100,000 名被拒之门外或获得不良服务的用户中，根据我们的假设，其中 10,000 人永久流失，剩下 390,000 个活跃用户。下一个小时将看到 250,000 个新的独立用户，前 10,000 名用户表现良好，但该网站仍然超负荷 240,000 个用户。这导致24000名用户流失，留下376000名永久用户。如果这种模式持续下去，经过lg(500000)或19小时后，新增用户数量将接近0，站点将达到稳定状态。显然，服务运营商在这19个小时内获得的稳定收入还不到400000名用户的收入，但这再次证明了利用率不足的论点，更不用说来自不满用户的坏名声了。
这种情况在实践中真的会发生吗？当我们通过Facebook提供服务时，它经历了需求激增，导致服务器数量在3天内从50台增加到3500台。即使每台服务器的平均利用率很低，也没有人能预见到资源需求会在3天内每12小时突然翻一番。峰值消退后，流量会下降到更低的水平。因此，在这个现实世界的例子中，扩展弹性使稳定状态的支出能够更紧密地匹配稳定状态的工作负荷。
### 云计算的 10 大障碍和机遇
[点击查看](./img/top-ten-obstacles-and-opportunities-for-cloud-computing.png)，前 3 个影响采用，接下来的 5 个影响增长，后 2 个是政策和业务障碍，每个障碍都与克服该障碍的机会配对，从产品开发到研究项目。
### The NIST Definition of Cloud Computing
### [NISTdef2011](https://www.nist.gov/publications/nist-definition-cloud-computing)
### NIST -National Institute of Standards and Technology Definition of Cloud Computing
cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources(such as networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal mgmt effort or service provider interaction; this cloud model is composed of 5 essential characteristics, 3 service models, and 4 deployment models; 
essential characteristics: #1 on-demand self-service -a consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with each service provider, #2 broad network access -capabilities are available over the network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms(such as mobile phones, tablets, laptops, and workstations), #3 resource pooling -provider's computing resources are pooled to serve multiple consumers using a multitenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand, there is a sense of location independence in that the customer generally has no control or knowledge over the exact location of the provided resources but may be able to specify location at a higher level of abstraction(such as country, state, or datacenter), examples of resources include storage, processing, mem, and network bdwidth, #4 rapid elasticity -capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward commensurate with demand, to the consumer, the capabilities available for provisioning often appear to be unlimited and can be appropriated in any quantity at any time, #5 measured service -cloud systems automatically control and optimize resource use by leveraging a metering capability(*typically this is done on a pay-as-you-go or charge-per-use basis) at some level of abstraction appropriate to type of service(such as storage, processing, bdwidth, and active user accounts), resources usage can be monitored, controlled, and reported, providing transparency for both provider and consumer of the utilized service; 
service models: #1 SaaS -the capability provided to the consumer it to use provider's application running on a cloud infra(*a cloud infra is collection of hw and sw that enables the 5 essential characteristics of cloud computing, the cloud infra can be viewed as containing both a physical layer and an abstraction layer, the physical layer consists of the hw resources that are necessary to support the cloud services being provided and typically includes server, storage, and network components, the abstraction layer consists of sw deployed across the physical layer which manifests the essential cloud chracteristics, conceptually the abstraction layer sits above the physical layer), the applications are accessible from various client services through either a thin client interface, such as a web browser(such as web-based email), or a program interface, consumer does not manage or control th eunderlying cloud infra including network, servers, os, storage, or even individual application capabilities, with the possible exception of limited user-specific application config settings; #2 PaaS -the capability provided to the consumer is to deploy onto the cloud infra consumer-created or acquired applications created using programming langs, libs, services, and tools supported by the provider(*this capability does not necessarily preclude use of compatible programming langs, libs, services, and tools from other sources), the consumer does not manage or control the underlying cloud infra including network, servers, os, or storage, but has control over the deployed applications and possibly config settings for the application-hosting environment; #3 IaaS -the capability provided to consumer is to provision processing, storage, networks, and other fundamental computing resources where consumer is able to deploy and run arbitrary sw, which can include os and applications, consumer does not manage or control the underlying cloud infra but has control over os, storage, and deployed applications, and possibly limited control of select networking components(such as host firewalls); 
deployment models: #1 private cloud -the cloud infra is provisioned for exclusive use by a single org comprising multiple consumers(such as business units), it may be owned, managed, and operated by the org, a third party, or some combinition of them, and it may exist on or off premises; #2 community cloud -the cloud infra is provisioned for exclusive use by a specific community of consumers from orgs that have shared concerns(such as mission, security requirements, policy, and compliance considerations), it may be owned, managed, and operated by one or more of the orgs in the community, a third party, or some combinition of them, and it may exist on or off premises; #3 public cloud -the cloud infra is provisioned for open use by the general public, it may be owned, managed, and operated by a business, academic, or government org, or some combinition of them, it exists on the premises of the cloud provider; #4 hybrid cloud -the cloud infra is a composition of two or more distinct cloud infras that remain unique entities, but are bound together by standardized or proprietary technology that enables data and application portability(such as cloud bursting for load balancing bt clouds).
### Dynamically Scaling Applications in the Cloud
### [Vaquero11](https://dl.acm.org/doi/pdf/10.1145/1925861.1925869)
### Scalability is said to be one of the major advantages brought by the cloud paradigm and, more specifically, the one that makes it different to an advanced outsourcing solution, however, there are some important pending issues before making the dreamed automated scaling for applications come true.
cloud computing is commonly associated to offering of new mechanisms for infra porvisioning including: the illusion of a virtually infinite computing infra, the employment of advanced billing mechanisms allowing a pay-per-use model on shared multitenant resources, the simplified programming mechanisms(platform); among these features, those introduced by adding scalability and automated on-demand self-service are responsible for making any particular service something more than just an outsourced service with a prettier marketing face; 
as a result of its relevance, the wealth of systems dealing with cloud application scalability is slowing gaining weight in the available literature; automation is typically achieved by using a set of service provider-defined rules that govern how the service scales up or down to adapt to a var load, these rules are themselves composed of a condition, which when met triggers some action on infra or platform, the degree of automation, abstraction for the user(service provider) and customization of rules governing the service vary, some systems offer users chance of building rather simple conditions based on fixed infra or platform matrics(such as cpu, mem, etc.), while others employ server-level metrics(such as cost to benefit ration) and allow for more complex conditions(such as arithmetic and logic combinations of simple rules) to be included in the rules, regarding the subsequent actions launched when conditions are met, available efforts focus on service horizontal scaling(i.e. adding new server replicas and load balancers to distribute load among all available replicas) or vertical scaling(i.e. on-th-fly changing of assigned resources to an already running instance, such as letting more physical cpu to a running vm), unfortunately, the most common os do not support on-the-fly(without rebooting) changes on the available cpu or mem to support this vertical scaling; 
beyond mere server scalability, some other elements need to be taken into account that affect the overall application scaling potential, for example, LBs -load balancers need to support aggregation of new servers(typically, but not necessarily, in the form of new vms) in order to distribute load among several servers, Amazon already provides strategies for load balancing your replicated vms via its elastic load balancing capabilities, hence LBs and the algorithms that distribute load to different servers are essential elements in achieving application scalability in the cloud; 
having several servers and the mechanisms to distribute load among them is a definitive step towards scaling a cloud application, however, there is another element of the datacenter infra to be considered towards complete application scalability, network scalability is an often neglected element that should also be considered; in a consolidated datacenter scenario, several vms share the same network, potentially producing a huge increase in the equired bdwidth(potentially collapsing the network), hence it is necessary to extend infra clouds to other kinds of underlying resources beyond servers, LBs, and storage; cloud applications should be able to request not only virtual servers at multiple points in the network, but also bdwidth-provisioned network pipes and other network resources to interconnect them -NaaS; 
clouds that offer simple virtual hw infra such as vms and networks are usually denoted IaaS, vs., PaaS clouds provide sets of online libs and service for supporting application design, implementation, and maintenance, despite being somewhat less flexible than IaaS clouds, PaaS clouds are becoming important elements for building applications in a faster manner and many important it players scuh as Google, Microsoft, have developed new PaaS clouds systems such as Google AppEngine, Microsoft Azure, we discuss scalability in PaaS clouds at 2 different levels -container level and db level; 
[click for an overview of mechanisms handy to accomplish the goal of the whole application scalability](./img/overview-of-mechanisms-handy-to-accomplish-the-goal-of-the-whole-application-scalability.png).
### Server Scalability
most of the available IaaS clouds deal with single vm mgmt primitives(such as elements for adding/removing vms), lacking mechanisms for treating applications as a whole single entity and dealing with relations among different application components, for example, relationships bt vms are often not considered, ordered deployment of vms contaning sw for different tiers of an appliation is not automated(such as the db's ip is only known at deployment time hence the db needs to be deployed first in order to get its ip and configure the web server connecting to it), etc., application providers typically want to deal with their application only, being released from burden of dealing with (virtual) infra terms; 
**towards increased abstraction and automation :the elasticity controller:** such a fine-grained mgmt(vm-based) may come on handy for few services or domestic users, but it may become intractable with a big number of deployed applications composed of several vms each, the problem gets worse if application providers aim at having their application automatically scaled according to load, they would need to monitor every vm for every application in the cloud and make decisions on whether or not every vm should be scaled, its LB re-configured, its network resources resizes, etc., 2 different approaches are possible: increasing the abstraction level of provided APIs and/or advancing towards a higher automation degree; 
automated scaling features are being included by some vendors, but the rules and policies they allow to express still deal with individual vms only, one cannot easily relate the scaling of vms at tier-1 with its load balancers or the scaling of vms at tier-3; 
on the other hand, more abstract frameworks are proposed(they allow users to deal with applications as a whole rather than per individual vm) that also convey automation, unavoidably, any scalability mgmt system(or [elasticity controller, click to view](./img/elasticity-controller.png)) is bound to the underlying cloud API(the problem of discrete actuators), hence one essential task for any application-level elasticity controller is mapping user scaling policies from the appropriate level of abstraction for the user to actual mechanisms provided by IaaS clouds(depending on specific underlying API, in practice, most of the available cloud APIs expose vm-level mgmt capabilities, so that controller-actuator pairs are limited to adding/removing vms); the implementation of elasticity controller can be done in several different ways with regard to the provided abstraction level: (i)a per-tier controller, so that there is a need for coordination and sync among multiple controller-actuator pairs, treating each tier as an independent actuator with its own control policy can cause shifting of the perf bottleneck bt tiers, a tier can only release resources when an interlock is not being held by the other tiers, (ii)a single controller for the whole application(for all tiers), which let users specify how an application should scale in a global manner, for example, application provider could specify(based on its accurate knowledge of application) to scale the application logic tier whenever #incoming reqs at the web tier is beyond a given threshold; 
**expressing how and when to scale :feeding controller with rules and policies:** all the works above rely on traditional control theory in which several sensors feed a decision making module(elasticity controller) with data to operate on an actuator(cloud API), [click to view](./img/elasticity-controller.png), similarly, all the systems above answer the question on how to automate scalability(i.e. how to implement elasticity controller) in a similar manner either for vm-level or for application-level scalability mgmt systems; 
user-defined rules are the chosen mechanism(as opposed to preconfigured equations sets) to express the policies controlling scalability as shown below: *RULE: if CONDITION(s) then ACTION(s) CONDITION: (1..*) (metric.value MODIFIER) ACTION: (*) IaaS cloud-enabled actions(such as deploy new vm)*, a rule is composed of a series of conditions that when met trigger some action over the underlying cloud, every condition itself is composed of a series of metrics or events that may be present in the system, such as money available, authorization received, etc.(either provided by infra or obtained by application provider itself), which are used together with a modifier(either a comparison against a threshold, the presence of such events) to trigger a set of cloud-provided actions; 
if we focus on application-level scalability(rather than dealing with per vm scaling), a mathematical foumulation in which user just configures threshold for replicating storage servers so as to increase the cloud storage capability is proposed(*called proportional thresholding mechanism, which considers the fact that going from 1 to 2 machines can increase capacity by 100% but going from 100 to 101 machines increases capacity by no more than 1%), a leave full control for users to express their rules and use a rule engine as a controller is also proposed, the OVF -open virtual format is extended to define the application, its components, its contextualization needs, and the rules for scaling, this ways, service providers can generate a description of their application components, the way their application behaves with regard to scalability and the relevant metrics that will trigger action expressed in such rules; 
[as shown in fig1](./img/overview-of-mechanisms-handy-to-accomplish-the-goal-of-the-whole-application-scalability.png), in addition to dynamically adding more vm replicas, application behavior could also include many other aspects determining application scalability: adding load balancers to distribute load among vm replicas is also an important point, in an IaaS cloud, #vms balanced by a single LB can hugely increase, hence overloading the balancer(*although we recognize the importance of load balancing algorithms, the wealth of available literature on this matter places them well beyond of the scope of this work); 
LB scalability requires total time taken to forward each req to the corrsponding server to be negligible for small loads and should grow no faster than O(p)(here p being small loads of balanced vms) when the load is big and #balanced vms is large, however, although mechanisms to scale the load balancing tier would benefit any cloud system, they are missing in most current cloud implementations; 
Amazon already provides its elastic load balancer service aimed at delivering users with a single LB to distribute load among vms, however, Amazon does not provide mechanisms to scale LBs themselves; virtualization of load balancing mechanisms offers chance to define scalability rules by using previously presented systems, recently, a rule of thumb procedure to configure presenration-tier(web server) scalability is proposed :for cpu-intensive web applications, it is better to use a LB to split computation among many instances, for network intensive applications, it may be better to use a cpu-powerful standalone instance to maximize network throughput, yet, for even more network intensive applications, it may be necessary to use dns load balancing to get around a single instance bdwidth limitation, the question emerges whether dns-based load balancing can be scalable enough or not, practical experiences with large well-known services such as Google's search engine and recent experimental works on cloud settings seem to point in that direction, also, for many enterprise applications, hw LB is the most common approach.
### Scaling the Network
properly replicated/sized vms led us to think about LBs as a possible bottleneck, assuming this problem is also resolved takes us to think about the link that keeps application elements stuck together, even across different cloud infra services :the network, which is often overlooked in cloud computing; 
networking over virtualized resources is typically done in 2 different manners: ethernet virtualization and overlay networks and tcp/ip virtualization, these techniques are respectively focused in usage of VLAN -virtual local area network tags (L2) to separate traffic or public key infras to build L2/L3 overlays; 
separating users' traffic is not enough for reaching complete application scalability :the need to scale the very network arises in consolidated datacenters hosting several vms per physical machine, this scalability is often achieved by over-provisioning resources to suit this increased demand; this approach is expensive and induces network instability while infra is being updated, also, it is static and does not take into account that not all applications consume all required bdwidth during all time; improved mechanisms taking into account actual network usage are required; on the other hand, one could periodically measure actual network usage per application and let applications momentarily use other applications' allocated bdwidth; on the other hand, applications could request more bdwidth on demand over the same links, instantiate bdwidth-provisioned network resources together with the vms composing the service across several cloud providers is proposed, similar to the OVF extensions mentioned above, these authors employ NDL -network description lang -based ontologies for expressing required network characteristics; these abstract requirements are mapped to the concrete underlying network peculiarities(such as dynamically provisioned circuits vs. ip overlays), a complete architecture to perform this mapping has also been proposed, unfortunately, there is no known production-ready system that fully accomplishes need for dynamically managing the network in synchrony with vms provisioning; 
these techniques to increase utilization of the network by virtually slicing it have been dubbled as NaaS, this *a la cloud* network provision paradigm can be supported by flow control, distirbuted rate limiting, and network slicing techniques, by applying this mechanism the actual bdwidth can be dynamically allocated to applications on demand, which would benefit from a dynamic resource allocation scheme in which all users pay for the actual bdwidth consumption; to optimize network usage statistical multiplexing is used to compute the final bdwidth allocated to each application, statistical multiplexing helps to allocate more bdwidth to some applications while some others are not using it(most system admins usually provision on a worst-case scenario and never use all requested resources), this way, cloud provider can make a more rational use of its network resources, while still meeting applications' needs.
### Scaling the Platform
IaaS clouds are handy for application providers to control resources used by their systems, however, IaaS clouds demand application developers or system admins to install and configure all sw stack the application components need, in contrast, PaaS clouds offer a ready to use exec environment, along with convenient services, for applications, hence when using PaaS clouds developers can focus on programming their components rather than on setting up environment those components require, but as PaaS clouds can be subject to an extensive usage(many concurrent users calling to hosted applications), PaaS providers must be able to scale exec environment accordingly, in this section, we explore how scalability impacts on the 2 core layers of PaaS platforms: the container and the DBMS, as they are the backbone of any PaaS platform :combination of container+db is the chosen stack to implement many networked(such as internet) applications, which are the ones PaaS platforms are oriented to; here, container is the sw platform where users' components will be deployed and run, different PaaS clouds can be based on different platforms, for example, GAE and its open source counterpart AppEngine provide containers for servlets(part of J2EE specification) and python scripts, while Azure, Aneka offer an environment for .NET applications, each platform type can define different lifecycles, services, and APIs for the components it hosts; db provides data persistence support, the db storage service must address demand for data transactions support combined with big availability and scalability requirements, as it is explained later in this section, this can be addressed in different manners; [click for an overview of possible architecture of a PaaS platform](./img/PaaS.png), where both container and db layer achieve scalability through replication of container and dbms(horizontal scaling), this is the scenario this work focuses on, as it is the only one the authors deem feasible in clouds with certain scalability requirements, vs., applying vertical scaling by using more powerful hw would soon fail, as many clouds will typically face loads that one single machine cannot handle whatever its capacity; other services can be offered by a PaaS platform apart from container and db, which also will need to be scaled to adapt to demand, for example, Azure offers services for inter-component communication(bus) or access control, unfortunately, in this work it would only be possible to study scalability issues of all those services in a too superficial manner, instead, a most thorough analysis of the most important services, the container and the db, has been preferred; 
**container-level scalability:** at container level, a better scalability can be achieved by enabling multitenant containers(having the ability to run components belonging to different users), this imposes strong isolation requirements which maybe not all platforms can achieve by default, for example, the standard java platform is known to carry important security limitations that hinder implementation of safe multitenant environments, a more straightforward option is to run each user's components on non-shared containers, which is the approach taken by GAE; 
in both cases scaling is implemented by instantiating/releasig containers where several replicas of the developer components can be run(i.e. it is a form of horizontal scaling), this should automatically be done by the platform, so developers are not forced to constantly monitor their services' state; either automatic scaling IaaS systems(such as those mentioned in section service availability) can be used, or the platform itself can scale up and down the resources, the latter approach is the one used by both AppEngine, Aneka, AppEngine can run in any Xen based cloud, private(built for example with Eucalyptus) or public(EC2), however, it is not clear from if AppEngine supports transparent cloud federation so that a private PaaS cloud could deploy its containers in vms running in third party owned IaaS clouds to face sudden load peaks, in any case AppScale could apply Eucalyptus ability to be integrated with other clouds, on the other hand, Aneka supports cloud federation natively, so federated Aneka clouds can borrow resources among them, or a given Aneka cloud can use containers hosted in vms running in different IaaS clouds; 
automatic scaling of containers has several implications for developers regarding component design, if the PaaS platform can stop container replicas at any moment(such as due to low load), components could be designed to be as stateless as possible(as GAE recommends for the applications it hosts); in order to ease application dev, support for stateful components should be offered by the platform, in this case with stateful components, LB and container replica mgmt modules must be aware of state, this way, LBs know which container replicas hold session data to forward reqs accordingly, and container instances are not removed as long as they hold some session; 
if more than one container instance holds data from the same session(for better scalability and failure resilience), then some mechanism is necessary that allows to keep the different data replicas updated; transparent session data(such as shopping cart) replication, usually denoted soft state replication can be offered through systems such as Tempest toll or SSM; it is also possible to use distributed cache systems such as memcached for explicit data sharing among component replicas; each solution will have a different impact on component dev, roughly speaking, distributed caches work at application level i.e. they are explicitly accessed by hosted components code to store/retrieve info shared among component replicas, while soft state/session replication systems work in a transparent manner for application developer; 
**db scalability:** PaaS systems must expect very high reqs rates as they can host many applications that demand intense data traffice, 3 mechanisms can be used(and combined) to handle this: distributed caching, nosql dbs, and db clustering, here, (i)caching systems, such as memcached, are used to store intermediate data to speed up frequent data queries, a req for some data item will first check cache to see if the data is present, and will query db only if the data is not in cache(or it has expired), distributed caching systems provide the same cache across several nodes, which is useful in clustered applications, for exmaple, GAE offers [memcache](http://code.google.com/appengine/docs/java/memcache/) as a service for application developers; (ii)nosql refers to a wide family of storage solutions for structured data that are different from traditional relational fully sql-compliant dbs, nosql systems offer high scalability and availability, which seems a good fit in cloud environments with potentially many applications hosted under high demand, on the other hand, the replica mgmt mechanisms they use, provide less guarantees than traditional systems, usually, updates on data copies are not immediately done after each write op, they will be eventually done at some point in the future, this causes 3 systems to be unable to implement support for transparent and fully acid-complaint transactions, hence imposing some limitations on how transactions can be used by developers, besides, the fact that they only support(the equivalent to) a subset of sql can be a hurdle for some applications, such as BigTable used by GAE to provide its object oriented data storage service, HBase an open source implementation of BigTable used by AppEngine; (iii)finally, if fully relational and sql-complaint dbs are to be provided(as in the case of Microsoft's Azure), clusters can be built to provide better scalability, availability, and fault tolerance to typical dbms systems, unfortunately, 3 clusters must be built so several or all nodes contain a replica of each data item, which is known to compromise perf even for moderate loads when transactions are supported, present db replication systems from every major relational dbms have several limitations, and further research is needed to achieve desired perf, the major problem comes from the fact that transactions require protecting data involved while transaction lasts, often making that data unavailable to other transactions, the more transactions running at the same time, the more conflicts will be raised with the corrsponding impact on the application perf; 
yet, some db replication solutions exist that offer some degree of scalability, these can be part of the dbms itself(in-core) or be implemented as a mw layer bt db and application, most of the mw based solutions use a proxy driver used by client applications to access db, this proxy redirects reqs to the replication mw, which forwards them to db, the mw layer handles reqs and transforms then in db ops to ensure that all data copies are updated, also, it takes care of load balancing tasks, exmaples of such solution are C-JDBC, Middle-R, DBFarm; 
it can be concluded from this section that replication of dbs/components is the most important issue to consider when scaling a PaaS platform, [click for main replication ideas presented](./img/replication.png), at container level, the same component can be run on different container instances to achieve better scalability and failure tolerance, but then the platform should make available some mechanism for consistent data replication among components, at db level, copies of application data can be hosted in different dbms instances again for better scalability and failure tolerance, unfortunately, keeping consistency can lead to transaction conflicts; 
[click for summary of most relevant works related to holistic application scaling in cloud environments at the 3 different levels: server, network, and platform level, the most relevant features are highlighted and appropriate references are given](./img/summary-of-most-relevant-works-related-to-holistic-application-scaling-in-cloud-environments-at-the-three-different-levels.png).
****
### Building a Carnegie Mellon Cloud and Openstack
### Virtual Infrastructure Management in Private and Hybrid Clouds
### [Sotomayor2009](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5233608)
### One of the many definitions of cloud is that of an IaaS system, which it infra is deployed in a provider's datacenter as vms, with IaaS clouds' growing popularity, tools and technologies are emerging that can transform an org's existing infra into a private or hybrid cloud; OpenNebula is an open source virtual infra manager that deploys virtualized servers on both a local pool of resources and external IaaS clouds; Haizea is a resource lease manager that can act as a scheduling backend for OpenNebula, providing features not found in other cloud sw or virtualization-based datacenter mgmt sw.
cloud computing is to use a cloud-inspired pun, a nebulously defined term, however, it was arguably first popularized in 2006 by Amazon's [EC2](www.amazon.com/ec2/) -elastic compute cloud, which started offering vms for $0.10 an hr using both a simple web interface and a programmer-friendly API, although not the first to propose a utility computing model, EC2 contributed to popularizing IaaS paradigm, which became closely tied to the notion of cloud computing; an IaaS cloud enables on-demand provisioning of computational resources in the form of vms deployed in a cloud provider's datacenter(such as Amazon's), minimizing or even eliminating associated capital costs for cloud consumers and letting those consumers add or remove capacity from their it infra to meet peak or fluctuating service demands while paying only for the actual capacity load; 
overtime, an ecosystem of providers, users, and technologies has coalesced around this IaaS cloud model, more IaaS cloud providers such as GoGrid, FlexiScale, ElasticHosts, etc., have emerged, and a strong number of companies base their it strategy on cloud-based resources, spending little or no capital to manage their own it infra([click for more examples](https://aws.amazon.com/solutions/case-studies/)); some providers such as Elastra, RightScale, focus on deploying and managing services on top of IaaS clouds, including web and db servers that benefit from such clouds' elastic capacity, and let their clients provision services directly instead of having no provision and set up infra themselves; other providers offer products that facilitate working with IaaS clouds, such as [rPath's rBuilder](www.rpath.org), which enables users to dynamically create sw environments to run on a cloud; 
although this ecosystem has evolved around public clouds -commercial cloud providers that offer a publicly accessible remote interface for creating and managing vm instances within their proprietary infra -internet is growing in open source cloud computing tools that let orgs build their own IaaS clouds using their internal infras, these private cloud deployments' primary aim is not to sell capacity over internet through publicly accessible interfaces but to give local users a flexible and agile private infra to run service wkloads within their administrative domains, private clouds can also support a hybrid cloud model by supplementing local infra with computing capacity from an external public cloud, private and hybrid clouds are not exclusive with being public clouds, a priavte or hybrid cloud can allow remote access to its resources over internet using remote interfaces, such as web services that Amazon EC2 uses, here, we look at 2 open source projects that facilitate mgmt of such private or hybrid cloud models.
### Virtual Infra Mgmt
to provide users with the same features found in commercial public clouds, private or hybrid cloud sw must: (i)provide a uniform and homogeneous view of virtualized resources, regardless of underlying virtualization platform(such as Xen, KVM -kernel-based vm, VMware, etc.), (ii)manage a vm's full life cycle, including setting up networks dynamically for groups of vms and managing their storage requirements, such as vm disk image deployment or on-the-fly sw environment creation, (iii)support configurable resource allocation policies to meet org's specific goals(high availability, server consolidation to minimize power usage, etc.), (iv)adapt to org's changing resource needs, including peaks in which local resource are insufficient, and changing resources including addition or failure of physical resources; 
hence a key component in private or hybrid clouds will be VI -virtual infra mgmt, the dynamic orchestration of vms that meets the requirements we have just outlined, here, we discuss vi mgmt's relevance not just for creating private or hybrid clouds but also within the emerging cloud ecosystem; our 2 open source projects, [OpenNebula](http://www.opennebula.org/), [Haizea](http://haizea.cs.uchicago.edu/), are complementary and can be used to manag vis in private or hybrid clouds, here, OpenNebula is a vi manager that orgs can use to deploy and manage vms, either individually or in groups that must be coscheduled on local resources or external public clouds, it automates vm setup(preparing disk images, setting up networking, etc.) regardless of underlying virtualization layer(Xen, KVM, VMware) or external cloud(EC2, ElasticHosts), Haizea is a resource lease manager that can act as a scheduling backend for OpenNebula, providing leasing capabilities not found in other cloud systems, such as ARs -advance researvations, and resource preemption, which are particularly relevant for private clouds.
### The Cloud Ecosystem
vi mgmt tools for datacenters have been around since before cloud computing became industry's new buzzword, several of these, such as the [Platform VM Orchestrator](https://www.platform.com/Products/platform-vm-orchestrator), [VMware vSphere](http://www.vmware.com/products/vsphere/), [Ovirt](http://ovirt.org/), meet many of the vi mgmt requirements we outlined earlier, providing features such as dynamic placement and vm mgmt on a pool of physical resources, automatic load balancing, server consolidation, and dynamic infra resizing and partitioning; although creating what we now call a private cloud was already possible with existing tools, these tools lack other features that are relevant for building IaaS clouds, such as public cloud-like interfaces, mechanisms for adding such interfaces easily, and the ability to deploy vms on external clouds; 
on the other hand, projects such as [Globus Nimbus](http://workspace.globus.org), [Eucalyptus](www.eucalyptus.com), which we term cloud toolkits, can help transform existing infra into an IaaS cloud with cloud-like interfaces, here, Eucalyptus is compatible with Amazon EC2 interface and is designed to support additional client-side interfaces, Globus Nimbus exposes EC2 and WSRF -web services resource framework interfaces and offers self-configuring virtual cluster support, however, although these tools are fully functional with respect to providing cloud-like interfaces and higher-level functionality for security, contextualization, and vm disk image mgmt, their vi mgmt capabilities are limited and lack features of solutions that specialize in vi mgmt; hence [an ecosystem of cloud tools is starting to form, click to view](./img/ecosystem-of-cloud-tools.png), in which cloud toolkits attempt to span both cloud mgmt and vi mgmt but, by focusing on the former, do not deliver the same functionality as sw written specifically for vi mgmt; although integrating cloud mgmt solutions with existing vi managers would seem like obvious solution, this is complicated by lack of open and standard interfaces bt the 2 layers, and lack of certain key features in existing vi managers, hence our aim is to produce a vi mgmt solution with a flexible and open architecture that orgs can employ to build private or hybrid clouds; with this goal in mind, we started developing OpenNebula and continue to enhance it as part of [EU's Reservoir porject](http://www.reservoir-fp7.eu/), which aims to develop open source technologies to enable deployment and mgmt of complex it services across different administrative domains; OpenNebula provides similar functionality to that found in existing vi managers but also aims to over those solutions' shortcomings, namely: (i)the inability to scale to external clouds, (ii)monolithic and closed architectures that are hard to extend or interface with other sw, not allowing seamless integration with existing storage and network mgmt solutions deployed in datacenters, (iii)a limited choice of preconfigured placement policies(first fit, round robin, etc.), (iv)a lack of support for scheduling, deploying, and configuring groups of vms(such as a group of vms representing a cluster where all the nodes either deploy entirely or do not deploy at all and where some vms' config depends on others' such as the head-worker replationship in compute clusters), [click for a more detailed comparison bt OpenNebula and several well-known vi managers, including cloud toolkits that perform vi mgmt](./img/detailed-comparison-between-OpenNebula-and-several-well-known-virtual-infrastructure-managers.png); 
a key feature of OpenNebula's architecture, which we describe more in the next section, is its highly modular design, which facilitates integration with any virtualization platform and third-party component in the cloud ecosystem, such as cloud toolkits, virtual image managers, service managers, and vm schedulers, for example, it specifies all actions pretaining to setting up a vm disk image(transferring the image, installing sw on it, etc.) in terms of well-defined hooks, although OpenNebula includes a default transfer manager that uses these hooks, it can also leverage contextualizers just by writing code that interfaces bt hooks and thir-party sw; 
Haizea which we developed independently from OpenNebula, was the first to leverage such an architecture in a way that was beneficial to both projects, Haizea originally cloud simulate vm scheduling only for research purposes, but we modified it to act as a drop-in replacement for OpenNebula's default scheduler, with few changes required in Haizea code and none in the OpenNebula code, by working together, OpenNebula could offer resource leases as a fundamental provisioning abstraction, and Haizea could operate with real hw through OpenNebula; 
in fact, integraing OpenNebula and Haizea provides the only vi mgmt solution offering advance reservation of capacity, [as tabel1 shows](./img/detailed-comparison-between-OpenNebula-and-several-well-known-virtual-infrastructure-managers.png), other vi managers use immediate provisioning, or best-effort provisioning, which we discuss in more detail later, however, private clouds -specifically those with limited resources in which not all reqs are satisfiable immediately owing to lack of resources -stand to benefit from more sophisticated vm placement strategies supporting queues, priorities, and ars; additionally, service provisioning clouds, such as the one being developed in Reservoir project, have requirements that are insupportable with only an immediate provisioning model -for example, they need capacity reservations at specific times to meet SLAs -service-level agreements, or peak capacity requirements.
### The OpenNebula Architecture
[click to view](.//img/opennebula.png), encompasses several components specialized in different aspects of vi mgmt; to control a vm's life cycle, OpenNebula core orchestrates 3 different mgmt areas including: image and storage technologies(i.e. virtual appliance tools or distributed file systems) for preparing disk images for vms, network fabric(such as DHCP -dynamic host config protocol servers, firewalls, or switches) for providing vms with a virtual network environment, and underlying hypervisors for creating and controlling vms; the core performs specific storage, network, or virtualization ops through pluggable drivers, hence OpenNebula is not tied to any specific environment, providing a uniform mgmt layer regardless of underlying infra; 
besides managing individual vm's life cycle, we also designed the core to support services deployment, such services typically include a set of interrelated components(such as a web server and db backend) requiring several vms, hence we can treat a group of related vms as a first-class entity in OpenNebula; besides managing vms as a unit, the core also handles delivery of context info(such as web server's ip addr, digital certificates, and sw licenses) to the vms; 
a separate scheduler component makes vm placement decisions, more specifically, the scheduler has access to info on all reqs OpenNebula receives and, based on these reqs, keeps track of current and future allocations, creating and updating a resource schedule and sending appropriate deployment commands to OpenNebula core; OpenNebula default scheduler provides a rank scheduling policy that places vms on physical resources according to a ranking algorithm that the administrator can configure, it relies on realtime data from both the running vms and available physical resources; 
OpenNebula offers mgmt interfaces to integrate core's functionality within other datacenter mgmt tools, such as accounting or monitoring frameworks, to this end, OpenNebula implements the [libvirt API](http://libvirt.org/), an open interface for vm mgmt, as well as a CLI, adiitionally, a subset of this functionality is exposed to external users through a cloud interface; 
finally, OpenNebula can support a hybrid cloud model by using cloud drivers to interface with external clouds, this lets orgs supplement local infra with computing capacity from a public cloud to meet peak demands, better serve their access reqs(such as by moving the service closer to user), or implement high availablity strategies; OpenNebula currently includes an EC2 driver, which can submit reqs to EC2, Eucalyptus, as well as an ElasticHosts driver.
### Haizea Lease Manager
Haizea is an open source lease manager and can act as a vm scheduler for OpenNebula or be used on its own as a simulator to evaluate different scheduling strategies' perf over time; the fundamental resource provisioning abstraction in Haizea is the lease, intuitively, a lease is a form of contract in which one party agrees to provide a set of resources to another, when a user wants to request computational resources from Haizea, it does so in the form of a lease, leases are then implemented as vms managed by OpenNebula, the lease terms Haizea supports include hw resources, sw environments, and the period during which hw and sw resources must be avialable; currently, Haizea supports ar leases which resources must be available at a specific time; best-effort leases, in which resources are provisioned as soon as possible, and reqs are placed in a queue, if necessary; immediate leases, in which resources are provisioned when requested or not at all; 
to satisfy use cases where resources must be guaranteed to be available at certain times, various researchers have studied advance reservation of computational resources in the context of parallel computing; in the absence of suspension/resumption capabilities, this approach produces resource underutilization due to need to vacate resources before an ar starts; by using vms to implement leases, an org can support ars more efficiently through resource preemption, suspending vms of lower-priority leases before a reservation starts, resuming them after reservation ends, and potentially migrating them to other available nodes or even other clouds, we can do this without having to make the applications inside the vm aware that they are going to be suspended, resumed, or even migrated, however, using vms introduces runtime overhead, which poses additional scheduling challenges, as does the preparation overhead of deploying the vm disk images that the lease needs, such overheads can noticeably affect perf if they are not adequately managed; Haizea's approach is to separately schedule preparation overhead rather than assuming it should just be deducted from a user's allocation, however, this is complicated when Haizea must support multiple lease types with conflicting requirements that it has to reconcile -for example, transfers for a lease starting at 2pm could require delaying transfers for best-effort leases, resulting in longer wait times, Haizea uses several optimizations, such as reusing disk images across leases, to minimize preparation overhead's impact, similarly, Haizea also schedules runtime overhead of suspending, resuming, and migrating vms; 
Haizea bases its scheduling on a resource slot table that represents all physical nodes it manages over time, it schedules best-effort leases using a first-come-first-serve queue with backfilling(a common optimization in queue-based systems), whereas ar leases use a greedy algorithm to select physical resources that minimize #preemptions, although the resource selection algorithm is currently hardcoded, future versions will include a policy decision module to let developers specify their own resource selection policies(such as policies to prioritize leases based on user, group, project, etc.), this policy decision module will also specify the conditions under which Haizea should accept or reject a lease.
****
### Encapsulating Computation (Intel Labs, guest)
### Xen and the Art of Virtualization
### [Barham03](https://dl.acm.org/doi/pdf/10.1145/1165389.945462)
### [Project Page](http://www.cl.cam.ac.uk/netos/xen)
numerous systems have been designed which use virtualization to subdivide ample resources of a modern computer, some require specialized hw, or cannot support commodity os, some target 100% binary compatibility at the expense of perf, others sacrifice security of functionality for speed, few offer resource isolation or perf guarantees, most provide only best-effort provisioning, risking denial of service; 
we present Xen, an x86 vm monitor which allows multiple commodity os to share conventional hw in a safe and resource managed fashion, but without sacrificing their perf or functionality, this is achieved by providing an idealized vm abstraction to which os such as linux, bsd, windows xp, can be ported with minimal effort; 
our design is targeted at hosting up to 100 vm instances simultaneously on a modern server, the virtualization approach taken by Xen is extremely efficient -we allow os such as linux, windows xp, to be hosted simultaneously for a negligible perf overhead -at most a few percent compared with the unvirtualized case, we considerably outperform competing commercial and freely available solutions in a range of microbenchmarks and system-wide tests. 
modern computers are sufficiently powerful to use virtualization to present illusion of many smaller vms, each running a separate os instance, this has lead to a resurgence of interest in vm technology, here we present Xen, a high perf resource-managed VMM -vm monitor which enables applications such as server consolidation, co-located hosting facilities, distributed web services, secure computing platforms, and application mobility; 
successful partitioning of a machine to support concurrent exec of multiple os poses several challenges including: (i)vms must be isolated from one another -it is not acceptable for exec of one to adversely affect perf of another, this is particularly true when vms are owned by mutually untrusting users, (ii)it is necessary to support a variety of different os to accomodate the heterogeneity of popular applications, (iii)the perf overhead introduced by virtualization should be small; 
Xen hosts commodity os, albeit with some source modifications, the prototype described and evaluated here can support multiple concurrent instances of our XenoLinux guest os, each instance exports an application binary interface identical to a non-virtualized linux2.4, our port of windows xp to Xen is not yet complete but is capable of running simple user-space processes, work is also progressing in porting NetBSD; 
Xen enables users to dynamically instantiate an os to execute whether they desire, in the XenoServer project we are deploying Xen on standard server hw at economically strategic locations within ISPs or at internet exchanges, we perform admission control when starting new vms and expect each vm to pay in some fashion for resources it requires, we discuss our ideas and approach in this direction elsewhere, here we focus on vmm; 
there are a number of ways to build a system to host multiple applications and servers on a shared machine, perhaps the simplest is to deploy one or more hosts running a standard os such as linux, windows, and then to allow users to install files and start processes -protection bt applications being provided by conventional os techniques, experience shows that system administration can quickly become a time-consuming task due to complex config interactions bt supposedly disjoint applications; 
more importantly, such systems do not adequately support perf isolation; the scheduling policy, mem demand, network traffic, and disk accesses of one process impact perf of others, this may be acceptable when there is adequate provisioning and a closed user group(such as in the case of computational grids, or the experimental PlanetLab platform), but not when resources are oversubscribed, or users uncooperative; one way to address this problem is to retrofit support for perf isolation to os, this has been demonstrated to a greater or lesser degree with resource containers, linux/rk, qlinux, and silk, one difficulty with such approaches is ensuring that all resource usage is accounted to the correct process -consider, for example, the complext interactions bt applications due to buffer cache or page replacement algorithms, this is effectively the problem of QoS crosstalk within the os, performing multiplexing at a low level can mitigate this problem, as demonstrated by the Exokernel and Nemesis os, unintentional or undesired interactions bt tasks are minimized; 
we use this same basic approach to build Xen, which multiplexes physical resources at the granularity of an entire os and is able to provide perf isolation bt them, in contrast to process-levlel multiplexing this also allows a range of guest os to gracefully coexist rather than mandating a specific application binary interface, there is a price to pay for this flexibility -running a full os is more heavyweight than running a process, both in terms of initialization(such as booting or resuming versus fork and exec), and in terms of resource consumption; 
for our target of up to 100 hosted os instances, we believe this price is worth paying -it allows individual users to run unmodified binaries, or collections of binaries, in a resource controlled fashion(such as an Apache server along with a PostgreSQL backend), furthermore it provides an extremely high level of flexibility since the user can dynamically create the precise exec environment their sw requires, unfortunate config interactions bt various services and applications are avoided(such as each windows instance maintains its own strategy).
### Xen :Approach and Overview
in a traditional vmm the virtual hw exposed is functionally identical to underlying machine, although full virtualization has obvious benefits of allowing unmodified os to be hosted, it also has a number of drawbacks, this is particularly true for the prevalent IA-32, x86, architecture; 
support for full virtualization was never part of the x86 architectural design, certain supervisor instructions must be handled by vmm for correct virtualization, but executing these with insufficient privilege fails silently rather than causing a convenient trap, efficiently virtualizing the x86 mmu is also difficult, these problems can be solved but only at the cost of increased complexity and reduced perf; VMware's ESX Server dynamically rewrites portions of hosted machine code to insert traps wherever vmm intervention might be required, this translation is applied to the entire guest os kernel(with associated translation, exec, and caching costs) since all non-trapping priviledged instructions must be caught and handled, ESX Server implements shadow versions of system structures such as page tables and maintains consistency with the virtual tables by trapping every update attempt -this approach has a high cost for update-intensive ops such as creating a new application process; 
notwithstanding the intricacies of x86, there are other arguments against full virtualization, in particular, there are situations in which it is desirable for hosted os to see real as well as virtual resources -providing both real and virtual time allows a guest os to better support time-sensitive tasks, and to correctly handle tcp timeouts and rtt estimates, while exposing real machine addr allows a guest os to improve perf by using superpages or page coloring; 
we avoid drawbacks of full virtualization by presenting a vm abstraction that is similar but not identical to the underlying hw -an approach which has been dubbed paravirtualization, this promises improved perf, although it does require modifications to the guest os, however, we do not require changes to the ABI -application binary interface, and hence no modifications are required to guest applications, we distill the discussion so far into a set of design principles including: (i)suport for unmodified application binaries is essential, or users will not transition to Xen, hence we must virtualize all architectural features required by existing standard ABIs, (ii)supporting full multiapplication os is important, as this allows complex server configs to be virtualized within a single guest os instance, (iii)paravirtualization is necessary to obtain high perf and strong resource isolation on uncooperative machine architestures such as x86, (iv)even on cooperative machine structures, completely hiding effects of resource virtualization from guest os risks both correctness and perf; 
note that our paravirtualized x86 abstraction is quite different from that proposed by the recent Denali project, Denali is designed to support thousands of vms running network services, the vast majority of which are small-scale and unpopular, in contrast, Xen is intended to scale to approximately 100 vms running industry standard applications and services, given these very different goals, it is instructive to contrast Denali's design choices with our own principles including: (i)Denali does not target existing ABIs, and so can elide certain architectural features from their vm interface, for example, Denali does not fully support x86 segmentation although it is exported(and widely used, such as segments are frequently used by thread libs to address thread-local data) in the ABIs of NetBSD, linux, windows xp; (ii)Denali implementation does not address the problem of supporting application multiplexing, nor multiple addr spaces, within a single guest os, rather, applications are linked explicitly against an instance of the Ilwaco guest os in a manner rather reminiscent of a libOS in the Exokernel, hence each vm essentially hosts a single-user single-application unprotected os, by contrast, in Xen a single vm hosts a real os which may itself securely multiplex thousands of unmodified user-level processes, although a prototype virtual mmu has been developed which may help Denali in this area, we are unaware of any published technical details or evaluation; (iii)in the Denali archiecture the vmm performs all paging to and from disk, this is perhaps related to lack of mem-mgmt support at virtualization layer, paging within vmm is contrary to our goal of perf isolation -malicious vms can encourage thrashing behaviour, unfairly depriving others of cpu time and disk bdwidth, in Xen we expect each guest os to perform its own paging using its own guaranteed mem reservation and disk allocation(an idea previously exploited by self-paging); (iv) Denali virtualizes the namespaces of all machine resources, taking the view that no vm can access the resource allocations of another vm if it cannot name them(such as vms have no knowledge of hw addrs but only the virtual addrs created for them by Denali), in contrast, we believe that secure access control within the hypervisor is sufficient to ensure protection -furthermore, as discussed previously, there are strong correctness and perf arguments for making physical resources directly visible to guest os; in the following section we describe vm abstraction exported by Xen and discuss how a guest os must be modified to conform to this, note that here we reserve the term guest os to refer to one of the os that Xen can host and we use the term domain to refer to a running vm within which a guest os executes, the distinction is analogous to that bt a program and a process in a conventional system, we call Xen itself the hypervisor since it operates at a higher privilege level than the supervisor code of the guest os that it hosts; 
**vm interface:** [click for an overview of the paravirtualized x86 interface](./img/paravirtualized-x86-interface.png). factored into 3 broad aspects of the system: mem mgmt, cpu, device io, in the following we address each machine subsystem in turn, and discuss how each is presented in our paravirtualized architecture, note that although certain parts of our implementation, such as mem mgmt, are specific to x86, many aspects(such as our virtual cpu and io devices) can be readily applied to other machine architectures, furthermore, x86 represents a worst case in the areas where it differs significantly from risc-style processors, such as efficiently virtualizing hw page tables is more difficult than virtualizing a sw-managed tlb; 
**mem mgmt:** virtualizing mem is undoubtedly the most difficult part of paravirtualizing an architecture, both in terms of mechanisms required in the hypervisor and modifications required to port each guest os, the task is easier if architecture provides a sw-managed tlb as these can be efficiently virtualized in a simple manner, a tagged tlb is another useful feature supported by most server-class risc architectures including: Alpha, MIPS, SPARC, associating an addr-space identifier tag with each tlb entry allows the hypervisor and each guest os to efficiently coexist in separate addr spaces because there is no need to flush the entire tlb when transferring exec; unfortunately, x86 does not have a sw-managed tlb, instead, tlb misses are serviced automatically by the processor by walking the page table structure in hw, hence to achieve the best possible perf, all valid page translators for the current addr space should be present in the hw-accessible page table, moreover, because tlb is not targeted, addr space switches typically require a complete tlb flush, given these limitations, we made 2 decisions: (i)guest os are responsible for allocating and managing the hw page tables, with minimal involvement from Xen to ensure safety and isolation, (ii)Xen exists in a 64MB section at the top of every addr space, hence avoiding a tlb flush when entering and leaving the hypervisor; each time a guest os requires a new page table, perhaps because a new process is being created, it allocates and initializes a page from its own mem reservation and registers it with Xen, at this point os must relinquish direct write privileges to the page-table mem, all subsequent updates must be validated by Xen, this restrictes updates in a number of ways, including only allowing an os to map pages that it owns and disallowing writable mappings of page tables; guest os may batch update reqs to amortize overhead of entering the hypervisor; the top 64MB region of each addr space, which is reserved for Xen, is not accessible or remappable by guest os, however, this addr region is not used by any of the common x86 ABIs, hence this restriction does not break application compatibility; segmentation is virtualized in a similar way, by validating updates to hw segment descriptor tables, the only restrictions on x86 segment descriptors are: (i)they must have lower privilege than Xen, (ii)they may not allow any access to Xen-reserved portion of the addr space; 
**cpu:** virtualizing cpu has several implications for guest os, principally, insertion of a hypervisor below os violates usual assumption that os is the most privileged entity in the system, in order to protect the hypervisor from os misbehavior(and domains from one another), guest os must be modified to run at a lower privilege level; many processor archiectures only provide 2 privilege levels, in these cases guest os would share the lower privilege level with applications, guest os would then protect itself by running in a separate addr space from its applications, and indirectly pass control to and from applications via hypervisor to set the virtual privilege level and change current addr space, again, if processor's tlb supports addr-space tags then expensive tlb flushes can be avoided; efficient virtualization of privilege levels is possible on x86 because it supports 4 distinct privilege levels in hw, the x86 privilege levels are generally described as rings, and are numbered from 0(most privileged) to 3(least privileged), os code typically executes in ring0 because no other ring can execute privileged instructions, while ring3 is generally used for application code, to our knowledge, rings1&2 have not been used by any well-known x86 os since os/2, any os which follows this common arrangement can be ported to Xen by modifying it to execute in ring1, this prevents guest os from directly executing privileged instructions, yet it remains safely isolated from applications running in ring3; privileged instructions are paravirtualized by requiring them to be validated and executed within Xen -this applies to ops such as installing a new page table, or yielding the processor when idle(rather than attempting to hlt it), any guest os attempt to directly execute a privileged instruction is failed by the processor, either silently or by taking a fault, since only Xen executes at a sufficiently privileged level; exceptions, including mem faults and sw traps, are virtualized on x86 very straightforwardly, a table describing the handler for each type of exception is registered with Xen for validation, the handlers specified in this table are generally identical to those for real x86 hw, this is possible because the exception stack frames are unmodified in our paravirtualized architecture, the sole modifications is to the page fault handler, which would normally read the faulting addr from a privileged processor register(CR2), since this is not possible, we write it into an extended stack frame(*in hindsight, writing the value into a pre-agreed shared mem location rather than modifying the stack frame would have simplified the xp port), when an exception occurs while executing outside ring0, Xen's handler creates a copy of the exception stack frame on guest os stack and returns control to the appropriate registered handler; typically only 2 types of exception occur frequently enough to affect system perf -system calls(which are usually implemented via a sw exception) and page faults, we improve the perf of system calls by allowing each guest os to register a fast exception handler which is accessed directly by the processor without indirecting via ring0, this handler is validated before installing it in the hw exception table, unfortunately it is not possible to apply the same technique to the page fault handler because only code executing in ring0 can read the faulting addr from reg CR2, page faults must therefore always be delivered via Xen so that this reg value can be saved for access in ring1; safely is ensured by validating exception handlers when they are presented to Xen, the only required check is that the handler's code segment does not specify exec in ring0, since no guest os can create such a segment it suffices to compare the specified segment selector to a small number of static values which are reserved by Xen, apart from this, any other handler problems are fixed up during exception propagation -for example, if the handler's code segment is not present or if the handler is not paged into mem then an appropriate fault will be taken when Xen executes the iret instruction which returns to the handler, Xen detects these double faults by checking the faulting program counter value -if addr resides within the exception-virtualizing code then the offending guest os is terminated; note that this lazy checking is safe even for the direct system-call handler -access faults will occur when cpu attempts to directly jump to guest os handler, in this case, the faulting addr will be outside Xen(since Xen will never execute a guest os system call) and so the fault is virtualized in the normal way, if propagation of the fault causes a further double fault then guest os is terminated as described above; 
**device io:** rather than emulating existing hw devices, as is typically done in fully virtualized environments, Xen exposes a set of clean and simple device abstractions, this allows us to design an interface that is both efficient and satisfies our requirements for protection and isolation, to this end, io data is transferred to and from each domain via Xen, using shared-mem, asynchronous buffer-descriptor rings, these provide a high-perf comm mechanism for passing buffer info vertically through the system, while allowing Xen to efficiently perform validation checks(such as checking that buffers are contained within a domain's mem reservation); similar to hw interrupts, Xen supports a lightweight event-delivery mechanism which is used for sending asynchronous notifications to a domain, these notifications are made by updating a bitmap of pending event types and, optionally, by calling an event handler specified of guest os(such as to avoid extra costs incurred by frequent wake-up notifications).
### The Cost of Porting an OS to Xen
[click for cost, in lines of code, of porting commodity os to Xen's paravirtualized x86 environment](./img/cost-of-porting-commodity-operating-systems-to-Xen-paravirtualized-x86-environment.png), note that our NetBSD port is at a very early stage, and hence we report no figures here, the xp port is more advanced, but still in progress, it can execute a number of user-space applications from a ram disk, but it currently lacks any virtual io drivers, for this reason, figures for xp's virtual device drivers are not presented, however, as with linux, we expect these drivers to be small and simple due to the idealized hw abstraction presented by Xen; 
windows xp required a surprising number of modifications to its architecture independent os code because it uses a variety of structures and unions for accessing PTEs -page-table entries, each page-table access had to be separately modified, although some of this process was automated with scripts, in contrast, linux needed far fewer modifications to its generic mem system as it uses pre-processor macros to access PTEs -the macro definitions provide a convenient place to add translation and hypervisor calls required by paravirtualization; in both os, the architecture-specific sections are effectively a port of x86 code to our paravirtualized architecture, this involved rewriting routines which used privileged instructions, and removing a large amount of low-level system initialization code, again, more changes were required in windows xp, mainly due to presence of legacy 16-bit emulation code and need for a somewhat different boot-loading mechanism, note that the x86-specific code base in xp is substantially larger than in linux and hence a larger porting effort should be expected.
### Control and Mgmt
throughout the design and implementation of Xen, a goal has been to separate policy from mechanism wherever possible, although the hypervisor must be involved in data-path aspects(such as scheduling cpu bt domains, filtering network packets before transmission, or enforcing access control when reading data blocks), there is no need for it to be involved in, or even aware of, higher level issues such as how cpu is be to shared, or which kinds of packet each domain may transmit; 
the resulting architecture is one in which the hypervisor itself provides only basic control ops, these are exported through an interface accessible from authorized domains, potentially complex policy decisions such as admission control are best performed by mgmt sw running over a guest os rather than in privileged hypervisor code; 
[click for the overall system structure](./img/overall-system-structure-of-a-machine-running-the-xen-hypervisor.png), note that a domain is created at boot time which is permitted to use the control interface, the initial domain, termed Domain0, is responsible for hosting the application-level mgmt sw, the control interface provides the ability to create and terminate other domains and to control their associated scheduling params, physical mem allocations, and the access they are given to the machine's physical disks and network devices; 
in addition to processor and mem resources, the control interface supports creation and delection of VIFs -virtual network interfaces and VBDs -virtual block devices, these virtual io devices have associated access-control info which determines which domains can access them, and with what restrictions(such as a read-only vbd may be created, or a vif may filter ip packets to prevent source-addr spoofing); 
this control interface, together with profiling stats on current state of the system, is exported to a suite of application-level mgmt sw running in Domain0, this complement of administrative tools allow convenient mgmt of the entire server -current tools can create and destroy domains, set network filters and routing rules, monitor per-domain network activity at packet and flow granularity, and create and delete virtual network interfaces and virtual block devices, we anticipate dev of higher-level tools to further automate the application of administrative policy.
### Detailed Design
in this section we introduce design of the major subsystems that make up a Xen-based server, in each case we present both Xen and guest os functionality for clarity of exposition, the current discussion of guest os focuses on XenoLinux as this is the most mature, nonetheless our ongoing porting of windows xp and netbsd gives us confidence that Xen is guest os agnostic.
### Control Transfer :Hypercalls and Events
2 mechanisms exist for control interactions bt Xen and an overlying domain :synchronous calls from a domain to Xen may be made using a hypercall, while notifications are delivered to domains from Xen using an asynchronous event mechanism; 
the hypercall interface allows domains to perform a synchronous sw trap into hypervisor to perform a privileged op, analogous to use of system calls in conventional os, an example use of a hypercall is to request a set of page-table updates, in which Xen validates and applies a list of updates, returning control to the calling domain when this is completed; 
comm from Xen to a domain is provided through an asynchronous event mechanism, which replaces usual delivery mechanisms for device interrupts and allows lightweight notification of important events such as domain-termination reqs, akin to traditionl unix signals, there are only a small number of events, each acting to flag a particular type of occurence, for example, events are used to indicate that new data has been received over the network, or that a virtual disk req has completed; 
pending events are stored in a per-domain bitmask which is updated by Xen before invoking an event-callback handler specified by guest os, the callback handler is responsible for resetting the set of pending events, and responding to the notifications in an appropriate manner, a domain may explicitly defer event handling by setting a Xen-readable sw flag :this is analogous to disabling interrupts on a real processor.
### Data Transfer :I/O Rings
the presence of a hypervisor means there is an additional protection domain bt guest os and io devices, so it is crucial that a data transfer mechanism be provided that allows data to move vertically through the system with as little overhead as possible; 
2 main factors have shaped the design of our io-transfer mechanism :resource mgmt and event notification, for resource accountability, we attempt to minimize the work required to demultiplex data to a specific domain when an interrupt is received from a device -the overhead of managing buffers is carried out later where computation may be accounted to the appropriate domain, similarly, mem committed to device io is provided by relevant domains wherever possible to prevent crosstalk inherent in shared buffer pools, io buffers are protected during data transfer by pinning underlying page frames within Xen; 
[click for structure of our io descriptor rings](./img/structure-of-asynchronous-io-rings.png), here a ring is a circular queue of descriptors allocated by a domain but accessible from within Xen, descriptors do not directly contain io data, instead, io data buffers are allocated out-of-band by guest os and indirectly referenced by io descriptors, access to each ring is based around 2 pairs of producer-consumer pointers :domains place reqs on a ring, advancing a req producer pointer, and Xen removes these reqs for handling, advancing an associated req consumer pointer, responses are placed back on the ring similarly, save with Xen as the producer and guest os as the consumer, there is no requirement that reqs be processed in order :guest os associates a unique identifier with each req which is reproduced in the associated response, this allows Xen to unambiguously reorder io ops due to scheduling or priority considerations; 
this structure is sufficiently generic to support a number of different device paradigms, for example, a set of reqs can provide buffers for network packet reception, subsequent responses then signal arrival of packets into these buffers, reordering is useful when dealing with disk reqs as it allows them to be scheduled within Xen for efficiency, and use of descriptors with out-of-band buffers makes implementing zero-copy transfer easy; 
we decouple production of reqs or responses from the notification of the other party -in the case of reqs, a domain may enqueue multiple entries before invoking a hypercall to alert Xen, in the case of responses, a domain can defer delivery of a notificatioon event by specifying a threshold number of responses, this allows each domain to trade-off latency and throughput requirements, similarly to the flow-aware interrupt dispatch in the ArseNIC Gigabit Ethernet interface.
### Building a New Domain
the task of building the initial guest os structures for a new domain is mostly delegated to Domain0 which uses its privileged control interfaces(see section Control and Mgmt) to access the new domain's mem and inform Xen of initial reg state, this approach has a number of advantages compared with building a domain entirely within Xen, including reduced hypervisor complexity and improved rebustness(accesses to privileged interface are sanity checked which allowed us to catch many bugs during initial deployment); however, most important is the case with which the building process can be extended and specialized to cope with new guest os, for example, the boot-time addr space assumed by the linux kernel is considerably simpler than that expected by windows xp, it would be possible to specify a fixed initial mem layout for all guest os, but this would require additional bootstrap code within every guest os to lay things out as required by the rest of os, unfortunately this type of code is tricky to implement correctly, for simplicity and robustness it is therefore better to implement it within Domain0 which can provide much richer diagnostics and debugging support than a bootstrap environment.
****
### Programming Models and Frameworks
### MapReuce :Simplified Data Processing on Large Clusters
### [Dean2004](https://www.usenix.org/legacy/events/osdi04/tech/full_papers/dean/dean.pdf)
MapReduce is a programming model and an associated implementation for processing and generating large datasets, users specify a map function that processes a key-value pair to generate a set of immediate key-value pairs, and a reduce function that merges all intermediate values associated with the same immediate key, many real world tasks are expressible in this model; 
programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines, the runtime system takes care of details of partitioning input data, scheduling the program's exec across a set of machines, handling machine failures, and managing required inter-machine comm, this allows programmers without any experience with parallel and distributed systems to easily utilize resources of a large distributed system; 
our implementation of MapReduce runs on a large cluster of commidity machines and it is highly scalable :a typical MapReduce computation processes many terabytes of data on thousands of machines, programmers find the system easy to use :hundreds of MapReduce programs have been implemented and 1000+ MapReduce jobs are executed on Google's clusters everyday. 
over the past 5 years, Google have implemented hundreds of special-purpose computations that process large amounts of raw data, such as crawled documents, web req logs, etc., to compute various kinds of derived data, such as inverted indices, various implementations of the graph structure of web documents, the summaries of #pages crawled per host, the set of most frequent queries in a given day, etc., most such computations are conceptually straightforward, however, input data is usually large and computatins have to be distributed across hundreds of thousands of machines in order to finish in a reasonable amount of time, the issues of how to parallelize computation, distribute data, and handle failures conspire to obscure original simple computation with large amounts of complex code to deal with these issues; 
as a reaction to this complexity, we designed a new abstraction that allows us to express the simple computations we were trying to perform but hides the messy details of parallelization, fault-tolerance, data distribution, and load balancing in a lib, our abstraction is inspired by map, reduce primitives present in Lisp and many other functional langs, we realized that most of our computations involved applying a map op to each logical record in our input in order to compute a set of intermediate key-value pairs, and then applying a reduce op to all values that shared the same key in order to combine derived data appropriately, our use of a functional model with user-specified map, reduce ops allows us to parallelize large computations easily and to use re-exec as the primary mechanism for fault tolerance; 
the major contributions of this work are a simple and powerful interface that enables automatic parallelization and distribution of large-scale computations, combined with an implementation of this interface that achieves high perf on large clusters of commidity PCs.
### Programming Model
the computation takes a set of input key-value pairs, and produces a set of output key-value pairs, the user of MapReduce lib expresses the computation as 2 functions :Map, Reduce, here, Map, written by user takes an input pair and produces a set of intermediate key-value pairs, the MapReduce lib groups together all intermediate values associated with the same intermediate key *I* and passes them to the Reduce function, Reduce, also written by user accepts an intermediate key *I* and and a set of values for that key, it merges together these values to form a possibly smaller set of values, typically just 0 or 1 output value is produced per Reduce invocation, the intermediate values are supplied to user's reduce function via an iterator, this allows us to handle lists of values that are too large to fit in mem. 
**example:** consider the problem of counting #occurrences of each word in a large collection of documents, user would write code similar to the following pseudo-code:
```
map(String key, String value):
  // key: document time
  // value: document contents
  for each word w in value:
    EmitIntermediate(w, "1");
reduce(String key, Iterator values):
  // key: a word
  // values: a list of counts
  int result = 0;
  for each v in values:
    result += ParseInt(v);
  Emit(AsString(result));
```
here, map function emits each word plus an associated count of occurrences(just 1 in this simple example), reduce function sums together all counts emitted for a particular word; 
in addition, user writes code to fill a mapreduce specification object with names of input and output files, and optional tuning params, user then invokes MapReduce function, passing it the specification obejct; user's code is linked together with MapReduce lib(implemented in C++);
**types:** even though the previous pseudo-code is written in terms of string inputs and outputs, conecptually the map and reduce functions supplied by user have associated types:
```
map    (k1,v1)       ->list(k2,v2)
reduce (k2,list(v2)) ->list(v2)
```
i.e., the input keys and values are drawn from a different domain than the output keys and values, furthermore, the intermediate keys and values are from the same domains as the output keys and values; 
our C++ implementation passes strings to and from user-defined functions and leaves it to user code to convert bt strings and appropriate types;
**more examples(that can be easily expressed as MapReduce computations):** distributed grep -map function emits a line if it matches a supplied pattern, reduce function is an identity function that just copies supplied intermediate data to the output; count of url access freq -map function processes logs of web page reqs and outputs <URL,1>, reduce function adds together all values for the same url and emits a <URL,totalcount> pair; reverse web-link graph -map function outputs <target,source> pairs for each link to a target url found in a page named source, reduce function concatenates the list of all source urls associated with a given target url and emits the pair <target,list(source)>; term-vector per host -a term vector summarizes the most important words that occur in a document or a set of documents as a list of <word,frequency> pairs, map function emits a <hostname,termvector> pair for each input document(where hostname is extracted from the url of the document), reduce function is passed all per-document term vectors for a given host, it adds these term vectors together, throwing away infrequent terms, and then emits a final <hostname,termvector> pair; inverted index -map function parses each document and emits a sequence of <word,documentID> pairs, reduce function accepts all pairs for a given word and sorts the corresponding document ids and emits a <word,list(documentID)> pair, the set of all output pairs forms a simple inverted index, it is easy to agument this computation to keep track of word positions; distributed sort -map function extracts the key from each record and emits a <key,record> pair, reduce function emits all pairs unchanges.
### Implementation
many different implementations of MapReduce interface are possible, the right choice depends on the environment, for example, one implementation may be suitable for a small shared-mem machine, another for a large numa multiprocessor, and yet another for an even larger collection of networked machines; 
this section describes an implementation targeted to the computing environment in wide use at Google -large clustes of commodity PCs connected together with switched ethernet, it is: (i)machines are typically dualprocessor x86 processers running linux, with 2-4GB of mem per machine, (ii)commodity networking hw is used -typically either 100 megabits/sec or 1 gigabit/sec at the machine level, but averaging considerably less in overall bisection bdwidth, (iii)a cluster consists of hundreds or thousands of machines, and therefore machine failures are common, (iv)storage is provided by inexpensive ide disks attached directly to individual machines, a distributed file system developed in-house is used to manage the data stored on these disks, the file system users replication to provide availability and reliability on top of unreliable hw, (v)users submit jobs to a scheduling system, each job consists of a set of tasks, and is mapped by scheduler to a set of available machines within a cluster; 
**exec overview:** Map invocations are distributed across multiple machines by automatically partitionalling input data into a set of *M* splits, input splits can be processed in parallel by different machines, Reduce invocations are distributed by partitioning the intermediate key space into *R* pieces using a partitioning function(such as *hash(key)* **mod** *R*), #partition(*R*) and the partitioning function are specified by user; [click for overall flow a MapReduce op in our implementation](./img/mapreduce.png), when our user program calls MapReduce function, the following sequence of actions occur(here the numbered labels correspond to the numbers in the list below): (i)MapReduce lib in the user program first splits input files into *M* pieces of typically 16MB to 64MB per piece(controllable by user via an optional param), it then starts up many copies of the program on a cluster of machines, (ii)one of the copies of the program is special -the master, the rest are workers that are assigned work by master, there are *M* map tasks and *R* reduce tasks to assign, master picks idle workers and assigns each one a map task or a reduce task, (iii)a worker who is assigned a map task reads the contents of the corresponding input split, it parses key-value pairs out of input data and passes each pair to user-defined Map function, the intermediate key-value pairs produced by Map function are buffered in mem, (iv)periodically, buffered pairs are written to local disk, partitioned into *R* regions by the partitioning function, locations of these buffered pairs on local disk are passed back to master, who is responsible for forwarding these locations to the reduce workers, (v)when a reduce worker is notified by master about these locations, it uses rpcs to read buffered data from local disks of the map workers, when a reduce worker has read all intermediate data, it sorts it by the intermediate keys so that all occurrences of the same key are grouped together, the sorting is needed because typically many different keys map to the same reduce taks, if the amount of intermediate data is too large to fit in mem, an external sort is uses, (vi)the reduce worker iterates over the sorted intermediate data and for each unique intermediate key encountered, it passes the key and the corrsponding set of intermediate values to user's Reduce function, output of Reduce function is appended to a final output file for this reduce partition, (vii) when all map tasks and reduce tasks have been completed, master wakes up user program, at this point, MapReduce call in the user program returns back to user code; 
after successful completion, output of the mapreduce exec is avialable in the *R* output files(1 per reduce task, with file names as specified by user), typically, users do not need to combine these *R* output files into 1 file -they often pass these files as input to another MapReduce call, or use them from another distributed application that is able to deal with input that is partitioned into multiple files; 
**master data structures:** master keeps several data structures, for each map task and reduce task, it stores the state(idle, in-progress, or complete), and the identity of worker machine(for non-idle tasks); master is the conduit through which the location of intermediate file regions is propagated from map tasks to reduce tasks, hence for each completed map taks, master stores locations and sizes of the *R* intermediate file regions produced by map task, updates to this location and size info are received as map tasks are completed, the info is pushed incrementally to workers that have in-progress reduce tasks; 
**fault tolerance:** since MapReduce lib is designed to help process very large amounts of data using hundreds of thousands of machines, the lib must tolerate machine failures gracefully: (i) worker failure: master pings every worker periodically, if no response is received from a worker in a certain amount of time, master marks the worker as failed, any map tasks completed by worker are reset back to their initial idel state, hence become eligible for scheduling on other workers, similarly, any map task or reduce task in-progress on a failed worker is also reset to idle and becomes eligible for rescheduling; completed map tasks are re-executed on a failure because their output is stored on local disks of the failed machine and is therefore inaccessible, completed reduce tasks do not need to be re-executed since their output is stored in a global file system; when a map task is executed first by worker *A* and then later executed by worker *B*(because *A* failed), all workers executing reduce tasks are notified of the exec, any reduce task that has not already read the data from worker *A* will read data from worker *B*; MapReduce is resilient to large-scale worker failures, for example, during 1 MapReduce op, network maintenance on a running cluster was causing groups of 80 machines at a time to become unreachable for several mins, MapReduce master simply re-executed the work done by unreachable worker machines, and continued to make forward progress, eventually completing MapReduce op; 
(ii) master failure: it is easy to make master write periodic checkpoints of master data structures described above, if master task dies, a new copy can be stored from the last checkpointed state, however, given that there is only a single master, its failure is unlikely, hence our current implementation aborts MapReduce computation if master fails, clients can check for this condition and retry MapReduce op if they desire; 
(iii) semantics in presence of failures: when the user-supplied map and reduce operators are deterministic functions of their input values, our distributed implementation produces the same output as would have been produced by a non-faulting sequential exec of the entire program; we rely on atomic commits of map and reduce task outputs to achieve this property, each in-progress tasks writes its output to private temporary files, a reduce task produces 1 such file, and a map task produces *R* such files(1 per reduce task), when a map task completes, worker sends a msg to master and includes names of the *R* temporary files in the msg, if master receives a completion msg for an already completed map task, it ignores the msg, otherwise, it records names of *R* files in a master data structure; when a reduce task completes, reduce worker atomically renames its temporary output file to the final output file, if the same reduce task is executed on multiple machines, multiple rename calls will be executed for the same final output file, we reply on the atomic rename op provided by underlying file system to guarantee that the final file system state contains just the data produced by 1 exec of the reduce task; the vast majority of our map and reduce operators are deterministic, and the fact that out semantics are equivalent to a sequential exec in this case makes it very easy for programmers to reason about their program's behavior, when map and/or reduce operators are non-deterministic, we provide weaker but still reasonable semantics, in the presence of non-deterministic operators, output of a particular reduce task *R1* is equivalent to output for *R1* produced by a sequential exec of the non-deterministic program, however, output for a different reduce task *R2* may correspond to output for *R2* produced by a different sequential exec of the non-deterministic program; consider map task *M* and reduce tasks *R1, R2*, let *e(Ri)* be the exec of *Ri* that committed(there is exactly 1 such exec), the weaker semantics arise because *e(R1)* may have read output produced by 1 exec of *M* and *e(R2)* may have read output produced by a different exec of *M*; 
**locality:** network bdwidth is a relatively scarce resource in our computing environment, we conserve network bdwidth by taking advantage of the fact that input data(managed by GFS) is stored on local disks of the machines that make up our cluster, GFS delivers each file into 64MB blocks, and stores several copies of each block(typically 3 copies) on different machines; MapReduce master takes location info of input files into account and attempts to schedule a map task on a machine that contains a replica of the corrsponding input data; failing that, it attempts to schedule a map task near a replica of that task's input data(such as on a worker machine that is on the same network switch as the machine containing the data); when running large MapReduce ops on a significant fraction of the workers in a cluster, most input data is read locally and consumes no network bdwidth; 
**task granularity:** we subdivide the map phase into *M* pieces and the reduce phase into *R* pieces, ideally, *M, R* should be much larger than #worker machines; having each worker perform many different tasks improves dynamic load balancing, and also speeds up recovery when a worker fails :the many map tasks it has completed can be spread out across all other worker machines; there are practical bounds on how large *M, R* can be in our implementation, since master must make *O(M+R)* scheduling decisions and keeps *O(M\*R)* state in mem(the constant factors for mem usage are small however :the *O(M\*R)* piece for the state consists of approximately 1 byte of data per map task-reduce task pair); furthermore, *R* is often constrained by users because output of each reduce task ends up in a separate output file, in practice, we tend to choose *M* so that each individual task is roughly 16-64MB of input data(so that the locality optimization described above is most effective), and we make *R* a small multiple of #worker machines we expect to use, we often perform MapReduce computations with *M=200000, R=5000, using 2000 worker machines*; 
**backup tasks:** one of the common causes that lengthens local time taken for a MapReduce op is a straggler :a machine that takes an unusually long time to complete one of the last few map or reduce tasks in the computation; straggler can arise for a whole host of reasons, for example, a machine with a bad disk may experience frequent correctable errors that slow its read perf from 30MB/sec to 1MB/sec, the cluster scheduling system may have scheduled other tasks on the machine, causing it to execute MapReduce code more slowly due to competition for cpu, mem, local disk, or network bdwidth, a recent problem we experienced was a bug in machine initialization code that caused processor caches to be disabled :computations on affected machines slowed down by over a factor of 100; we have a general mechanism to alleviate the problem of stragglers, when a MapReduce op is close to completion, master schedules backup exec of the remaining in-progress tasks, the task is marked as completed whenever either the primary or the backup exec completes, we have tuned this mechanism so that it typically increases the computational resources used by the op by no more than a few percent, we have found that this significally reduces the time to complete large MapReduce ops.
### Spark :Cluster Computing with Working Sets
### [Zaharia10](https://www.usenix.org/legacy/event/hotcloud10/tech/full_papers/Zaharia.pdf)
MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters, however, most of these systems are built around an acylic data flow model that is not suitable for other popular applications, here we focus on one such class of applications :those that reuse a working set of data across multiple parallel ops, this includes many iterative ml algorithms as well as interactive data analysis tools, we propose a new framework called Spark that supports these applications while retaining the scalability and fault tolerance of MapReduce, to achieve these goals, Spark introduces an abstraction called RDDs -resilient distributed datasets, an rdd is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost, Spark can outperform Hadoop by 10x in iterative ml jobs and can be used to interactively query a 39GB dataset with sub-second response time. 
a new model of cluster computing has been widely popular, in which data-parallel computations are executed on clusters of unreliable machines by systems that automatically provide locality-aware scheduling, fault tolerance, and load balancing; MapReduce pioneered this model while system like Dryad, MapReudceMerge generalized types of data flows supported, these systems achieve their scalability and fault tolerance by providing a programming model where user creates acyclic data flow graphs to pass input data through a set of operators, this allows underlying system to manage scheduling and to react to faults without user intervention; 
while this data flow programming model is useful for a large class of applications, there are applications that cannot be expressed efficiently as acyclic data flows, here we focus on one such class of applications :those that reuse a working set of data across multiple parallel ops, this includes 2 use cases where we have seen Hadoop users report that MapReduce is deficient: (i)iterative jobs -many common ml algorithms apply a function repeatedly to the same dataset to optimize a param(such as through gradient descent), while each iteration can be expressed as a MapReduce/Dryad job, each job must reload the data from disk, incurring a significant perf penalty, (ii)interactive analytics -Hadoop is often used to run ad-hoc exploratory queries on large datasets, through sql interfaces such as Pig, Hive, ideally, a user would be able to load a dataset of interest into mem across a number of machines and query it repeatedly, however, with Hadoop each query incurs significant latency(tens of secs) because it runs as a separate MapReduce job and reads data from disk; 
here we present a new cluster computing framework called Spark, which supports applications with working sets while providing similar scalability and fault tolerance properties to MapReduce; 
the main abstraction in Spark is that of a rdd, which represents a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost; users can explicitly cache an rdd in mem across a set of machines and reuse it in multiple MapReduce-like parallel ops; rdds achieve fault tolerance through a notion of lineage :if a partition of an rdd is lost, the rdd has enough info about how it was was derived from other rdds to be able to rebuild just that partition; although rdds are not a general shared mem abstraction, trhey represent a sweet-spot bt expressively on the one hand and scalability and reliablity on the other hand, and we have found them well-suited for a variety of applications; 
Spark is implemented in Scala, a statically typed high-level programming lang for java vm, and exposes a functional programming interface similar to DryadLINQ, in addition, Spark can be used interactively from a modified version of the Scala interpreter, which allows user to define rdds, functions, vars, and classes, and use them in parallel ops on a cluster, we believe that Spark is the first system to allow an efficient, general-purpose programming lang to be used interactively to process large datasets on a cluster; 
although our implementation of Spark is still a prototype, early experience with the system is encouraging, we show that Spark can outperform Hadoop by 10x in iterative ml wkloads and can be used interactively to scan a 39GB dataset with sub-second latency.
### Programming Model
to use Spark, developers write a driver program that implements the high-level control flow of their application and launches various ops in parallel, Spark provides 2 main abstractions for parallel programming :rdds and parallel ops, on these datasets(invoked by passing a function to apply on a dataset), in addition, Spark supports 2 restricted types of a shared vars that can be used in functions running on the cluster, which we shall explain later; 
**rdds:** a rdd is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost, elements of an rdd need to exist in physical storage, instead, a handle to an rdd contains enough info to compute the rdd starting from data in reliable storage, this means rdds can always be reconstructed if nodes fail; in Spark, each rdd is represented by a Scala object, Spark lets programmers construct rdds in 4 ways including: (i)from a file in a shared file system such as HDFS -Hadoop distributed file system, (ii)by parallelizing a Scala collection(such as an array) in the driver program which means dividing it into a number of slices that will be sent to multiple nodes, (iii)by transforming an existing rdd, a dataset with elements of type A can be transformed into a dataset with elements of type B using an op called flatMap, which passes each element through a user-provided function of type *A->List\[B\]*(*faltMap has the same semantics as the map in MapReduce, but map is usually used to refer to a one2one function of type *A->B* in Scala), other transformations can be expressed using faltMap, including map(passes elements through a function of type *A->B*) and filter(pick elements matching a predicate), (iv)by changing the persistence of an existing rdd, by default, rdds are lazy and ephemeral i.e. partitions of a dataset are materialized on demand when they are used in a parallel op(such as by passing a block of a file through a map function), and are discarded from mem after use(*this is how distributed collections function in DryadLINQ), however, a user can alter the persistence of an rdd through 2 actions :the cache action leaves the dataset lazy but hints that it shoudl be kept in mem after the first time it is computed because it will be reused, the save action evaluates the dataset and writes it to a distributed file system such as hdfs the saved version is used in future ops on it; we note that our cache action is only a hint :if there is not enough mem in the cluster to cache all partitions of a dataset, Spark will recompute them when they are used, we chose this design so that Spark programs keep working(at reduced perf) if nodes fail or if a dataset is too big, this idea is loosely analogous to vm; we also plan to extend Spark to support other levels of persistence(such as in-mem replication across multiple nodes), our goal is to let users trade off bt the cost of storing an rdd, the speed of accessing it, the prob of losing part of it, and the cost of recomputing it; 
**parallel ops:** several parallel ops can be performed on rdds including: (i)reduce -combines dataset elements using an associative function to produce a result at the driver program, (ii)collect -sends all elements of the dataset to the driver program such as an easy way to update an array in parallel is to parallelize, map, and collect the array, (iii)foreach -passes each element through a user provided functions, this is only done for the side effects of the function(which might be to copy data to another system or to update a shared var as explained below); we note that Spark does not currently support a grouped reduce op as in MapReduce, results are only collected at 1 process(the driver, however, local functions are first performed at each node), we plan to support grouped reductions in the future using a shuffle transformation on distributed datasets, however, even using a single reducer is enough to express a variety of useful algorithms such as a recent paper on MapReduce for ml on multicore systems implemented 10 learning algorithms without supporting parallel reduction; 
**shared vars:** programmers invoke ops like map, filter, and reduce by passing closures(functions) to Spark, as is typical in functional programming, these clusters can refer to vars in the scope where they are created; normally, when Spark runs a closure on a worker node, these vars are copied to the worker, however, Spark also lets programmers create 2 restricted types of shared vars to support 2 simple but common usage patterns including: (i)broadcast vars -if a large read-only piece of data(such as a lookup table) is used in multiple parallel ops, it is preferable to distribute it to workers only once instead of packaging it with every closure, Spark lets programmers create a broadcast var object that wraps the value and ensures that it is only copied to each worker once, (ii)accumulators -these are vars that workers can only add to using an associative op, and that only the driver can read, they can be used to implement counters as in MapReduce and to provide a more imperative syntax for parallel sums, accumulators can be defined for any type that has an add op and a 0 value, due to their add-only semantics, they are easy to make fault-tolerant.
### Examples
we now show some sample Spark programs, note that we omit var types because Scala supports type inference; 
**text search:** suppose that we wish to count lines containing errors in a large log file stored in hdfs, this can be implemented by starting with a file dataset object as follows:
```
val file = spark.textFile("hdfs://...")
val errs = file.filter(_.contains("ERROR"))
val ones = errs.map(_ => 1)
val count = ones.reduce(_+_)
```
we first create a distributed dataset called file that represents the hdfs file as a collection of lines, we transform this dataset to create the set of lines containing ERROR(errs), and then amp each line a 1 and add up these 1s using reduce, the arguments to filter, map, and reduce are Scala syntax for function literals; note that errs and ones are lazy rdds that are never materialized, instead, when reduce is called, each worker node scans input blocks in a streaming manner to evaluate ones, adds these to perform a local reduce, and sends its local count to the driver, when used with lazy datasets in this manner, Spark closely emulates MapReduce; where Spark differs from other frameworks is that it can make some of the intermediate datasets persist across operations, such as if wanted to reuse the errs dataset, we could create a cached rdd from it as follows:
```
val cachedErrs = errs.cache()
```
we would now be able to invoke parallel ops on cachedErrs or on datasets derived from it as usual, but nodes would cache partitions of cachedErrs in mem after the first time they compute them, greatly speeding up subsequent ops on it; 
**logistic regression:** the following program implements logistic regression, an iterative classification algorithm that attempts to find hyperplane *w* that best separates 2 sets of points, the algorithm performs gradient descent :it starts *w* at a random value, and on each iteration, it sums a function of *w* over the data to move *w* in a direction that improves it, it hence benifits from caching the data in mem across iterations:
```
// read points from a text file and cache them
val points = spark.textFile(...).map(parsePoint).cache()
// initiate w to random d-dim vector
var w = Vector.random(D)
// run multiple iterations to update w
for (i <- 1 to ITERATIONS) {
  val grad = spark.accumulator(new Vector(D))
  for (p <- points) { // runs in parallel
    val s = (1/(1+exp(-p.y*(w dot p.x)))-1*p.y)
    grad += s*p.x
  }
  w -= grad.value
}
```
first although we create an rdd called points, we process it by running a for loop over it, the for keyword in Scala is syntactic sugar for invoking the foreach method of a collection with the loop body as a closure i.e. the code *for(p <- points){body}* is equivalent to *points.foreach(p => {body})*, hence we are invoking Spark's parallel foreach op, second to sum up the gradient, we use an accumulator var called gradient(with a value of type Vector), note that the loop adds to gradient using an overloaded += operator, the combination of accumulators and for syntax allows Spark porgrams to look much like imperative serial programs, indeed, this example differs from a serial version of logistic regression in only 3 lines; 
**ALS -alternating least squares:** used for collaborative filtering problems such as predicting users' ratings for movies that they have not seen based on their movie rating history(as in the Netflix Challenge), unlike our previous examples, als is cpu-intensive rather than data-intensive; we briefly sketch als -suppose that we wanted to predict the ratings of *u* users for *m* movies and that we had a partially filled matrix *R* containing known ratings for some user-movie pairs, als models *R* as the product of 2 matrices *M, U* of dims *mtimesk, ktimesu* respectively i.e. each user and each movie has a *k*-dim feature vector describing its characteristics, and a user's rating for a movie is the dot product of its feature vector and the movie's, als solves for *M, U* using the known ratings and then computes *MtimesU* to predict the unknown ones, this is done using the following iterative process: 1.initialize *M* to a random value, 2.optimize *U* given *M* to minimize error on *R*, 3.optimize *M* given *U* to minimize error on *R*, 4.repeat steps2&3 until convergence; als can be parallelized by updating different users/movies on each node in steps2&3, however, because all of the steps use *R*, it is helpful to make *R* a broadcast var so that it does not get re-sent to each node on each step, note that we parallelize the collection 0 until u(a Scala range object) and collect it to update each array:
```
val Rb = spark.broadcast(R)
for (i <- 1 to ITERATIONS) {
  U = spark.parallelize(0 until u).map(j => updateUser(j, Rb, M)).collect()
  M = spark.parallelize(0 until m).map(j => updateUser(j, Rb, U)).collect()
}
```
### Implementation
Spark is built on top of Mesos, a cluster os that lets multiple parallel applications share a cluster in a fine-grained manner and provides an api for applications to launch tasks on a cluster, this allows Spark to run alongside existing cluster computing frameworks such as Mesos ports of Hadoop and MPI, and share data with them, in addition, building on Mesos greatly reduced the programming effort that had to go into Spark; 
the core of Spark is the implementation of rdds, for example, suppose that we define a cached dataset called cachedErrs representing error msgs in a log file, and that we count its elements using map and reduce as in section text search, [these datasets will be stored as a chain of objects capturing the lineage of each rdd, click to view](./img/resilient-distributed-datasets-linear-chain.png), each dataset object contains a pointer to its parent and info about how the parent was transformed; 
internally, each rdd object implementats the same simple interface, which consists of 3 ops including: getPartitions -which returns a list of partition IDs, getIterator(partition) -which iterates over a partition, getPreferredLocations(partition) -which is used for task scheduling to achieve data locality; when a parallel op is invoked on a dataset, Spark creates a task to process each partition of the dataset and sends these tasks to worker nodes, we try to send each task to one of its preferred locations using a technique called delay scheduling, once launched on a worker, each task calls getIterator to start reading its partition; 
the different types of rdds differ only in how they implement the rdd interface, for example, for a HDFS-Textfile, the partitions are block IDs in hdfs, their preferred locations are the block locations, and getIterator opens a stream to read a block, vs., in a MappedDataset, the partitions and preferred locations are the same as for the parent, but the iterator applies the map function to elements of the parent, vs., in a CachedDataset, the getIterator method looks for a locally cached copy of a transformed partition, and each partition's preferred locations start out equal to parent's preferred locations, but get updated after the partition is cached on some node to prefer using that node, this design makes faults easy to handle :if a node fails, its partitions are re-read from their parent datasets andd eventually cached on other nodes; 
shipping tasks to worker requires shipping closures to them -both the closures used to define a distributed dataset, and closures passed to ops such as reduce, to achieve this, we rely on the fact that Scala closures are java objects and can be serialized using java serialization, this is a feature of Scala that makes it relatively straightforward to send a computation to another machine; Scala's built-in closure implementation is not ideal, however, because we have found cases where a closure object references vars in the closure's outer scope that are not actually used in its body, we have filed a bug report about this, but in the meantime, we have solved the issue by performing a static analysis of closure classes' bytecode to detect these unused vars and set the corresponding fields in the closure object to null; 
**shared vars:** the 2 types of shared vars in Spark, broadcast vars and accumulators, are implemented using classes with custom serialization formats, when one creates a broadcast var b with a value v, v is saved to a file in a shared file system, the serialized form of b is a path to this file, when b's value is required on a worker node, Spark first checks whether v is in a local cache, and reads it from the file system if it is not, we initially used hdfs to broadcast vars, but we are developing a more efficient streaming broadcast system; accumulators are implemented using a different serialization trick, each accumulator is given a unique ID when it is created, when the accumulator is saved, its serialized form contains its ID and the 0 value for its type, on the workers, a separate copy of the accumulator is created for each thread that runs a task using thread-local vars, and is set to 0 when a task begins, after each task runs, worker sends a msg to the driver program containing the updates it made to various accumulators, the driver applies updates from each partition of each op only once to prevent double-counting when tasks are re-executed due to failures; 
**interpreter integration:** here we only sketch how we have integrated Spark into Scala interpreter, the Scala interpreter normally operates by compiling a class for each line typed by user, this class includes a singleton object that contains vars or functions on that line and runs the line's code in its constructor, for example, if user types *var x=5* followed by *println(x)*, the interpreter defines a class(say *Linel*) containing *x* and causes the second line to compile to *print(Linel.getInstance().x)*, these classes are loaded into jvm to run each line, to make the interpreter work with Spark, we made 2 changes including: (i)we made the interpreter output the classes it defines to a shared filesystem, from which they can be loaded by workers using a custom java class loader, (ii)we changed the generated code so that the singleton object for each line references the singleton objects for previous lines directly rather than going through the static *getInstance* methods, this allows closures to capture current state of the singletons they reference whenever they are serialized to be sent to a worker, if we had not done this, then updates to the singleton objects(such as a line setting *x=7* above) would not propagate to workers.
### Scaling Distributed Machine Learning with the Parameter Server
### [Li14](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf)
we propose a param server framework for distributed ml problems, both data and wkloads are distributed over worker nodes, while the server nodes maintain globally shared params, represented as dense or sparse vectors and matrices, the framework manages asynchronous data comm bt nodes, and supports flexible consistency models, elastic scalability, and continuous fault tolerance; to demonstrate scalability of the proposed framework we show experimental results on petabytes of real data with billions of examples and params on problems ranging from Sparse Logistic Regression to Latent Dirichlet Allocation and Distributed Sketching. 
distributed optimization and inference is becoming a prerequisite for solving large scale ml problems, at scale, no single machine can solve these problems sufficiently rapidly, due to growth of data and the resulting model complexity, often manifesting itself in an increased number of params, however, implementing an efficient distributed algorithms is not easy, both intensive computational wkloads and the volumne of data comm demand careful system design; 
realistic quantities of training data can range bt 1TB-1PB, this allows one to create powerful and complex models with 10^9-10^12 params, these models are often shared globally by all worker nodes, which must frequently accesses shared params as they perform computation to refine it, sharing imposes 3 challenges including: accessing the params requires an enormous amount of network bdwidth; many ml algorithms are sequential, the resulting barriers hurt perf when cost of synchronization and machine latency is high; at scale, fault tolerance is critical, learning tasks are often performed in a cloud environment where machines can be unreliable and jobs can be preempted; to illustrate the last point, we collected all job logs for a 3-month period from one cluster at a large internet company, [click for stats of batch ml tasks serving a production environment](./img/statistics-of-machine-learning-jobs.png), here task failure is mostly due to being preempted or losing machines without necessary fault tolerance mechanisms; unlike in many research settings where jobs run exclusively on a cluster without contention, fault tolerance is a necessity in real world deployments; 
**contributions:** since its introduction, the param server framework has proliferated in academia and industry, here we describe a 3rd-gen open source implementation of a param server that focuses on the systems aspects of distributed inference, it confers 2 advantages to developers :first by factoring out commonly required components of ml systems, it enables application-specific code to remain concise, at the same time, as a shared platform to target for sysytems-level optimizations, it provides a robust versatile and high-perf implementation capable of handling a diverse array of algorithms from sparse logistic regression to topic models and distributed sketching; our design decisions were guided by wkloads found in real systems, our param server provides 5 key features including: (i)efficient comm -the asynchronous comm models does not block computation(unless requested), it is optimized for ml tasks to reduce network traffic and overhead, (ii)flexible consistency models -relaxed consistency further hides synchronization cost and latency, we allow the algorithm designer to balance algorithmic convergence rate and system efficiency, the best trade-off depends on data, algorithm, and hw, (iii)elastic scalability -new nodes can be added without restarting the running framework, (iv)fault tolerance and durability -recovery from and repair of non-catastrophic machine failures within 1sec, without interrupting computation, vector clocks ensure well-defined behavior after network partition and failure, (v)ease of use -the globally shared params are represented as (potentially sparse) vectors and matrices to facilitate dev of ml applications, the linear algebra data types come with high-perf multithreaded libs; the novelty of proposed system lies in the synergy achieved by picking the right systems techinuqes, adapting them to ml algorithms, and modifying the ml algorithms to be more systems-friendly, in particular, we can relax a number of otherwise hard systems constraints since the associated ml algorithms are quite tolerant to perturbations, the consequence is the first general purpose ml system capable of scaling to industrial scale sizes; 
**engineering challenges:** when solving distributed data analysis problems, the issue of reading and updating params shared bt different worker nodes is ubiquitous, the param server framework provides an efficient mechanism for aggregating and synchronizing model params and stats bt workers, each param server node maintains only a part of the params, and each worker node typically requires only a subset of these params when operating, 2 key challenges arise in constructing a high perf param server system: (i) comm: while params could be updated as key-value pairs in a conventional datastore, using this abstraction natively is inefficient :values are typically small(floats or ints), and the overhead of sending each update as a key value op is high; our insight to improve this situation comes from the observation that many learning algorithms represent params as structured mathematical objects such as vectors, matrices, or tensors, at each logical time(or an iteration) typically a part of the object is updated, i.e. workers usually send a segment of a vector, or an entire row of the matrix, this provides an opportunity to automatically batch both comm of updates and their processing on the param server, and allows the consistency tracking to be implemented efficiently; 
(ii) fault tolerance: as noted earlier, is critical at scale, and for efficient op it must not require a full restart of a long-running computation, live replication of params bt servers supports hot failover, failover and self-repair in turn support dynamic scaling by treating machine removal or addition as failure or repair respectively; [click for an overview of the scale of the largest supervised and unsupervised ml experiments performed on a number of systems](./img/comparison-of-the-public-largest-machine-learning-experiments-each-system-performed.png), when possible, we confirmed the scaling limits with authors of each of these systems(data current as of 4/2014), as is evident, we are able to cover orders of magnitude more data on orders of magnitude more processors than any other published system, furthermore, [click for an overview of main characteristics of several ml systems](./img/attributes-of-distributed-data-analysis-systems.png), our param server offers the greatest degree of flexibility in terms of consistency, it is the only system offering continuous fault tolerance, its native data types make it particularly friendly for data analysis.
### Machine Learning
ml systems are widely used in web search, spam detection, recommendation systems, computational advertising, and document analysis, these systems automatically learn models from examples, termed training data, and typically consist of 3 components including: feature extraction, objective function, and learning; here, feature extraction processes raw data such as documents, images, and user query logs, to obtain feature vectors, where each feature captures an attribute of the training data, preprocessing can be executed efficiently by existing frameworks such as MapReduce; 
**goals:** goal of many ml algorithms can be expressed via a object function, this function captures properties of the learned model, such as low error in the case of classifying emails into ham and spam, how well the data is explained in the context of estimating topics in documents, or a concise summary of counts in the context of sketching data; the learning algorithm typically minimizes this objective function to obtain the model, in general, there is no closed-form solution, instead, learning starts from an initial model, it iteratively refines this model by processing training data, possibly multiple times, to approach the solution, it stops when a (near) optimal solution is found or the model is considered to be converged; the training data may be extremely large, for example, a larget internet company using 1yr of an ad impression log to train an ad click predictor would have trillions of trainig examples, each training example is typically represented as a possibly very high-dim feature vector, hence training data may consist of trillions of trillion-length feature vectors, iteratively processing such large scale data requires enormous computing and bdwidth resources, moreover, billions of new ad impressions may arrive daily, adding this data into the system often improves both prediction accuracy and coverage, but it also requires the learning algorithm to run daily, possibly in real time. efficient exec of these algorithms is the main focus; to motivate design decisions in our system, next we briefly outline the 2 widely used ml technologies that we will use to demonstrate the efficacy of our param server; 
**risk minimization:** the most intuitive variant of ml problems is that of risk minimization, the risk is roughly a measure of prediction error such as if we were to predict tomorrow's stock price the risk might be the deviation bt prediction and the actual value of stock; training data consists of *n* examples, *xi* is the *i*-th such example and is often a vector of length *d*, as noted earlier both *n, d* may be on the order of billions to trillions of examples and dims respectively, in many cases, each training example *xi* is associated with a label *yi*, for example, in ad click prediction, *yi* might be 1 for clicked or -1 for not clicked; risk minimization learns a model that can predict the value *y* of a feature example *x*, the model consists of params *w*, in the simplest example, the model params might be the clickiness of each feature in an ad impression, to predict whether a new impression would be clicked, the system might simply suns its clickiness based upon features present in the impression, namely *xTw := _sum{j=1}{d}xjwj*, and then decide based on the sign; in any learning algorithm, there is an important relationship bt the amount of training data and model size, a more detailed model typically improves accuracy, but only up to a point :if there is too little training data, a highly-detailed model will overfit and become merely a system that uniquely memorizes every item in the training set, on the other hand, a too-small model will fail to capture interesting and relevant attributes of the data that are important to making a correct decision; regularized risk minimization is a method to find a model that balances model complexity and training error, it does so by minimizing sum of 2 terms :a loss *l(x,y,w)* representing prediction error on training data, and a regularizater $\Omega$\[*w*\] penalizing model complexity, a good model is one with low error and low complexity, consequently we strive to minimize: *F(w) = sigma_{i=1}{n}l(xi,yi,w)+*$\Omega$(*w*); the specific loss and regularizer functions used are important to the prediction perf of the ml algorithm, but relatively unimportant for the purpose of this paper :algorithms we present can be used with all of the most popular loss functions and regularizers; in future section Sparse Logistic Regression, we use a high-perf distributed learning algorithm to evaluate the param server, for the sake of simplicity we describe a much simpler model called [distributed subgradient descent](./img/distributed-subgradient-descent.png)(*the unfamiliar reader could read this as gradient descent, the subgradient aspect is simply a generalization to loss functions and regularizers that need not be continuously differentiate, such as |w| at w=0), here, training data is partitioned among all of the workers, which jointly learn the param vecor *w*, the algorithm operates iteratively, in each iteration, every worker independently uses its own training data to determine what changes should be made to *w* in order to get closer to an optimal value, because each worker's updates reflect only its own training data, the system needs a mechanism to allow these updates to mix, it does so by expressing updates as a subgradient -a direction in which the param vector *w* should be shifted -and aggregates all subgradients before applying them to *w*, these gradients are typically scaled down, with considerable attention paid in algorithm design to the right lr $\eta$ that should be applied in order to ensure that the algorithm converges quickly; here the most expensive step is computing the subgradient to update *w*, this task is divided among all of the workers, each of which execute WORKERITERATE, as part of this, workers compute *wTx_{i_k}* which should be infeasible for very high-dim *w*, fortunately, a worker needs to know a coordinate of *w* iff some of its training data references that entry; for example, in ad click prediction one of the key features are the words in ad, if only very few advertisements contain the phrase OSDI 2014, then most workers will not generate any updates to the corresponding entry in *w*, and hence do not require this entry, while the total size of *w* may exceed capacity of a single machine, the working set of entries needed by a particular worker can be trivially cached locally, to illustrate this, we randomly assigned data to workers and then counted the average working set size per worker on the dataset that is used in future section Sparse Logistic Regression, [click for for 100 workers each worker only needs 7.8% of the total params, with 10000 workers this reduces to 0.15%](./img/each-workers-set-of-parameters-shrinks-as-more-workers-are-used.png); 
**generative models:** in a second major of class of ml algorithms, the label is to be applied to training examples is unknown, such settings call for unsupervised algorithms(for labeled training data one can use supervised or semi-supervised algorithms), they attempt to capture underlying structure of the data, for example, a common problem in this area is topic modeling :given a collection of documents, infer the topics contained in each document; when run on for example the SOSP'13 proceedings, an algorithm might generate topics such as "distributed systems", "machine learnig", and "performance", the algorithms infer these topics from the content of the documents themselves, not an external topic list, in practical settings such as content personalization for recommendation systems, the scale of these problems is huge -hundreds of millions of users and billions of documents, making it cirtical to parallelize the algorithms across large clusters; because of their scale and data volumes, these algorithms only became commercially applicable following the introduction of 1st-gen param servers, a key challenge in topic models is that params describing current estimate of how documents are supposed to be generated must be shared; a popular topic modeling approach is LDA -Latent Dirichlet Allocation, while the statistical model is quite different, the resulting algorithm for learning it is very similar to Distributed Subgradient Descent(*the specific algorithm we use in the evaluation is a parallelized variant of a stochastic variational sampler with an update strategy similar to that used in YahooLDA), however, the key difference is that the update step is not a gradient computation, but an estimate of how well the document can be explained by current model, this computation requires access to auxiliary metadata for each document that is updated each time a document is accessed, because of #documents, metadata is typically read from and written back to disk whenever document is processed; here, this auxiliary data is the set of topics assigned to each word of a document, and the param *w* being learned consists of the relative freq of occurrence of a word; as before, each worker needs to store only the params for words occuring in the documents it processes, distributing documents across workers has the same effect as in the previous section -we can process much bigger models than a single worker may load.
### Architecture
an instance of the param server can run more than one algorithm simultaneously, param server nodes are grouped into a server group and several worker groups as shown in [fig4](./img/architecture-of-a-parameter-server-communication-with-several-groups-of-servers.png), a server node in the server group maintains a partition of the globally shared params, server nodes communicate with each other to replicate and/or to migrate params for reliability and scaling, a server manager node maintains a consistent view of the metadata of servers such as node liveness and the assignment of param partitions; 
each worker group runs an application, a worker typically stores locally a portion of the training data to compute local stats such as gradients, workers communicate only with the server nodes(not among themselves), updating and retrieving shared params, there is a scheduler node for each worker group, it assigns tasks to workers and monitors their progress, if workers are added or removed, it reschedules unfinished tasks; 
the param server supports independent param namespaces, this allows a worker group to isolate its set of shared params from others, several worker groups may also share the same namespace :we may use more than one worker group to solve the same dl application to increase parallelization; another example is that of a model being actively queried by some nodes such as online services consuming this model, simultaneously the model is updated by a different group of worker nodes as new training data arrives; 
the param server is designed to simplify developing distributed ml applications such as those discussed in section Machine Learning, the shared params are represented as (key,value) vectors to facilitate linear algebra ops(see future section (key,value) vectors), they are distributed across a grooup of server nodes(see future section consistent hashing), any node can both push out its local params and pull params from remote nodes(see future section range push and pull), by default, wkloads or tasks are executed by worker nodes, however, they can also be assigned to server nodes via user defined functions(see future section user-defined functions on the server), tasks are asynchronous and run in parallel(see future section asynchronous tasks and dependency), the param server provides the algorithm designer with flexibility in choosing a consistency model via task dependency graph(see future section flexible consistency) and predicates to communicate a subset of params(see future section user-defined filters); 
**(key,value) vectors:** the model shared among nodes can be represented as a set of (key,value) pairs, for example, in a loss minimization problem, the pair is a feature ID and its weight, for LDA, the pair is a combination of word ID and topic ID, and a count, each entry of the model can be read and written locally or remotely by its key, this (key,value) abstraction is widely adopted by existing approaches; our param server improves upon this basic approach by acknowledging underlying meaning of these key value items -ml algorithms typically treat the model as a linear algebra object, for example, *w* is used as a vector for both objective function and optimization in Distributed Subgradient Descent by risk minimization, by treating these objects as sparse linear algebra objects, the param server can provide the same functionality as the (key,value) abstraction, but admits important optimized ops such as vector addition *w+u*, multiplication *Xw*, finding the 2-norm ||w||2, and other more sophisticated ops; to support these optimizations, we assume that the keys are ordered, this lets us treat params as (key,value) pairs while endowing them with vector and matrix semantics, where non-existing keys are associated with 0s, this helps with linear algebra in ml, it reduces the programming effort to implement optimization algorithms, beyond convenience, this interface design leads to efficient code by leveraging cpu-efficient multithreaded self-tuning linear algebra libs such as BLAS, LAPACK, ATLAS; 
**range push and pull:** data is sent bt nodes using push and pull ops, in Distributed Subgradient Descent each worker pushes its entries local gradient into servers and then pulls updated weight back, the more advanced algorithm described in [Delayed Block Proximal Gradient](./img/delayed-block-proximal-gradient.png) uses the same pattern except that only a range of keys is communicated each time; the param server optimizes these updates for programmer convenience as well as computational and network bdwidth efficiency by supporting range-based push and pull -if *R* is a key range, then *w.push(R,dest)* sends all existing entries of *w* in key range *R* to destination, which can be either a particular node or a node group such as the server group, similarly, *w.pull(R,dest)* reads all existing entries of *w* in key range *R* from destination, if we set *R* to be the whole key range then the whole vector *w* will be communicated, if we set *R* to include a single key then only an individual entry will be sent; this interface can be extended to communicate any local data structures that share the same keys as *w*, for example, in Distributed Subgradient Descent a worker pushes its temporary local gradient *g* globally shared, however, note that *g* shares the keys of the worker's working set *w*, hence the programmer can use *w.push(R,g,dest)* for local gradients to save mem and also enjoy the optimization discussed in the following sections; 
**user-defined functions on the server:** beyond aggregating data from workers, server nodes can execute user-defined functions, it is beneficial because server nodes often have more complete or up-to-date info about the shared params, in Distributed Subgradient Descent server nodes evaluate subgradients of the regularizer $\Omega$ in order to update *w*, at the same time in Delayed Block Proximal Gradient a more complicated proximal operator is solved by servers to update the model; 
**asynchronous tasks and dependency:** a tasks is issued by a rpc, it can be a push or a pull that a worker issues to servers, it can also be a user-defined function that the scheduler issues to any node, tasks may include any number of subtasks such as the task WorkerIterate in Distributed Subgradient Descent contains 1 push and 1 pull; tasks are executed asynchronously :the caller can perform further computation immediately after issuing a task, the caller marks a task as finished only once it receives the callee's reply, a reply could be the function return of a user-defined function, the (key,value) pairs requested by the pull or an empty acknowledgement, the callee marks a task as finished only if the call of the task is returned and all subtasks issued by this call are finished; by default, callees execute tasks in parallel for best perf, a caller that wishes to serialize task exec can place an exec-after-finished dependency bt tasks, [click for 3 example iterations of WorkerIterate](./img/three-example-iterations-of-workerIterate.png), here Iterations10&11 are independent, but 12 depends on 11, the callee therefore begins iteration 11 immediately after local gradients are computed in 10, however, 12 is postponed until the pull of 11 finishes; task dependencies help implement algorithm logic, for example, the aggregation logic in ServerIterate of Distributed Subgradient Descent updates the weight *w* only after all worker gradients have been aggregated, this can be implemented by having the updating task depend on the push tasks of all workers, the second important use of dependencies is to support the flexible consistency models described next; 
**flexible consistency:** independent tasks improve system efficiency via parallelizing use of cpu, disk, and network bdwidth, however, this may lead to data inconsistency bt nodes, in [fig5](./img/three-example-iterations-of-workerIterate.png) worker *r* starts Iteration11 before *w^(11)* has been pulled back so it uses the old *w_r^(10)* in this iteration and so obtains the same gradient as in 10, namely *g_r^(11)=g_r^(10)*, this inconsistency potentially slows down the convergence progress of Distributed Subgradient Descent, however, some algorithms may be less sensitive to this type of inconsistency, for example, only a segment of *w* is updated each time in Delayed Block Proximal Gradient, hence starting 11 without waiting for 10 causes only a part of *w* to be inconsistent; the best trade-off bt system efficiency and algorithm convergence rate usually depends on a variety of factors including the algorithm's sensitivity to data inconsistency, feature correlation in training data, and capacity difference of hw components, instead of forcing user to adopt 1 particular dependency that may be illsuited to the problem, the param server gives the algorithm designer flexibility in defining consistency models, this is a substantial difference to other ml systems; [we show 3 different models that can be implemented by task dependency, click for their associated directed acyclic graphs](./img/directed-acyclic-graphs-for-different-consistency-models.png), it is: (i)sequential consistency, all tasks are executed one by one, the next task can be started only if the previous one has finished, it produces results identical to the single-thread implementation, and also named Bulk Synchronous Processing, (ii)eventual consistency, is the opposite -all tasks may be started simultaneously, however, this is only recommendable if underlying algorithms are robust with regard to delays, (iii)bounded delay, when a maximal delay time $\tau$ is set, a new task will blocked until all previous tasks $\tau$ times ago have been finished, Delayed Block Proximal Gradient uses such a model, this model provides more flexible controls than the previous two :$\tau$*=0* is the sequential consistency model, and an infinite delay $\tau$*=*$\infty$ becomes the eventual consistency model; note that the dependency graphs may be dynamic such as the scheduler may increase or decrease the maximal delay according to runtime progress to balance system efficiency and convergence of underlying optimization algorithm, in this case caller traverses directed acyclic graph, if the graph is static, caller can send all tasks with directed acyclic graph to callee to reduce synchronization cost; 
**user-friendly filters:** complementary to a scheduler-based flow control, the parameter server supports user-defined filters to selectively synchronize individual (key,value) pairs, allowing fine-grained control of data consistency within a task, the insight is that the optimization algorithm itself usually possesses info on which params are most useful for synchronization, one example is the significantly modified filter which only pushes entries that have changed by more than a threshold since their last synchronization.
### Implementation
servers store params(key value pairs) using consistent hashing(see future section consistent hashing), for fault tolerance entries are replicated using chain replication(see future section replication and consistency), different from prior (key,value) systems the param server is optimized for range-based comm with compression on both data(see future section msgs) and range-based vector clocks(see future section vector clock);
**vector clock:** given the potentially complex task dependency graph and the need for fast recovery, each (key,value) pair is associated with a vector clock, which records time of each individual node on this (key,value) pair, vector clocks are convenient such as for tracking aggregation status or rejecting doubly sent data, however, a naive implementation of the vector clock requires O(nm) space to handle n nodes and m params, with thousands of nodes and billions of params, this is infeasible in terms of mem and bdwidth; fortunately, many params hare the same timestamp as a result of the range-based comm pattern of the param server :if a node pushes params in a range then the timestamps of the params associated with the node are likely the same, hence they can be compressed into a single range vector clock, more specifically, assume that *vc_i (k)* is the time of key *k* for node *i*, given a key range *R*, the ranged vector clock *vc_i (R)=t* means for any key *k belongstoR, vc_i (k)=t*; initially, there is only 1 range vector clock for each node *i*, it covers the entire param key space as its range with 0 as its initial timestamp, each range set may split the range and create at most 3 new vector clocks, click to view(./img/set-vector-clock-to-t-for-range-R-and-node-i.png), let *k* be the total #unique ranges communicated by the algorithm, *m* is #nodes, then there are at most O(mk) vector clocks, here, *k* is typically much smaller than the total #params, this significantly reduces the space required for range vector clocks(*ranges can be also merged to reduce #fragments, however, in practice both *m, k* are small enough to be easily handled);
**msgs:** nodes may send msgs to individual nodes or node groups, a msg consists of a list of (key,value) pairs in the key range *R* and the associated range vector clock: *\[vc(R),(k1,v1),...,(k_p,v_p)\] k belongstoR and j belongsto{1,...,p}*, this is the basic comm format of the param server not only for shared params but also for tasks, for the latter, a (key,value) pair might assume the form (task ID, arguments or return results); msgs may carry a subset of all available keys within range *R*, the missing keys are assigned the same timestamp without changing their values, a msg can be split by the key range, this happens when a worker sends a msg to the whole server group, or when the key assignment of the receiver node has changed, by doing so, we partition the (key,value) lists and split the range vector clock similar to Set Vector Clock to *t* for Range *R* and Node *i*; because ml problems typically require high bdwidth, msg compression is desirable, training data often remains unchanged bt iterations, a worker might send the same key lists again, hence it is desirable for the receiving node to cache the key lists, later the sender only needs to send a hash of the list rather than the list itself, values, in turn, may contain many zero entries, likewise, [a user-defined filter may also zero out a large fraction of the values, click to view](./img/unique-features-ie-keys-filtered-by-the-Karush-Kuhn-Tucker-filter-as-optimization-proceeds.png), hence we need only send nonzero (key,value) pairs, we use the fast Snappy compression lib to compress msgs, effectively removing the 0s, note that key caching and value-compression can be used jointly;
