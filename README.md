# CMU-15719
### Introduction, Use cases, and Elasticity
### A View of Cloud Computing
### [Armbrust2010](https://dl.acm.org/doi/10.1145/1721654.1721672)
### Clearing the clouds away from the true potential and obstacles posed by this computing capability.
### Oracle's CEO Larry Ellison vent his frustration: "The interesting thing about cloud computing is that we've redefined cloud computing to include exerything that we already do.... I don't understand what we would do differently in the light of cloud computing other than change the wording of some of our ads."
### Just as large ISPs use multiple network providers so that failure by a single company will not take them off the air, we believe the only plausible solution to very high availability is multiple cloud computing providers.
cloud computing, the long-held dream of computing as a utility, has the potential to transform a large part of it industry, making sw even more attractive aaS and shaping the way it hw is designed and purchased; 
developers with innovative ideas for new internet services no longer require the large capital outlays in hw to deploy their service or the human expense to operate it, they need not be concerned about overprovisioning for a service whose popularity does not meet their predictions, hus wasting costly resources, or underprovisioning for one that becomes wildly popular, thus missing potential customers and revenue; 
moreover, companies with large batch-oriented tasks can get results as quickly as their programs can scale, since using 1000 servers for 1hr costs no more than using 1 server for 1000hrs, this elasticity of resources, without paying a premium for large scale, is unprecedented in history of it.
### Defining Cloud Computing
cloud computing refers to both the applications delivered aaS over the internet and the hw and systems sw in the datacenters that provide those services, the services themselves have long been referred to as SaaS(*here we use the term SaaS to mean applications delivered over the internet, the broadest definition would encompass any on demand sw, including those that run sw locally but control use via remote sw licensing), some vendors use terms such as IaaS, PaaS to describe their products, but we eschew these because accepted definitions for them still vary widely, the line bt low-level infra and a higher-level platform is not crisp, we believe the two are more alike than different, and we consider them together; similarly, the related term grid computing, from the hp computing community, suggests protocols to offer shared computation and storage over long distances, but those protocols did not lead to a sw environment that grew beyond its community; 
the datacenter hw and sw is what we will call a **cloud**, when a cloud is made available in a pay-as-you-go manner to the general public, we call it a **public cloud**, the service being sold is **utility computing**, we use the term **private cloud** to refer to internal datacenters of a business or other org, not made available to the general public, when they are large enough to benefit from the advantages of cloud computing that we discuss here; thus cloud computing is sum of SaaS and utility computing, but does not include small or medium sized datacenters, even if these rely on virtualization for mgmt, people can be users or providers of SaaS, or users or providers of utility computing; we focus on SaaS providers(cloud users) and cloud providers, which have received less attention than SaaS users, [click for making provider-user relationships clear](./img/making-provider-user-relationships-clear.png), in some cases, the same actor can play multiple roles, for example, a cloud provider might also host its own customer-facing services on cloud infra; 
from a hw provisioning and pricing point of view, 3 aspects are new in cloud computing including: the appearance of infinite computing resources available on demand, quickly enough to follow load surges, thereby eliminating need for cloud computing users to plan far ahead for provisioning; elimination of an up-front commitment by cloud users, thereby allowing companies to start small and increase hw resources only when there is an increase in their needs(*note, however, that upfront commitments can still be used to reduce per-usage charges, for exmaple, AWS also offers long-term rental of servers, which they call reserved instances); the ability to pay for use of computing resources on a short-term basis as needed(such as processors by the hr and storage by the day) and release them as needed, thereby rewarding conservation by letting machines and storage go when they are no longer useful; 
we argue that construction and operation of extremely large scale, commodity-computer datacenters at low cost locations was the key necessary enabler of cloud computing, for they uncovered factors of 5`7 decrease in cost of electricity, network bdwidth, ops, sw, and hw available at these very large economies of scale; these factors, combined with statistical multiplexing to increase utilization compared to traditional datacenters, meant that cloud computing could offer services below the costs of a medium sized datacenter and yet still make a good profit; 
out proposed definition allows us to clearly identify certain installations as examples and non-examples of cloud computing, consider a public-facing internet service hosted on an ISP who can allocate more machines to the service given 4hrs notice, since load surges on the public internet can happen much more quickly than that(we saw its load to double every 12 hrs for nearly 3 days), this is not cloud computing, in contrast, consider an internal enterprise datacenter whose applications are modified only with significant advance notice to admins, in this scenario large load surges on the scale of mins are highly unlikely, so as long as allocation can track expected load increases, this scenario fulfills one of the necessary conditions for operating as a cloud, the enterprise datacenter may still fail to meet other conditions for being a cloud, however, such as the appearance of infinite resources for fine-grained billing; a private datacenter may also not benefit from economies of scale that make public clouds financially attractive; 
omitting private clouds from cloud computing has led to considerable debate in the blogosphere, we believe the confusion and skepticism illustrated by Larry Ellison's quote occurs when advantages of public clouds are also claimed for medium sized datacenters, except for extremely large datacenters of hundreds of thousands of machines, such as those that might be operated by Google, Microsoft, [most datacenters enjoy only a subset of the potential advantages of public clouds, click to view](./img/most-data-centers-enjoy-only-a-subset-of-the-potential-advantages-of-public-clouds.png), hence we believe that including traditional datacenters in the definition of cloud computing will lead to exaggerated claims for smaller so-called private clouds, which is why we exclude them, however, here we describe how so-called private clouds can get more of the benefits of public clouds through surge computing, hybrid cloud computing.
### Classes of Utility Computing
any application needs a model of computation, a model of storage, and a model of comm, the statistical multiplexing necessary to achieve elasticity and the appearance of infinite capacity available on demand requires automatic allocation and mgmt, in practice, this is done with virtualization of some sort, we argue that different utility computing offerings will be distinguished based on the cloud sytem sw's level of abstraction and on the level of mgmt of resources; 
Amazon EC2 is at one end of the spectrum, an EC2 instance looks much like physical hw, and users can control nearly the entire sw stack, from the kernel upward, this low level makes it inherently difficult for Amazon to offer automatic scalability and failover because the semantics associated with replication and other state mgmt issues are highly application-dependent; at the other extreme of the spectrum are application domain-specific platforms such as Google AppEngine, which is targeted exclusively at traditional web applications, enforcing an application structure of clean separation bt a stateless computation tier and a stateful storage tier, AppEngine's impressive automatic scaling and high-availability mechanisms, and the proprietary MegaStore data storage available to AppEngine applications, all rely on these constraints; applications for Microsoft's Azure are written using .NET libs, and compiled to the Common Language Runtime, a lang-independent managed environment, the framework is significantly more flexible than AppEngine's, but still constrains user's choice of storage model and application structure, hence Azure is intermediate bt hw vms like EC2 and application frameworks like AppEngine.
### Cloud Computing Economics
we see 3 particularly compelling use cases that favor utility computing over conventional hosting, including: (i)when demand for a service varies with time, for example, provisioning a datacenter for the peak load it must sustain a few days per month leads to underutilization at other times, instead, cloud computing lets an org pay by hr of computing resources, potentially leading to cost savings even if the hourly rate to rent a machine from a cloud provider is higher than the rate to own one; (ii)when demand is unknown in advance, for example, a web startup will need to support a spike in demand when it becomes popular, followed potentially by a reduction once some visitors turn away; (iii)orgs that perform batch analytics can use the cost associativity of cloud computing to finish computations faster :using 1000 EC2 machines for 1hr costs the same as using 1 machine for 1000hrs; 
although the economic appeal of cloud computing is often described as CapEx2OpEx -converting capital expenses to operating expenses, we believe the phrase pay-as-you-go more directly captures the economic benefit to the buyer, hrs purchased via cloud computing can be distributed non-uniformly in time(for example, use 100 server hrs today and no server hrs tomorrow, and still pay only for 100), in the networking community, this way of selling bdwidth is already known as usage-based pricing(*usage-based pricing is not renting, renting a resources involves paying a negotiated cost to have the resource over some time period, whether or not you use the resource, vs., pay-as-you-go involves metering usage and charging based on actual use, independently of the time period over which the usage occurs); in addition, absence of upfront capital expense allows capital to be rediected to core business investment; 
hence, even if Amazon's pay-as-you-go pricing was more expensive than buying and depreciating a comparable server over the same period, we argue that the cost is outwrighed by the extremely important cloud computing economic benefits of elasticity and transference of risk, esp. risks of overprovisioning(underutilization) and underprovisioning(saturation); 
we start with elasticity, key observation is that cloud computing's ability to add or remove resources at a fine grain(1 server at a time with EC2) and with a lead time of mins rather than weeks allows matching resources to wkload much more closely, real world estimates of average server utilization in datacenters range from 5%`20%, this may sound shockingly low, but it is consistent with the observation that for many services the peak wkload exceeds the average by factors of 2`10, since few users deliberately provision for less than the expected peak, resources are idle at nonpeak times, the more pronounced the variation, the more the waste, [click for example in figure2a](./img/elasticity.png), here, we assume our service has a predictable demand where the peak requires 500 servers at noon but the trough requires only 100 servers at midnight, as long as the average utilization over a whole day is 300 servers, the actual cost per day(area under the curve) is 300times24=7200 server hrs, but since we must provision to the peak of 500 servers, we pay for 500times24=12000 server hrs, a factor of 1.7 more; hence, as long as the pay-as-you-go cost per server hr over 3 years(typically amortization time) is less than 1.7 times cost of buying the server, utility computing is cheaper; in fact, this example underestimates benefits of elasticity, because in addition to simple diurnal patterns, most services also experience seasonal or other periodic demand variation(such as e-commerce in December and photo sharing sites after holidays) and some unexpected demand bursts due to external events(such as news events), since it can take weeks to acquire and rack new equipment, to handle such spikes you must provision for them in advance, we already saw that even if service operators predict the spike sizes correctly, capacity is wasted, and if they overestimate the spike they provision for, it is even worse; 
[they may also underestimate spike, click to view in figure2b](./img/elasticity.png), however, accidentally turning away excess users, while cost of overprovisioning is easily measured, cost of underprovisioning is more difficult to measure yet potentially equally serious :not only do rejected users generate 0 revenue, they may never come back; [for example, Friendster's decline in popularity relative to competitors Facebook, MySpace, is believed to have resulted partly from user dissatisfaction with slow repsonse times(up to 40 secs), click to view in figure2c](./img/elasticity.png) :users will desert an underprovisioned service until peak user load equals datacenter's usable capability, at which point users again receive acceptable service; for another simplified example, assume that users of a hypothetical site fall into 2 classes :active users(those who use the site regularly) and defectors(those who abandon the site due to poor perf), further, suppose that 10% of active users who receive poor service due to underprovisioning are permanently lost opportunities(become defectors), i.e., users who would have remained regular visitors with a better experience, the site is initially provisioned to handle an expected peak of 400000 users(1000 users per server times 400 servers), but unexpected positive press drives 500000 users in the first hr, of the 100000 who are turned away or receive bad service, by our assumption 10000 of them are permanently lost, leaving an active user base of 390000, the next hr sees 250000 new unique users, the first 10000 do fine, but the site is still overcapacity by 240000 users, this results in 24000 additional defections, leaving 376000 permanent users, if this pattern continues, after lg(500000) or 19hrs, #new users will approach 0 and the site will be at capacity in steady state, clearly, the service operator has collected less than 400000 users' worth of steady revenue during those 19hrs, however, again illustrating the underutilization argument, to say nothing of the bad reputation from disgruntled users; 
do such scenarios really occur in practice? when we made our service available via Facebook, it experienced a demand surge that resulted in growing from 50 servers to 3500 servers in 3 days, even if the average utilization of each server was low, no one could have foreseen that resource needs would suddenly double every 12hrs for 3 days, after the peak subsided, traffic fell to a lower level, so in this real world example, scale-up elasticity allowed the steady-state expenditure to more closely match the steady-state wkload.
### Top 10 Obstacles and Opportunities for Cloud Computing
[click to view](./img/top-ten-obstacles-and-opportunities-for-cloud-computing.png), the first 3 affect adoption, the next 5 affect growth, the last 2 are policy and business obstacles, each obstacle is paired with an opportunity to overcome that obstacle, ranging from product dev to research projects.
### 拨开云雾，展现云计算的真正潜力，并消除其带来的障碍。
### Oracle 首席执行官 Larry Ellison 表达了他的不满：“云计算的有趣之处在于，我们重新定义了云计算，将我们已经在做的一切都涵盖其中……除了修改一些广告措辞之外，我不知道在云计算的背景下，我们还能做什么不同的事情。”
云计算，作为计算作为一种实用工具的长期梦想，有可能改变 IT 行业的很大一部分，使软件即服务 (aaS) 更具吸引力，并塑造硬件的设计和购买方式；
拥有创新互联网服务理念的开发人员不再需要投入大量的硬件资金来部署他们的服务，也不再需要投入大量的人力来运营它，他们不必担心为受欢迎程度未达预期的服务过度配置资源，从而浪费昂贵的资源，也不必担心为已经非常流行的服务配置资源不足，从而错失潜在客户和收入；
此外，拥有大量批处理任务的公司能够像其程序扩展一样快速获得结果，因为使用 1000 台服务器 1 小时的成本不超过使用 1 台服务器 1000 小时的成本，这种资源弹性在历史上是前所未有的，无需为大规模支付额外费用。
### 定义云计算
云计算既指通过互联网以“即服务”（aaS）形式交付的应用程序，也指数据中心中提供这些服务的硬件和系统软件。这些服务本身长期以来被称为 SaaS（*此处我们使用 SaaS 一词表示通过互联网交付的应用程序，最广泛的定义涵盖任何按需软件，包括在本地运行软件但通过远程软件许可控制使用的软件）。一些供应商使用 IaaS、PaaS 等术语来描述他们的产品，但我们避免使用这些术语，因为它们的公认定义仍然差异很大，低级基础设施和高级平台之间的界限并不明确，我们认为两者的相同之处多于不同之处，我们认为将它们结合在一起；同样，来自惠普计算社区的相关术语“网格计算”提出了提供远距离共享计算和存储的协议，但这些协议并没有催生出一个超越其社区的软件环境；数据中心硬件和软件就是我们所说的**云**，当云以按需付费的方式向公众开放时，我们称之为**公共云**，所出售的服务是**效用计算**，我们使用术语**私有云**来指代企业或其他组织的内部数据中心，这些数据中心不向公众开放，但当它们规模足够大到能够从我们在此讨论的云计算优势中受益时；因此，云计算是 SaaS 和效用计算的总和，但不包括中小型数据中心，即使这些数据中心依赖虚拟化进行管理，人们可以是 SaaS 的用户或提供商，也可以是效用计算的用户或提供商；我们关注的是 SaaS 提供商（云用户）和云提供商，这两者的关注度低于 SaaS 用户。[点击查看提供商-用户关系](./img/making-provider-user-relationships-clear.png)，在某些情况下，同一个参与者可以扮演多个角色，例如，云提供商可能还会在云端基础设施上托管自己的面向客户的服务；
从硬件配置和定价的角度来看，云计算有三个新特点：无限计算资源的出现，可以按需提供，并且能够快速应对负载激增，从而消除了云计算用户提前规划配置的需要；取消了云用户的前期承诺，从而允许公司从小规模开始，仅在需求增加时才增加硬件资源（*但请注意，前期承诺仍然可以用来降低每次使用的费用，例如，AWS 也提供服务器的长期租赁，他们称之为预留实例）；能够根据需要短期支付计算资源的使用费用（例如按小时支付处理器费用和按天支付存储费用），并根据需要释放这些资源，从而通过在不再使用时释放机器和存储来奖励节约；
我们认为，在低成本地点建设和运营超大规模商用计算机数据中心是云计算的关键必要因素，因为它们揭示了在这些非常大的规模经济下，电力、网络带宽、操作、软件和硬件成本降低 5 至 7 倍的因素；这些因素与统计复用相结合，提高了与传统数据中心相比的利用率，意味着云计算可以提供低于中型数据中心成本的服务，同时仍然获得良好的利润；
我们提出的定义使我们能够清楚地识别某些安装作为示例和非云计算的例子，考虑一个托管在 ISP 上的面向公众的互联网服务，该 ISP 可以在提前 4 小时通知的情况下为该服务分配更多机器，因为公共互联网上的负载激增可能发生得更快（我们看到它的负载每 12 小时翻一番，持续了近 3 天），这不是云计算。相反，考虑一个内部企业数据中心，其应用程序只有在提前通知管理员的情况下才会进行修改，在这种情况下，分钟级的大规模负载激增极不可能发生，因此只要分配可以跟踪预期的负载增长，这种情况就满足了作为云运营的必要条件之一。然而，企业数据中心可能仍然无法满足成为云的其他条件，例如出现无限资源以实现细粒度计费；私有数据中心也可能无法从使公共云具有经济吸引力的规模经济中受益；
将私有云从云计算中剔除在博客圈引发了相当大的争议，我们认为，拉里·埃里森的引言所体现的困惑和怀疑，发生在公有云的优势也适用于中型数据中心的情况下，除了拥有数十万台机器的超大型数据中心，例如谷歌、微软运营的那些。[大多数数据中心只享有公有云的部分潜在优势，点击查看](./img/most-data-centers-enjoy-only-a-subset-of-the-potential-advantages-of-public-clouds.png)，因此，我们认为将传统数据中心纳入云计算的定义会导致对规模较小的所谓私有云的夸大宣传，这就是我们将其排除在外的原因。然而，我们在这里描述了所谓的私有云如何通过激增计算、混合云计算获得更多公有云的优势。
### 效用计算的类别
任何应用程序都需要一个计算模型、一个存储模型和一个comm，实现弹性和按需无限容量所需的统计复用需要自动分配和管理，实际上，这是通过某种虚拟化实现的，我们认为不同的效用计算产品将根据云系统软件的抽象级别和资源管理级别进行区分；
Amazon EC2 处于一个极端，EC2 实例看起来很像物理硬件，用户可以控制几乎整个软件堆栈，从内核向上。这种低级别使得 Amazon 很难提供自动可扩展性和故障转移，因为与复制和其他状态管理问题相关的语义高度依赖于应用程序；另一个极端是应用程序领域特定平台，例如 Google AppEngine，它专门针对传统的 Web 应用程序，强制执行无状态计算层和有状态存储层干净分离的应用程序结构，AppEngine 令人印象深刻的自动扩展和高可用性机制，以及 AppEngine 应用程序可用的专有 MegaStore 数据存储，都依赖于这些约束；微软 Azure 的应用程序使用 .NET 库编写，并编译为公共语言运行时（Common Language Runtime，一个独立于语言的托管环境）。该框架比 AppEngine 的框架灵活得多，但仍然限制了用户对存储模型和应用程序结构的选择，因此 Azure 介于 EC2 等硬件虚拟机和 AppEngine 等应用程序框架之间。
### 云计算经济学
我们发现 3 个特别引人注目的用例表明，效用计算优于传统托管，包括：(i) 当服务需求随时间变化时，例如，为数据中心配置每月必须维持几天的峰值负载，会导致其他时间利用率不足。相反，云计算允许组织按小时支付计算资源费用，即使从云提供商租用机器的小时费率高于拥有机器的费率，也有可能节省成本； (ii) 当需求事先未知时，例如，一家网络初创公司需要支持其在流行时出现的需求激增，而一旦一些访问者流失，需求可能会减少；(iii) 执行批量分析的组织可以利用云计算的成本关联性来更快地完成计算：使用 1000 台 EC2 机器 1 小时的成本与使用 1 台机器 1000 小时的成本相同；
虽然云计算的经济吸引力通常被描述为资本支出转化为运营支出（CapEx2OpEx），即将资本支出转化为运营支出，但我们认为“按需付费”这一说法更直接地体现了买方的经济效益，通过云计算购买的服务器小时数可以不均匀地分配（例如，今天使用 100 小时服务器时间，明天不使用，仍然只需支付 100 小时的费用），在网络社区中，这种销售带宽的方式被称为基于使用量的定价（*基于使用量的定价不是租用，租用资源涉及支付协商好的费用以在一段时间内拥有资源，无论是否使用资源；而按需付费则涉及计量使用量并根据实际使用情况收费，与使用发生的时间段无关）；此外，由于没有前期资本支出，资本可以重新用于核心业务投资；因此，即使亚马逊的按需付费定价比在同一时期购买和折旧一台同类服务器的成本更高，我们认为，其成本也被云计算极其重要的经济效益所抵消，即弹性和风险转移，尤其是过度配置（利用不足）和配置不足（饱和）的风险；
我们从弹性开始，关键的观察是云计算能够以细粒度添加或删除资源（使用 EC2 一次添加或删除一台服务器），并且只需几分钟而不是几周的准备时间，就可以更紧密地匹配资源以进行负载均衡，现实世界中数据中心的平均服务器利用率估计在 5% 到 20% 之间，这听起来可能低得惊人，但这与以下观察结果一致：对于许多服务而言，峰值负载是平均值的 2 到 10 倍，因为很少有用户故意提供低于预期峰值的资源，资源在非峰值时段处于闲置状态，变化越明显，浪费就越多，[单击图 2a 中的示例](./img/elasticity.png)，在这里，我们假设我们的服务具有可预测的需求，峰值在中午需要 500 台服务器，但低谷在午夜只需要 100 台服务器，只要全天的平均利用率为 300 台服务器，则每天的实际成本（曲线下面积）为300x24=7200 服务器小时，但由于我们必须为 500 台服务器的峰值进行配置，因此我们需要支付 500x24=12000 服务器小时的费用，比峰值高出 1.7 倍；因此，只要 3 年内（通常是摊销期）每服务器小时的即用即付成本小于购买服务器成本的 1.7 倍，效用计算就会更便宜；事实上，这个例子低估了弹性的优势，因为除了简单的昼夜模式外，大多数服务还会经历季节性或其他周期性需求变化（例如 12 月的电子商务和节假日后的照片分享网站）以及一些由于外部事件（例如新闻事件）导致的意外需求爆发，因为购买和上架新设备可能需要数周时间，为了应对这样的峰值，您必须提前做好准备，我们已经看到，即使服务运营商正确预测了峰值规模，容量也会被浪费，如果他们高估了他们所准备的峰值，情况会更糟；
[他们也可能低估峰值，单击查看图 2b](./img/elasticity.png)，然而，意外地拒绝了多余的用户，虽然过度配置的成本很容易衡量，但配置不足的成本更难衡量，但可能同样严重：被拒绝的用户不仅不会产生任何收入，而且可能永远不会回来；[例如，Friendster 相对于竞争对手 Facebook、MySpace 的受欢迎程度下降被认为部分是由于用户对缓慢的响应时间（长达 40 秒）不满意，单击查看图 2c](./img/elasticity.png)：用户将放弃配置不足的服务，直到峰值用户负载等于数据中心的可用容量，此时用户再次获得可接受的服务；另一个简化的例子，假设一个网站的用户分为两类：活跃用户（经常使用该网站的用户）和流失用户（由于性能不佳而放弃该网站的用户）。进一步假设，由于配置不足而获得较差服务的用户中有 10% 永远失去了机会（成为流失用户），即那些本来可以成为常客并获得更好体验的用户。该网站最初配置为处理预期峰值 400,000 个用户（每台服务器 1,000 个用户乘以 400 台服务器），但意外的正面报道在第一个小时内吸引了 500,000 名用户，在 100,000 名被拒之门外或获得不良服务的用户中，根据我们的假设，其中 10,000 人永久流失，剩下 390,000 个活跃用户。下一个小时将看到 250,000 个新的独立用户，前 10,000 名用户表现良好，但该网站仍然超负荷 240,000 个用户。这导致24000名用户流失，留下376000名永久用户。如果这种模式持续下去，经过lg(500000)或19小时后，新增用户数量将接近0，站点将达到稳定状态。显然，服务运营商在这19个小时内获得的稳定收入还不到400000名用户的收入，但这再次证明了利用率不足的论点，更不用说来自不满用户的坏名声了。
这种情况在实践中真的会发生吗？当我们通过Facebook提供服务时，它经历了需求激增，导致服务器数量在3天内从50台增加到3500台。即使每台服务器的平均利用率很低，也没有人能预见到资源需求会在3天内每12小时突然翻一番。峰值消退后，流量会下降到更低的水平。因此，在这个现实世界的例子中，扩展弹性使稳定状态的支出能够更紧密地匹配稳定状态的工作负荷。
### 云计算的 10 大障碍和机遇
[点击查看](./img/top-ten-obstacles-and-opportunities-for-cloud-computing.png)，前 3 个影响采用，接下来的 5 个影响增长，后 2 个是政策和业务障碍，每个障碍都与克服该障碍的机会配对，从产品开发到研究项目。
### The NIST Definition of Cloud Computing
### [NISTdef2011](https://www.nist.gov/publications/nist-definition-cloud-computing)
### NIST -National Institute of Standards and Technology Definition of Cloud Computing
cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources(such as networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal mgmt effort or service provider interaction; this cloud model is composed of 5 essential characteristics, 3 service models, and 4 deployment models; 
essential characteristics: #1 on-demand self-service -a consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with each service provider, #2 broad network access -capabilities are available over the network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms(such as mobile phones, tablets, laptops, and workstations), #3 resource pooling -provider's computing resources are pooled to serve multiple consumers using a multitenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand, there is a sense of location independence in that the customer generally has no control or knowledge over the exact location of the provided resources but may be able to specify location at a higher level of abstraction(such as country, state, or datacenter), examples of resources include storage, processing, mem, and network bdwidth, #4 rapid elasticity -capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward commensurate with demand, to the consumer, the capabilities available for provisioning often appear to be unlimited and can be appropriated in any quantity at any time, #5 measured service -cloud systems automatically control and optimize resource use by leveraging a metering capability(*typically this is done on a pay-as-you-go or charge-per-use basis) at some level of abstraction appropriate to type of service(such as storage, processing, bdwidth, and active user accounts), resources usage can be monitored, controlled, and reported, providing transparency for both provider and consumer of the utilized service; 
service models: #1 SaaS -the capability provided to the consumer it to use provider's application running on a cloud infra(*a cloud infra is collection of hw and sw that enables the 5 essential characteristics of cloud computing, the cloud infra can be viewed as containing both a physical layer and an abstraction layer, the physical layer consists of the hw resources that are necessary to support the cloud services being provided and typically includes server, storage, and network components, the abstraction layer consists of sw deployed across the physical layer which manifests the essential cloud chracteristics, conceptually the abstraction layer sits above the physical layer), the applications are accessible from various client services through either a thin client interface, such as a web browser(such as web-based email), or a program interface, consumer does not manage or control th eunderlying cloud infra including network, servers, os, storage, or even individual application capabilities, with the possible exception of limited user-specific application config settings; #2 PaaS -the capability provided to the consumer is to deploy onto the cloud infra consumer-created or acquired applications created using programming langs, libs, services, and tools supported by the provider(*this capability does not necessarily preclude use of compatible programming langs, libs, services, and tools from other sources), the consumer does not manage or control the underlying cloud infra including network, servers, os, or storage, but has control over the deployed applications and possibly config settings for the application-hosting environment; #3 IaaS -the capability provided to consumer is to provision processing, storage, networks, and other fundamental computing resources where consumer is able to deploy and run arbitrary sw, which can include os and applications, consumer does not manage or control the underlying cloud infra but has control over os, storage, and deployed applications, and possibly limited control of select networking components(such as host firewalls); 
deployment models: #1 private cloud -the cloud infra is provisioned for exclusive use by a single org comprising multiple consumers(such as business units), it may be owned, managed, and operated by the org, a third party, or some combinition of them, and it may exist on or off premises; #2 community cloud -the cloud infra is provisioned for exclusive use by a specific community of consumers from orgs that have shared concerns(such as mission, security requirements, policy, and compliance considerations), it may be owned, managed, and operated by one or more of the orgs in the community, a third party, or some combinition of them, and it may exist on or off premises; #3 public cloud -the cloud infra is provisioned for open use by the general public, it may be owned, managed, and operated by a business, academic, or government org, or some combinition of them, it exists on the premises of the cloud provider; #4 hybrid cloud -the cloud infra is a composition of two or more distinct cloud infras that remain unique entities, but are bound together by standardized or proprietary technology that enables data and application portability(such as cloud bursting for load balancing bt clouds).
### Dynamically Scaling Applications in the Cloud
### [Vaquero11](https://dl.acm.org/doi/pdf/10.1145/1925861.1925869)
### Scalability is said to be one of the major advantages brought by the cloud paradigm and, more specifically, the one that makes it different to an advanced outsourcing solution, however, there are some important pending issues before making the dreamed automated scaling for applications come true.
cloud computing is commonly associated to offering of new mechanisms for infra porvisioning including: the illusion of a virtually infinite computing infra, the employment of advanced billing mechanisms allowing a pay-per-use model on shared multitenant resources, the simplified programming mechanisms(platform); among these features, those introduced by adding scalability and automated on-demand self-service are responsible for making any particular service something more than just an outsourced service with a prettier marketing face; 
as a result of its relevance, the wealth of systems dealing with cloud application scalability is slowing gaining weight in the available literature; automation is typically achieved by using a set of service provider-defined rules that govern how the service scales up or down to adapt to a variable load, these rules are themselves composed of a condition, which when met triggers some action on infra or platform, the degree of automation, abstraction for the user(service provider) and customization of rules governing the service vary, some systems offer users chance of building rather simple conditions based on fixed infra or platform matrics(such as cpu, mem, etc.), while others employ server-level metrics(such as cost to benefit ration) and allow for more complex conditions(such as arithmetic and logic combinations of simple rules) to be included in the rules, regarding the subsequent actions launched when conditions are met, available efforts focus on service horizontal scaling(i.e. adding new server replicas and load balancers to distribute load among all available replicas) or vertical scaling(i.e. on-th-fly changing of assigned resources to an already running instance, such as letting more physical cpu to a running vm), unfortunately, the most common os do not support on-the-fly(without rebooting) changes on the available cpu or mem to support this vertical scaling; 
beyond mere server scalability, some other elements need to be taken into account that affect the overall application scaling potential, for example, LBs -load balancers need to support aggregation of new servers(typically, but not necessarily, in the form of new vms) in order to distribute load among several servers, Amazon already provides strategies for load balancing your replicated vms via its elastic load balancing capabilities, hence LBs and the algorithms that distribute load to different servers are essential elements in achieving application scalability in the cloud; 
having several servers and the mechanisms to distribute load among them is a definitive step towards scaling a cloud application, however, there is another element of the datacenter infra to be considered towards complete application scalability, network scalability is an often neglected element that should also be considered; in a consolidated datacenter scenario, several vms share the same network, potentially producing a huge increase in the equired bdwidth(potentially collapsing the network), hence it is necessary to extend infra clouds to other kinds of underlying resources beyond servers, LBs, and storage; cloud applications should be able to request not only virtual servers at multiple points in the network, but also bdwidth-provisioned network pipes and other network resources to interconnect them -NaaS; 
clouds that offer simple virtual hw infra such as vms and networks are usually denoted IaaS, vs., PaaS clouds provide sets of online libs and service for supporting application design, implementation, and maintenance, despite being somewhat less flexible than IaaS clouds, PaaS clouds are becoming important elements for building applications in a faster manner and many important it players scuh as Google, Microsoft, have developed new PaaS clouds systems such as Google AppEngine, Microsoft Azure, we discuss scalability in PaaS clouds at 2 different levels -container level and db level; 
[click for an overview of mechanisms handy to accomplish the goal of the whole application scalability](./img/overview-of-mechanisms-handy-to-accomplish-the-goal-of-the-whole-application-scalability.png).
### Server Scalability
most of the available IaaS clouds deal with single vm mgmt primitives(such as elements for adding/removing vms), lacking mechanisms for treating applications as a whole single entity and dealing with relations among different application components, for example, relationships bt vms are often not considered, ordered deployment of vms contaning sw for different tiers of an appliation is not automated(such as the db's ip is only known at deployment time hence the db needs to be deployed first in order to get its ip and configure the web server connecting to it), etc., application providers typically want to deal with their application only, being released from burden of dealing with (virtual) infra terms; 
**towards increased abstraction and automation :the elasticity controller:** such a fine-grained mgmt(vm-based) may come on handy for few services or domestic users, but it may become intractable with a big number of deployed applications composed of several vms each, the problem gets worse if application providers aim at having their application automatically scaled according to load, they would need to monitor every vm for every application in the cloud and make decisions on whether or not every vm should be scaled, its LB re-configured, its network resources resizes, etc., 2 different approaches are possible: increasing the abstraction level of provided APIs and/or advancing towards a higher automation degree; 
automated scaling features are being included by some vendors, but the rules and policies they allow to express still deal with individual vms only, one cannot easily relate the scaling of vms at tier-1 with its load balancers or the scaling of vms at tier-3; 
on the other hand, more abstract frameworks are proposed(they allow users to deal with applications as a whole rather than per individual vm) that also convey automation, unavoidably, any scalability mgmt system(or [elasticity controller, click to view](./img/elasticity-controller.png)) is bound to the underlying cloud API(the problem of discrete actuators), hence one essential task for any application-level elasticity controller is mapping user scaling policies from the appropriate level of abstraction for the user to actual mechanisms provided by IaaS clouds(depending on specific underlying API, in practice, most of the available cloud APIs expose vm-level mgmt capabilities, so that controller-actuator pairs are limited to adding/removing vms); the implementation of elasticity controller can be done in several different ways with regard to the provided abstraction level: (i)a per-tier controller, so that there is a need for coordination and sync among multiple controller-actuator pairs, treating each tier as an independent actuator with its own control policy can cause shifting of the perf bottleneck bt tiers, a tier can only release resources when an interlock is not being held by the other tiers, (ii)a single controller for the whole application(for all tiers), which let users specify how an application should scale in a global manner, for example, application provider could specify(based on its accurate knowledge of application) to scale the application logic tier whenever #incoming reqs at the web tier is beyond a given threshold; 
**expressing how and when to scale :feeding controller with rules and policies:** all the works above rely on traditional control theory in which several sensors feed a decision making module(elasticity controller) with data to operate on an actuator(cloud API), [click to view](./img/elasticity-controller.png), similarly, all the systems above answer the question on how to automate scalability(i.e. how to implement elasticity controller) in a similar manner either for vm-level or for application-level scalability mgmt systems; 
user-defined rules are the chosen mechanism(as opposed to preconfigured equations sets) to express the policies controlling scalability as shown below: *RULE: if CONDITION(s) then ACTION(s) CONDITION: (1..*) (metric.value MODIFIER) ACTION: (*) IaaS cloud-enabled actions(such as deploy new vm)*, a rule is composed of a series of conditions that when met trigger some action over the underlying cloud, every condition itself is composed of a series of metrics or events that may be present in the system, such as money available, authorization received, etc.(either provided by infra or obtained by application provider itself), which are used together with a modifier(either a comparison against a threshold, the presence of such events) to trigger a set of cloud-provided actions; 
if we focus on application-level scalability(rather than dealing with per vm scaling), a mathematical foumulation in which user just configures threshold for replicating storage servers so as to increase the cloud storage capability is proposed(*called proportional thresholding mechanism, which considers the fact that going from 1 to 2 machines can increase capacity by 100% but going from 100 to 101 machines increases capacity by no more than 1%), a leave full control for users to express their rules and use a rule engine as a controller is also proposed, the OVF -open virtual format is extended to define the application, its components, its contextualization needs, and the rules for scaling, this ways, service providers can generate a description of their application components, the way their application behaves with regard to scalability and the relevant metrics that will trigger action expressed in such rules; 
[as shown in fig1](./img/overview-of-mechanisms-handy-to-accomplish-the-goal-of-the-whole-application-scalability.png), in addition to dynamically adding more vm replicas, application behavior could also include many other aspects determining application scalability: adding load balancers to distribute load among vm replicas is also an important point, in an IaaS cloud, #vms balanced by a single LB can hugely increase, hence overloading the balancer(*although we recognize the importance of load balancing algorithms, the wealth of available literature on this matter places them well beyond of the scope of this work); 
LB scalability requires total time taken to forward each req to the corrsponding server to be negligible for small loads and should grow no faster than O(p)(here p being small loads of balanced vms) when the load is big and #balanced vms is large, however, although mechanisms to scale the load balancing tier would benefit any cloud system, they are missing in most current cloud implementations; 
Amazon already provides its elastic load balancer service aimed at delivering users with a single LB to distribute load among vms, however, Amazon does not provide mechanisms to scale LBs themselves; virtualization of load balancing mechanisms offers chance to define scalability rules by using previously presented systems, recently, a rule of thumb procedure to configure presenration-tier(web server) scalability is proposed :for cpu-intensive web applications, it is better to use a LB to split computation among many instances, for network intensive applications, it may be better to use a cpu-powerful standalone instance to maximize network throughput, yet, for even more network intensive applications, it may be necessary to use dns load balancing to get around a single instance bdwidth limitation, the question emerges whether dns-based load balancing can be scalable enough or not, practical experiences with large well-known services such as Google's search engine and recent experimental works on cloud settings seem to point in that direction, also, for many enterprise applications, hw LB is the most common approach.
### Scaling the Network
properly replicated/sized vms led us to think about LBs as a possible bottleneck, assuming this problem is also resolved takes us to think about the link that keeps application elements stuck together, even across different cloud infra services :the network, which is often overlooked in cloud computing; 
networking over virtualized resources is typically done in 2 different manners: ethernet virtualization and overlay networks and tcp/ip virtualization, these techniques are respectively focused in usage of VLAN -virtual local area network tags (L2) to separate traffic or public key infras to build L2/L3 overlays; 
separating users' traffic is not enough for reaching complete application scalability :the need to scale the very network arises in consolidated datacenters hosting several vms per physical machine, this scalability is often achieved by over-provisioning resources to suit this increased demand; this approach is expensive and induces network instability while infra is being updated, also, it is static and does not take into account that not all applications consume all required bdwidth during all time; improved mechanisms taking into account actual network usage are required; on the other hand, one could periodically measure actual network usage per application and let applications momentarily use other applications' allocated bdwidth; on the other hand, applications could request more bdwidth on demand over the same links, instantiate bdwidth-provisioned network resources together with the vms composing the service across several cloud providers is proposed, similar to the OVF extensions mentioned above, these authors employ NDL -network description lang -based ontologies for expressing required network characteristics; these abstract requirements are mapped to the concrete underlying network peculiarities(such as dynamically provisioned circuits vs. ip overlays), a complete architecture to perform this mapping has also been proposed, unfortunately, there is no known production-ready system that fully accomplishes need for dynamically managing the network in synchrony with vms provisioning; 
these techniques to increase utilization of the network by virtually slicing it have been dubbled as NaaS, this *a la cloud* network provision paradigm can be supported by flow control, distirbuted rate limiting, and network slicing techniques, by applying this mechanism the actual bdwidth can be dynamically allocated to applications on demand, which would benefit from a dynamic resource allocation scheme in which all users pay for the actual bdwidth consumption; to optimize network usage statistical multiplexing is used to compute the final bdwidth allocated to each application, statistical multiplexing helps to allocate more bdwidth to some applications while some others are not using it(most system admins usually provision on a worst-case scenario and never use all requested resources), this way, cloud provider can make a more rational use of its network resources, while still meeting applications' needs.
### Scaling the Platform
IaaS clouds are handy for application providers to control resources used by their systems, however, IaaS clouds demand application developers or system admins to install and configure all sw stack the application components need, in contrast, PaaS clouds offer a ready to use exec environment, along with convenient services, for applications, hence when using PaaS clouds developers can focus on programming their components rather than on setting up environment those components require, but as PaaS clouds can be subject to an extensive usage(many concurrent users calling to hosted applications), PaaS providers must be able to scale exec environment accordingly, in this section, we explore how scalability impacts on the 2 core layers of PaaS platforms: the container and the DBMS, as they are the backbone of any PaaS platform :combination of container+db is the chosen stack to implement many networked(such as internet) applications, which are the ones PaaS platforms are oriented to; here, container is the sw platform where users' components will be deployed and run, different PaaS clouds can be based on different platforms, for example, GAE and its open source counterpart AppEngine provide containers for servlets(part of J2EE specification) and python scripts, while Azure, Aneka offer an environment for .NET applications, each platform type can define different lifecycles, services, and APIs for the components it hosts; db provides data persistence support, the db storage service must address demand for data transactions support combined with big availability and scalability requirements, as it is explained later in this section, this can be addressed in different manners; [click for an overview of possible architecture of a PaaS platform](./img/PaaS.png), where both container and db layer achieve scalability through replication of container and dbms(horizontal scaling), this is the scenario this work focuses on, as it is the only one the authors deem feasible in clouds with certain scalability requirements, vs., applying vertical scaling by using more powerful hw would soon fail, as many clouds will typically face loads that one single machine cannot handle whatever its capacity; other services can be offered by a PaaS platform apart from container and db, which also will need to be scaled to adapt to demand, for example, Azure offers services for inter-component communication(bus) or access control, unfortunately, in this work it would only be possible to study scalability issues of all those services in a too superficial manner, instead, a most thorough analysis of the most important services, the container and the db, has been preferred; 
**container-level scalability:** at container level, a better scalability can be achieved by enabling multitenant containers(having the ability to run components belonging to different users), this imposes strong isolation requirements which maybe not all platforms can achieve by default, for example, the standard java platform is known to carry important security limitations that hinder implementation of safe multitenant environments, a more straightforward option is to run each user's components on non-shared containers, which is the approach taken by GAE; 
in both cases scaling is implemented by instantiating/releasig containers where several replicas of the developer components can be run(i.e. it is a form of horizontal scaling), this should automatically be done by the platform, so developers are not forced to constantly monitor their services' state; either automatic scaling IaaS systems(such as those mentioned in section service availability) can be used, or the platform itself can scale up and down the resources, the latter approach is the one used by both AppEngine, Aneka, AppEngine can run in any Xen based cloud, private(built for example with Eucalyptus) or public(EC2), however, it is not clear from if AppEngine supports transparent cloud federation so that a private PaaS cloud could deploy its containers in vms running in third party owned IaaS clouds to face sudden load peaks, in any case AppScale could apply Eucalyptus ability to be integrated with other clouds, on the other hand, Aneka supports cloud federation natively, so federated Aneka clouds can borrow resources among them, or a given Aneka cloud can use containers hosted in vms running in different IaaS clouds; 
automatic scaling of containers has several implications for developers regarding component design, if the PaaS platform can stop container replicas at any moment(such as due to low load), components could be designed to be as stateless as possible(as GAE recommends for the applications it hosts); in order to ease application dev, support for stateful components should be offered by the platform, in this case with stateful components, LB and container replica mgmt modules must be aware of state, this way, LBs know which container replicas hold session data to forward reqs accordingly, and container instances are not removed as long as they hold some session; 
if more than one container instance holds data from the same session(for better scalability and failure resilience), then some mechanism is necessary that allows to keep the different data replicas updated; transparent session data(such as shopping cart) replication, usually denoted soft state replication can be offered through systems such as Tempest toll or SSM; it is also possible to use distributed cache systems such as memcached for explicit data sharing among component replicas; each solution will have a different impact on component dev, roughly speaking, distributed caches work at application level i.e. they are explicitly accessed by hosted components code to store/retrieve info shared among component replicas, while soft state/session replication systems work in a transparent manner for application developer; 
**db scalability:** PaaS systems must expect very high reqs rates as they can host many applications that demand intense data traffice, 3 mechanisms can be used(and combined) to handle this: distributed caching, nosql dbs, and db clustering, here, (i)caching systems, such as memcached, are used to store intermediate data to speed up frequent data queries, a req for some data item will first check cache to see if the data is present, and will query db only if the data is not in cache(or it has expired), distributed caching systems provide the same cache across several nodes, which is useful in clustered applications, for exmaple, GAE offers [memcache](http://code.google.com/appengine/docs/java/memcache/) as a service for application developers; (ii)nosql refers to a wide family of storage solutions for structured data that are different from traditional relational fully sql-compliant dbs, nosql systems offer high scalability and availability, which seems a good fit in cloud environments with potentially many applications hosted under high demand, on the other hand, the replica mgmt mechanisms they use, provide less guarantees than traditional systems, usually, updates on data copies are not immediately done after each write op, they will be eventually done at some point in the future, this causes 3 systems to be unable to implement support for transparent and fully acid-complaint transactions, hence imposing some limitations on how transactions can be used by developers, besides, the fact that they only support(the equivalent to) a subset of sql can be a hurdle for some applications, such as BigTable used by GAE to provide its object oriented data storage service, HBase an open source implementation of BigTable used by AppEngine; (iii)finally, if fully relational and sql-complaint dbs are to be provided(as in the case of Microsoft's Azure), clusters can be built to provide better scalability, availability, and fault tolerance to typical dbms systems, unfortunately, 3 clusters must be built so several or all nodes contain a replica of each data item, which is known to compromise perf even for moderate loads when transactions are supported, present db replication systems from every major relational dbms have several limitations, and further research is needed to achieve desired perf, the major problem comes from the fact that transactions require protecting data involved while transaction lasts, often making that data unavailable to other transactions, the more transactions running at the same time, the more conflicts will be raised with the corrsponding impact on the application perf; 
yet, some db replication solutions exist that offer some degree of scalability, these can be part of the dbms itself(in-core) or be implemented as a mw layer bt db and application, most of the mw based solutions use a proxy driver used by client applications to access db, this proxy redirects reqs to the replication mw, which forwards them to db, the mw layer handles reqs and transforms then in db ops to ensure that all data copies are updated, also, it takes care of load balancing tasks, exmaples of such solution are C-JDBC, Middle-R, DBFarm; 
it can be concluded from this section that replication of dbs/components is the most important issue to consider when scaling a PaaS platform, [click for main replication ideas presented](./img/replication.png), at container level, the same component can be run on different container instances to achieve better scalability and failure tolerance, but then the platform should make available some mechanism for consistent data replication among components, at db level, copies of application data can be hosted in different dbms instances again for better scalability and failure tolerance, unfortunately, keeping consistency can lead to transaction conflicts; 
[click for summary of most relevant works related to holistic application scaling in cloud environments at the 3 different levels: server, network, and platform level, the most relevant features are highlighted and appropriate references are given](./img/summary-of-most-relevant-works-related-to-holistic-application-scaling-in-cloud-environments-at-the-three-different-levels.png).
****
### Building a Carnegie Mellon Cloud and Openstack
### Virtual Infrastructure Management in Private and Hybrid Clouds
### [Sotomayor2009](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5233608)
### One of the many definitions of cloud is that of an IaaS system, which it infra is deployed in a provider's datacenter as vms, with IaaS clouds' growing popularity, tools and technologies are emerging that can transform an org's existing infra into a private or hybrid cloud; OpenNebula is an open source virtual infra manager that deploys virtualized servers on both a local pool of resources and external IaaS clouds; Haizea is a resource lease manager that can act as a scheduling backend for OpenNebula, providing features not found in other cloud sw or virtualization-based datacenter mgmt sw.
cloud computing is to use a cloud-inspired pun, a nebulously defined term, however, it was arguably first popularized in 2006 by Amazon's [EC2](www.amazon.com/ec2/) -elastic compute cloud, which started offering vms for $0.10 an hr using both a simple web interface and a programmer-friendly API, although not the first to propose a utility computing model, EC2 contributed to popularizing IaaS paradigm, which became closely tied to the notion of cloud computing; an IaaS cloud enables on-demand provisioning of computational resources in the form of vms deployed in a cloud provider's datacenter(such as Amazon's), minimizing or even eliminating associated capital costs for cloud consumers and letting those consumers add or remove capacity from their it infra to meet peak or fluctuating service demands while paying only for the actual capacity load; 
overtime, an ecosystem of providers, users, and technologies has coalesced around this IaaS cloud model, more IaaS cloud providers such as GoGrid, FlexiScale, ElasticHosts, etc., have emerged, and a strong number of companies base their it strategy on cloud-based resources, spending little or no capital to manage their own it infra([click for more examples](https://aws.amazon.com/solutions/case-studies/)); some providers such as Elastra, RightScale, focus on deploying and managing services on top of IaaS clouds, including web and db servers that benefit from such clouds' elastic capacity, and let their clients provision services directly instead of having no provision and set up infra themselves; other providers offer products that facilitate working with IaaS clouds, such as [rPath's rBuilder](www.rpath.org), which enables users to dynamically create sw environments to run on a cloud; 
although this ecosystem has evolved around public clouds -commercial cloud providers that offer a publicly accessible remote interface for creating and managing vm instances within their proprietary infra -internet is growing in open source cloud computing tools that let orgs build their own IaaS clouds using their internal infras, these private cloud deployments' primary aim is not to sell capacity over internet through publicly accessible interfaces but to give local users a flexible and agile private infra to run service wkloads within their administrative domains, private clouds can also support a hybrid cloud model by supplementing local infra with computing capacity from an external public cloud, private and hybrid clouds are not exclusive with being public clouds, a priavte or hybrid cloud can allow remote access to its resources over internet using remote interfaces, such as web services that Amazon EC2 uses, here, we look at 2 open source projects that facilitate mgmt of such private or hybrid cloud models.
### Virtual Infra Mgmt
to provide users with the same features found in commercial public clouds, private or hybrid cloud sw must: (i)provide a uniform and homogeneous view of virtualized resources, regardless of underlying virtualization platform(such as Xen, KVM -kernel-based vm, VMware, etc.), (ii)manage a vm's full life cycle, including setting up networks dynamically for groups of vms and managing their storage requirements, such as vm disk image deployment or on-the-fly sw environment creation, (iii)support configurable resource allocation policies to meet org's specific goals(high availability, server consolidation to minimize power usage, etc.), (iv)adapt to org's changing resource needs, including peaks in which local resource are insufficient, and changing resources including addition or failure of physical resources; 
hence a key component in private or hybrid clouds will be VI -virtual infra mgmt, the dynamic orchestration of vms that meets the requirements we have just outlined, here, we discuss vi mgmt's relevance not just for creating private or hybrid clouds but also within the emerging cloud ecosystem; our 2 open source projects, [OpenNebula](http://www.opennebula.org/), [Haizea](http://haizea.cs.uchicago.edu/), are complementary and can be used to manag vis in private or hybrid clouds, here, OpenNebula is a vi manager that orgs can use to deploy and manage vms, either individually or in groups that must be coscheduled on local resources or external public clouds, it automates vm setup(preparing disk images, setting up networking, etc.) regardless of underlying virtualization layer(Xen, KVM, VMware) or external cloud(EC2, ElasticHosts), Haizea is a resource lease manager that can act as a scheduling backend for OpenNebula, providing leasing capabilities not found in other cloud systems, such as ARs -advance researvations, and resource preemption, which are particularly relevant for private clouds.
### The Cloud Ecosystem
vi mgmt tools for datacenters have been around since before cloud computing became industry's new buzzword, several of these, such as the [Platform VM Orchestrator](https://www.platform.com/Products/platform-vm-orchestrator), [VMware vSphere](http://www.vmware.com/products/vsphere/), [Ovirt](http://ovirt.org/), meet many of the vi mgmt requirements we outlined earlier, providing features such as dynamic placement and vm mgmt on a pool of physical resources, automatic load balancing, server consolidation, and dynamic infra resizing and partitioning; although creating what we now call a private cloud was already possible with existing tools, these tools lack other features that are relevant for building IaaS clouds, such as public cloud-like interfaces, mechanisms for adding such interfaces easily, and the ability to deploy vms on external clouds; 
on the other hand, projects such as [Globus Nimbus](http://workspace.globus.org), [Eucalyptus](www.eucalyptus.com), which we term cloud toolkits, can help transform existing infra into an IaaS cloud with cloud-like interfaces, here, Eucalyptus is compatible with Amazon EC2 interface and is designed to support additional client-side interfaces, Globus Nimbus exposes EC2 and WSRF -web services resource framework interfaces and offers self-configuring virtual cluster support, however, although these tools are fully functional with respect to providing cloud-like interfaces and higher-level functionality for security, contextualization, and vm disk image mgmt, their vi mgmt capabilities are limited and lack features of solutions that specialize in vi mgmt; hence [an ecosystem of cloud tools is starting to form, click to view](./img/ecosystem-of-cloud-tools.png), in which cloud toolkits attempt to span both cloud mgmt and vi mgmt but, by focusing on the former, do not deliver the same functionality as sw written specifically for vi mgmt; although integrating cloud mgmt solutions with existing vi managers would seem like obvious solution, this is complicated by lack of open and standard interfaces bt the 2 layers, and lack of certain key features in existing vi managers, hence our aim is to produce a vi mgmt solution with a flexible and open architecture that orgs can employ to build private or hybrid clouds; with this goal in mind, we started developing OpenNebula and continue to enhance it as part of [EU's Reservoir porject](http://www.reservoir-fp7.eu/), which aims to develop open source technologies to enable deployment and mgmt of complex it services across different administrative domains; OpenNebula provides similar functionality to that found in existing vi managers but also aims to over those solutions' shortcomings, namely: (i)the inability to scale to external clouds, (ii)monolithic and closed architectures that are hard to extend or interface with other sw, not allowing seamless integration with existing storage and network mgmt solutions deployed in datacenters, (iii)a limited choice of preconfigured placement policies(first fit, round robin, etc.), (iv)a lack of support for scheduling, deploying, and configuring groups of vms(such as a group of vms representing a cluster where all the nodes either deploy entirely or do not deploy at all and where some vms' config depends on others' such as the head-worker replationship in compute clusters), [click for a more detailed comparison bt OpenNebula and several well-known vi managers, including cloud toolkits that perform vi mgmt](./img/detailed-comparison-between-OpenNebula-and-several-well-known-virtual-infrastructure-managers.png); 
a key feature of OpenNebula's architecture, which we describe more in the next section, is its highly modular design, which facilitates integration with any virtualization platform and third-party component in the cloud ecosystem, such as cloud toolkits, virtual image managers, service managers, and vm schedulers, for example, it specifies all actions pretaining to setting up a vm disk image(transferring the image, installing sw on it, etc.) in terms of well-defined hooks, although OpenNebula includes a default transfer manager that uses these hooks, it can also leverage contextualizers just by writing code that interfaces bt hooks and thir-party sw; 
Haizea which we developed independently from OpenNebula, was the first to leverage such an architecture in a way that was beneficial to both projects, Haizea originally cloud simulate vm scheduling only for research purposes, but we modified it to act as a drop-in replacement for OpenNebula's default scheduler, with few changes required in Haizea code and none in the OpenNebula code, by working together, OpenNebula could offer resource leases as a fundamental provisioning abstraction, and Haizea could operate with real hw through OpenNebula; 
in fact, integraing OpenNebula and Haizea provides the only vi mgmt solution offering advance reservation of capacity, [as tabel1 shows](./img/detailed-comparison-between-OpenNebula-and-several-well-known-virtual-infrastructure-managers.png), other vi managers use immediate provisioning, or best-effort provisioning, which we discuss in more detail later, however, private clouds -specifically those with limited resources in which not all reqs are satisfiable immediately owing to lack of resources -stand to benefit from more sophisticated vm placement strategies supporting queues, priorities, and ars; additionally, service provisioning clouds, such as the one being developed in Reservoir project, have requirements that are insupportable with only an immediate provisioning model -for example, they need capacity reservations at specific times to meet SLAs -service-level agreements, or peak capacity requirements.
### The OpenNebula Architecture
[click to view](.//img/opennebula.png), encompasses several components specialized in different aspects of vi mgmt; to control a vm's life cycle, OpenNebula core orchestrates 3 different mgmt areas including: image and storage technologies(i.e. virtual appliance tools or distributed file systems) for preparing disk images for vms, network fabric(such as DHCP -dynamic host config protocol servers, firewalls, or switches) for providing vms with a virtual network environment, and underlying hypervisors for creating and controlling vms; the core performs specific storage, network, or virtualization ops through pluggable drivers, hence OpenNebula is not tied to any specific environment, providing a uniform mgmt layer regardless of underlying infra; 
besides managing individual vm's life cycle, we also designed the core to support services deployment, such services typically include a set of interrelated components(such as a web server and db backend) requiring several vms, hence we can treat a group of related vms as a first-class entity in OpenNebula; besides managing vms as a unit, the core also handles delivery of context info(such as web server's ip addr, digital certificates, and sw licenses) to the vms; 
a separate scheduler component makes vm placement decisions, more specifically, the scheduler has access to info on all reqs OpenNebula receives and, based on these reqs, keeps track of current and future allocations, creating and updating a resource schedule and sending appropriate deployment commands to OpenNebula core; OpenNebula default scheduler provides a rank scheduling policy that places vms on physical resources according to a ranking algorithm that the administrator can configure, it relies on realtime data from both the running vms and available physical resources; 
OpenNebula offers mgmt interfaces to integrate core's functionality within other datacenter mgmt tools, such as accounting or monitoring frameworks, to this end, OpenNebula implements the [libvirt API](http://libvirt.org/), an open interface for vm mgmt, as well as a CLI, adiitionally, a subset of this functionality is exposed to external users through a cloud interface; 
finally, OpenNebula can support a hybrid cloud model by using cloud drivers to interface with external clouds, this lets orgs supplement local infra with computing capacity from a public cloud to meet peak demands, better serve their access reqs(such as by moving the service closer to user), or implement high availablity strategies; OpenNebula currently includes an EC2 driver, which can submit reqs to EC2, Eucalyptus, as well as an ElasticHosts driver.
### Haizea Lease Manager
Haizea is an open source lease manager and can act as a vm scheduler for OpenNebula or be used on its own as a simulator to evaluate different scheduling strategies' perf over time; the fundamental resource provisioning abstraction in Haizea is the lease, intuitively, a lease is a form of contract in which one party agrees to provide a set of resources to another, when a user wants to request computational resources from Haizea, it does so in the form of a lease, leases are then implemented as vms managed by OpenNebula, the lease terms Haizea supports include hw resources, sw environments, and the period during which hw and sw resources must be avialable; currently, Haizea supports ar leases which resources must be available at a specific time; best-effort leases, in which resources are provisioned as soon as possible, and reqs are placed in a queue, if necessary; immediate leases, in which resources are provisioned when requested or not at all; 
to satisfy use cases where resources must be guaranteed to be available at certain times, various researchers have studied advance reservation of computational resources in the context of parallel computing; in the absence of suspension/resumption capabilities, this approach produces resource underutilization due to need to vacate resources before an ar starts; by using vms to implement leases, an org can support ars more efficiently through resource preemption, suspending vms of lower-priority leases before a reservation starts, resuming them after reservation ends, and potentially migrating them to other available nodes or even other clouds, we can do this without having to make the applications inside the vm aware that they are going to be suspended, resumed, or even migrated, however, using vms introduces runtime overhead, which poses additional scheduling challenges, as does the preparation overhead of deploying the vm disk images that the lease needs, such overheads can noticeably affect perf if they are not adequately managed; Haizea's approach is to separately schedule preparation overhead rather than assuming it should just be deducted from a user's allocation, however, this is complicated when Haizea must support multiple lease types with conflicting requirements that it has to reconcile -for example, transfers for a lease starting at 2pm could require delaying transfers for best-effort leases, resulting in longer wait times, Haizea uses several optimizations, such as reusing disk images across leases, to minimize preparation overhead's impact, similarly, Haizea also schedules runtime overhead of suspending, resuming, and migrating vms; 
Haizea bases its scheduling on a resource slot table that represents all physical nodes it manages over time, it schedules best-effort leases using a first-come-first-serve queue with backfilling(a common optimization in queue-based systems), whereas ar leases use a greedy algorithm to select physical resources that minimize #preemptions, although the resource selection algorithm is currently hardcoded, future versions will include a policy decision module to let developers specify their own resource selection policies(such as policies to prioritize leases based on user, group, project, etc.), this policy decision module will also specify the conditions under which Haizea should accept or reject a lease.
****
### Encapsulating Computation (Intel Labs, guest)
### Xen and the Art of Virtualization
### [Barham03](https://dl.acm.org/doi/pdf/10.1145/1165389.945462)
### [Project Page](http://www.cl.cam.ac.uk/netos/xen)
numerous systems have been designed which use virtualization to subdivide ample resources of a modern computer, some require specialized hw, or cannot support commodity os, some target 100% binary compatibility at the expense of perf, others sacrifice security of functionality for speed, few offer resource isolation or perf guarantees, most provide only best-effort provisioning, risking denial of service; 
we present Xen, an x86 vm monitor which allows multiple commodity os to share conventional hw in a safe and resource managed fashion, but without sacrificing their perf or functionality, this is achieved by providing an idealized vm abstraction to which os such as linux, bsd, windows xp, can be ported with minimal effort; 
our design is targeted at hosting up to 100 vm instances simultaneously on a modern server, the virtualization approach taken by Xen is extremely efficient -we allow os such as linux, windows xp, to be hosted simultaneously for a negligible perf overhead -at most a few percent compared with the unvirtualized case, we considerably outperform competing commercial and freely available solutions in a range of microbenchmarks and system-wide tests. 
modern computers are sufficiently powerful to use virtualization to present illusion of many smaller vms, each running a separate os instance, this has lead to a resurgence of interest in vm technology, here we present Xen, a high perf resource-managed VMM -vm monitor which enables applications such as server consolidation, co-located hosting facilities, distributed web services, secure computing platforms, and application mobility; 
successful partitioning of a machine to support concurrent exec of multiple os poses several challenges including: (i)vms must be isolated from one another -it is not acceptable for exec of one to adversely affect perf of another, this is particularly true when vms are owned by mutually untrusting users, (ii)it is necessary to support a variety of different os to accomodate the heterogeneity of popular applications, (iii)the perf overhead introduced by virtualization should be small; 
Xen hosts commodity os, albeit with some source modifications, the prototype described and evaluated here can support multiple concurrent instances of our XenoLinux guest os, each instance exports an application binary interface identical to a non-virtualized linux2.4, our port of windows xp to Xen is not yet complete but is capable of running simple user-space processes, work is also progressing in porting NetBSD; 
Xen enables users to dynamically instantiate an os to execute whether they desire, in the XenoServer project we are deploying Xen on standard server hw at economically strategic locations within ISPs or at internet exchanges, we perform admission control when starting new vms and expect each vm to pay in some fashion for resources it requires, we discuss our ideas and approach in this direction elsewhere, here we focus on vmm; 
there are a number of ways to build a system to host multiple applications and servers on a shared machine, perhaps the simplest is to deploy one or more hosts running a standard os such as linux, windows, and then to allow users to install files and start processes -protection bt applications being provided by conventional os techniques, experience shows that system administration can quickly become a time-consuming task due to complex config interactions bt supposedly disjoint applications; 
more importantly, such systems do not adequately support perf isolation; the scheduling policy, mem demand, network traffic, and disk accesses of one process impact perf of others, this may be acceptable when there is adequate provisioning and a closed user group(such as in the case of computational grids, or the experimental PlanetLab platform), but not when resources are oversubscribed, or users uncooperative; one way to address this problem is to retrofit support for perf isolation to os, this has been demonstrated to a greater or lesser degree with resource containers, linux/rk, qlinux, and silk, one difficulty with such approaches is ensuring that all resource usage is accounted to the correct process -consider, for example, the complext interactions bt applications due to buffer cache or page replacement algorithms, this is effectively the problem of QoS crosstalk within the os, performing multiplexing at a low level can mitigate this problem, as demonstrated by the Exokernel and Nemesis os, unintentional or undesired interactions bt tasks are minimized; 
we use this same basic approach to build Xen, which multiplexes physical resources at the granularity of an entire os and is able to provide perf isolation bt them, in contrast to process-levlel multiplexing this also allows a range of guest os to gracefully coexist rather than mandating a specific application binary interface, there is a price to pay for this flexibility -running a full os is more heavyweight than running a process, both in terms of initialization(such as booting or resuming versus fork and exec), and in terms of resource consumption; 
for our target of up to 100 hosted os instances, we believe this price is worth paying -it allows individual users to run unmodified binaries, or collections of binaries, in a resource controlled fashion(such as an Apache server along with a PostgreSQL backend), furthermore it provides an extremely high level of flexibility since the user can dynamically create the precise exec environment their sw requires, unfortunate config interactions bt various services and applications are avoided(such as each windows instance maintains its own strategy).
### Xen :Approach and Overview
in a traditional vmm the virtual hw exposed is functionally identical to underlying machine, although full virtualization has obvious benefits of allowing unmodified os to be hosted, it also has a number of drawbacks, this is particularly true for the prevalent IA-32, x86, architecture; 
support for full virtualization was never part of the x86 architectural design, certain supervisor instructions must be handled by vmm for correct virtualization, but executing these with insufficient privilege fails silently rather than causing a convenient trap, efficiently virtualizing the x86 mmu is also difficult, these problems can be solved but only at the cost of increased complexity and reduced perf; VMware's ESX Server dynamically rewrites portions of hosted machine code to insert traps wherever vmm intervention might be required, this translation is applied to the entire guest os kernel(with associated translation, exec, and caching costs) since all non-trapping priviledged instructions must be caught and handled, ESX Server implements shadow versions of system structures such as page tables and maintains consistency with the virtual tables by trapping every update attempt -this approach has a high cost for update-intensive ops such as creating a new application process; 
notwithstanding the intricacies of x86, there are other arguments against full virtualization, in particular, there are situations in which it is desirable for hosted os to see real as well as virtual resources -providing both real and virtual time allows a guest os to better support time-sensitive tasks, and to correctly handle tcp timeouts and rtt estimates, while exposing real machine addr allows a guest os to improve perf by using superpages or page coloring; 
we avoid drawbacks of full virtualization by presenting a vm abstraction that is similar but not identical to the underlying hw -an approach which has been dubbed paravirtualization, this promises improved perf, although it does require modifications to the guest os, however, we do not require changes to the ABI -application binary interface, and hence no modifications are required to guest applications, we distill the discussion so far into a set of design principles including: (i)suport for unmodified application binaries is essential, or users will not transition to Xen, hence we must virtualize all architectural features required by existing standard ABIs, (ii)supporting full multiapplication os is important, as this allows complex server configs to be virtualized within a single guest os instance, (iii)paravirtualization is necessary to obtain high perf and strong resource isolation on uncooperative machine architestures such as x86, (iv)even on cooperative machine structures, completely hiding effects of resource virtualization from guest os risks both correctness and perf; 
note that our paravirtualized x86 abstraction is quite different from that proposed by the recent Denali project, Denali is designed to support thousands of vms running network services, the vast majority of which are small-scale and unpopular, in contrast, Xen is intended to scale to approximately 100 vms running industry standard applications and services, given these very different goals, it is instructive to contrast Denali's design choices with our own principles including: (i)Denali does not target existing ABIs, and so can elide certain architectural features from their vm interface, for example, Denali does not fully support x86 segmentation although it is exported(and widely used, such as segments are frequently used by thread libs to address thread-local data) in the ABIs of NetBSD, linux, windows xp; (ii)Denali implementation does not address the problem of supporting application multiplexing, nor multiple addr spaces, within a single guest os, rather, applications are linked explicitly against an instance of the Ilwaco guest os in a manner rather reminiscent of a libOS in the Exokernel, hence each vm essentially hosts a single-user single-application unprotected os, by contrast, in Xen a single vm hosts a real os which may itself securely multiplex thousands of unmodified user-level processes, although a prototype virtual mmu has been developed which may help Denali in this area, we are unaware of any published technical details or evaluation; (iii)in the Denali archiecture the vmm performs all paging to and from disk, this is perhaps related to lack of mem-mgmt support at virtualization layer, paging within vmm is contrary to our goal of perf isolation -malicious vms can encourage thrashing behaviour, unfairly depriving others of cpu time and disk bdwidth, in Xen we expect each guest os to perform its own paging using its own guaranteed mem reservation and disk allocation(an idea previously exploited by self-paging); (iv) Denali virtualizes the namespaces of all machine resources, taking the view that no vm can access the resource allocations of another vm if it cannot name them(such as vms have no knowledge of hw addrs but only the virtual addrs created for them by Denali), in contrast, we believe that secure access control within the hypervisor is sufficient to ensure protection -furthermore, as discussed previously, there are strong correctness and perf arguments for making physical resources directly visible to guest os; in the following section we describe vm abstraction exported by Xen and discuss how a guest os must be modified to conform to this, note that here we reserve the term guest os to refer to one of the os that Xen can host and we use the term domain to refer to a running vm within which a guest os executes, the distinction is analogous to that bt a program and a process in a conventional system, we call Xen itself the hypervisor since it operates at a higher privilege level than the supervisor code of the guest os that it hosts; 
**vm interface:** [click for an overview of the paravirtualized x86 interface](./img/paravirtualized-x86-interface.png). factored into 3 broad aspects of the system: mem mgmt, cpu, device io, in the following we address each machine subsystem in turn, and discuss how each is presented in our paravirtualized architecture, note that although certain parts of our implementation, such as mem mgmt, are specific to x86, many aspects(such as our virtual cpu and io devices) can be readily applied to other machine architectures, furthermore, x86 represents a worst case in the areas where it differs significantly from risc-style processors, such as efficiently virtualizing hw page tables is more difficult than virtualizing a sw-managed tlb; 
**mem mgmt:** virtualizing mem is undoubtedly the most difficult part of paravirtualizing an architecture, both in terms of mechanisms required in the hypervisor and modifications required to port each guest os, the task is easier if architecture provides a sw-managed tlb as these can be efficiently virtualized in a simple manner, a tagged tlb is another useful feature supported by most server-class risc architectures including: Alpha, MIPS, SPARC, associating an addr-space identifier tag with each tlb entry allows the hypervisor and each guest os to efficiently coexist in separate addr spaces because there is no need to flush the entire tlb when transferring exec; unfortunately, x86 does not have a sw-managed tlb, instead, tlb misses are serviced automatically by the processor by walking the page table structure in hw, hence to achieve the best possible perf, all valid page translators for the current addr space should be present in the hw-accessible page table, moreover, because tlb is not targeted, addr space switches typically require a complete tlb flush, given these limitations, we made 2 decisions: (i)guest os are responsible for allocating and managing the hw page tables, with minimal involvement from Xen to ensure safety and isolation, (ii)Xen exists in a 64MB section at the top of every addr space, hence avoiding a tlb flush when entering and leaving the hypervisor; each time a guest os requires a new page table, perhaps because a new process is being created, it allocates and initializes a page from its own mem reservation and registers it with Xen, at this point os must relinquish direct write privileges to the page-table mem, all subsequent updates must be validated by Xen, this restrictes updates in a number of ways, including only allowing an os to map pages that it owns and disallowing writable mappings of page tables; guest os may batch update reqs to amortize overhead of entering the hypervisor; the top 64MB region of each addr space, which is reserved for Xen, is not accessible or remappable by guest os, however, this addr region is not used by any of the common x86 ABIs, hence this restriction does not break application compatibility; segmentation is virtualized in a similar way, by validating updates to hw segment descriptor tables, the only restrictions on x86 segment descriptors are: (i)they must have lower privilege than Xen, (ii)they may not allow any access to Xen-reserved portion of the addr space; 
**cpu:** virtualizing cpu has several implications for guest os, principally, insertion of a hypervisor below os violates usual assumption that os is the most privileged entity in the system, in order to protect the hypervisor from os misbehavior(and domains from one another), guest os must be modified to run at a lower privilege level; many processor archiectures only provide 2 privilege levels, in these cases guest os would share the lower privilege level with applications, guest os would then protect itself by running in a separate addr space from its applications, and indirectly pass control to and from applications via hypervisor to set the virtual privilege level and change current addr space, again, if processor's tlb supports addr-space tags then expensive tlb flushes can be avoided; efficient virtualization of privilege levels is possible on x86 because it supports 4 distinct privilege levels in hw, the x86 privilege levels are generally described as rings, and are numbered from 0(most privileged) to 3(least privileged), os code typically executes in ring0 because no other ring can execute privileged instructions, while ring3 is generally used for application code, to our knowledge, rings1&2 have not been used by any well-known x86 os since os/2, any os which follows this common arrangement can be ported to Xen by modifying it to execute in ring1, this prevents guest os from directly executing privileged instructions, yet it remains safely isolated from applications running in ring3; privileged instructions are paravirtualized by requiring them to be validated and executed within Xen -this applies to ops such as installing a new page table, or yielding the processor when idle(rather than attempting to hlt it), any guest os attempt to directly execute a privileged instruction is failed by the processor, either silently or by taking a fault, since only Xen executes at a sufficiently privileged level; exceptions, including mem faults and sw traps, are virtualized on x86 very straightforwardly, a table describing the handler for each type of exception is registered with Xen for validation, the handlers specified in this table are generally identical to those for real x86 hw, this is possible because the exception stack frames are unmodified in our paravirtualized architecture, the sole modifications is to the page fault handler, which would normally read the faulting addr from a privileged processor register(CR2), since this is not possible, we write it into an extended stack frame(*in hindsight, writing the value into a pre-agreed shared mem location rather than modifying the stack frame would have simplified the xp port), when an exception occurs while executing outside ring0, Xen's handler creates a copy of the exception stack frame on guest os stack and returns control to the appropriate registered handler; typically only 2 types of exception occur frequently enough to affect system perf -system calls(which are usually implemented via a sw exception) and page faults, we improve the perf of system calls by allowing each guest os to register a fast exception handler which is accessed directly by the processor without indirecting via ring0, this handler is validated before installing it in the hw exception table, unfortunately it is not possible to apply the same technique to the page fault handler because only code executing in ring0 can read the faulting addr from reg CR2, page faults must therefore always be delivered via Xen so that this reg value can be saved for access in ring1; safely is ensured by validating exception handlers when they are presented to Xen, the only required check is that the handler's code segment does not specify exec in ring0, since no guest os can create such a segment it suffices to compare the specified segment selector to a small number of static values which are reserved by Xen, apart from this, any other handler problems are fixed up during exception propagation -for example, if the handler's code segment is not present or if the handler is not paged into mem then an appropriate fault will be taken when Xen executes the iret instruction which returns to the handler, Xen detects these double faults by checking the faulting program counter value -if addr resides within the exception-virtualizing code then the offending guest os is terminated; note that this lazy checking is safe even for the direct system-call handler -access faults will occur when cpu attempts to directly jump to guest os handler, in this case, the faulting addr will be outside Xen(since Xen will never execute a guest os system call) and so the fault is virtualized in the normal way, if propagation of the fault causes a further double fault then guest os is terminated as described above; 
**device io:** rather than emulating existing hw devices, as is typically done in fully virtualized environments, Xen exposes a set of clean and simple device abstractions, this allows us to design an interface that is both efficient and satisfies our requirements for protection and isolation, to this end, io data is transferred to and from each domain via Xen, using shared-mem, asynchronous buffer-descriptor rings, these provide a high-perf comm mechanism for passing buffer info vertically through the system, while allowing Xen to efficiently perform validation checks(such as checking that buffers are contained within a domain's mem reservation); similar to hw interrupts, Xen supports a lightweight event-delivery mechanism which is used for sending asynchronous notifications to a domain, these notifications are made by updating a bitmap of pending event types and, optionally, by calling an event handler specified of guest os(such as to avoid extra costs incurred by frequent wake-up notifications).
### The Cost of Porting an OS to Xen
[click for cost, in lines of code, of porting commodity os to Xen's paravirtualized x86 environment](./img/cost-of-porting-commodity-operating-systems-to-Xen-paravirtualized-x86-environment.png), note that our NetBSD port is at a very early stage, and hence we report no figures here, the xp port is more advanced, but still in progress, it can execute a number of user-space applications from a ram disk, but it currently lacks any virtual io drivers, for this reason, figures for xp's virtual device drivers are not presented, however, as with linux, we expect these drivers to be small and simple due to the idealized hw abstraction presented by Xen; 
windows xp required a surprising number of modifications to its architecture independent os code because it uses a variety of structures and unions for accessing PTEs -page-table entries, each page-table access had to be separately modified, although some of this process was automated with scripts, in contrast, linux needed far fewer modifications to its generic mem system as it uses pre-processor macros to access PTEs -the macro definitions provide a convenient place to add translation and hypervisor calls required by paravirtualization; in both os, the architecture-specific sections are effectively a port of x86 code to our paravirtualized architecture, this involved rewriting routines which used privileged instructions, and removing a large amount of low-level system initialization code, again, more changes were required in windows xp, mainly due to presence of legacy 16-bit emulation code and need for a somewhat different boot-loading mechanism, note that the x86-specific code base in xp is substantially larger than in linux and hence a larger porting effort should be expected.
### Control and Mgmt
throughout the design and implementation of Xen, a goal has been to separate policy from mechanism wherever possible, although the hypervisor must be involved in data-path aspects(such as scheduling cpu bt domains, filtering network packets before transmission, or enforcing access control when reading data blocks), there is no need for it to be involved in, or even aware of, higher level issues such as how cpu is be to shared, or which kinds of packet each domain may transmit; 
the resulting architecture is one in which the hypervisor itself provides only basic control ops, these are exported through an interface accessible from authorized domains, potentially complex policy decisions such as admission control are best performed by mgmt sw running over a guest os rather than in privileged hypervisor code; 
[click for the overall system structure](./img/overall-system-structure-of-a-machine-running-the-xen-hypervisor.png), note that a domain is created at boot time which is permitted to use the control interface, the initial domain, termed Domain0, is responsible for hosting the application-level mgmt sw, the control interface provides the ability to create and terminate other domains and to control their associated scheduling params, physical mem allocations, and the access they are given to the machine's physical disks and network devices; 
in addition to processor and mem resources, the control interface supports creation and delection of VIFs -virtual network interfaces and VBDs -virtual block devices, these virtual io devices have associated access-control info which determines which domains can access them, and with what restrictions(such as a read-only vbd may be created, or a vif may filter ip packets to prevent source-addr spoofing); 
this control interface, together with profiling stats on current state of the system, is exported to a suite of application-level mgmt sw running in Domain0, this complement of administrative tools allow convenient mgmt of the entire server -current tools can create and destroy domains, set network filters and routing rules, monitor per-domain network activity at packet and flow granularity, and create and delete virtual network interfaces and virtual block devices, we anticipate dev of higher-level tools to further automate the application of administrative policy.
### Detailed Design
in this section we introduce design of the major subsystems that make up a Xen-based server, in each case we present both Xen and guest os functionality for clarity of exposition, the current discussion of guest os focuses on XenoLinux as this is the most mature, nonetheless our ongoing porting of windows xp and netbsd gives us confidence that Xen is guest os agnostic.
### Control Transfer :Hypercalls and Events
2 mechanisms exist for control interactions bt Xen and an overlying domain :synchronous calls from a domain to Xen may be made using a hypercall, while notifications are delivered to domains from Xen using an asynchronous event mechanism; 
the hypercall interface allows domains to perform a synchronous sw trap into hypervisor to perform a privileged op, analogous to use of system calls in conventional os, an example use of a hypercall is to request a set of page-table updates, in which Xen validates and applies a list of updates, returning control to the calling domain when this is completed; 
comm from Xen to a domain is provided through an asynchronous event mechanism, which replaces usual delivery mechanisms for device interrupts and allows lightweight notification of important events such as domain-termination reqs, akin to traditionl unix signals, there are only a small number of events, each acting to flag a particular type of occurence, for example, events are used to indicate that new data has been received over the network, or that a virtual disk req has completed; 
pending events are stored in a per-domain bitmask which is updated by Xen before invoking an event-callback handler specified by guest os, the callback handler is responsible for resetting the set of pending events, and responding to the notifications in an appropriate manner, a domain may explicitly defer event handling by setting a Xen-readable sw flag :this is analogous to disabling interrupts on a real processor.
### Data Transfer :I/O Rings
the presence of a hypervisor means there is an additional protection domain bt guest os and io devices, so it is crucial that a data transfer mechanism be provided that allows data to move vertically through the system with as little overhead as possible; 
2 main factors have shaped the design of our io-transfer mechanism :resource mgmt and event notification, for resource accountability, we attempt to minimize the work required to demultiplex data to a specific domain when an interrupt is received from a device -the overhead of managing buffers is carried out later where computation may be accounted to the appropriate domain, similarly, mem committed to device io is provided by relevant domains wherever possible to prevent crosstalk inherent in shared buffer pools, io buffers are protected during data transfer by pinning underlying page frames within Xen; 
[click for structure of our io descriptor rings](./img/structure-of-asynchronous-io-rings.png), here a ring is a circular queue of descriptors allocated by a domain but accessible from within Xen, descriptors do not directly contain io data, instead, io data buffers are allocated out-of-band by guest os and indirectly referenced by io descriptors, access to each ring is based around 2 pairs of producer-consumer pointers :domains place reqs on a ring, advancing a req producer pointer, and Xen removes these reqs for handling, advancing an associated req consumer pointer, responses are placed back on the ring similarly, save with Xen as the producer and guest os as the consumer, there is no requirement that reqs be processed in order :guest os associates a unique identifier with each req which is reproduced in the associated response, this allows Xen to unambiguously reorder io ops due to scheduling or priority considerations; 
this structure is sufficiently generic to support a number of different device paradigms, for example, a set of reqs can provide buffers for network packet reception, subsequent responses then signal arrival of packets into these buffers, reordering is useful when dealing with disk reqs as it allows them to be scheduled within Xen for efficiency, and use of descriptors with out-of-band buffers makes implementing zero-copy transfer easy; 
we decouple production of reqs or responses from the notification of the other party -in the case of reqs, a domain may enqueue multiple entries before invoking a hypercall to alert Xen, in the case of responses, a domain can defer delivery of a notificatioon event by specifying a threshold number of responses, this allows each domain to trade-off latency and throughput requirements, similarly to the flow-aware interrupt dispatch in the ArseNIC Gigabit Ethernet interface.
### Building a New Domain
the task of building the initial guest os structures for a new domain is mostly delegated to Domain0 which uses its privileged control interfaces(see section Control and Mgmt) to access the new domain's mem and inform Xen of initial reg state, this approach has a number of advantages compared with building a domain entirely within Xen, including reduced hypervisor complexity and improved rebustness(accesses to privileged interface are sanity checked which allowed us to catch many bugs during initial deployment); however, most important is the case with which the building process can be extended and specialized to cope with new guest os, for example, the boot-time addr space assumed by the linux kernel is considerably simpler than that expected by windows xp, it would be possible to specify a fixed initial mem layout for all guest os, but this would require additional bootstrap code within every guest os to lay things out as required by the rest of os, unfortunately this type of code is tricky to implement correctly, for simplicity and robustness it is therefore better to implement it within Domain0 which can provide much richer diagnostics and debugging support than a bootstrap environment.
