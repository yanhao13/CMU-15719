# CMU-15719
### Introduction, Use cases, and Elasticity
### A View of Cloud Computing
### [Armbrust2010](https://dl.acm.org/doi/10.1145/1721654.1721672)
### Clearing the clouds away from the true potential and obstacles posed by this computing capability.
### Oracle's CEO Larry Ellison vent his frustration: "The interesting thing about cloud computing is that we've redefined cloud computing to include exerything that we already do.... I don't understand what we would do differently in the light of cloud computing other than change the wording of some of our ads."
### Just as large ISPs use multiple network providers so that failure by a single company will not take them off the air, we believe the only plausible solution to very high availability is multiple cloud computing providers.
cloud computing, the long-held dream of computing as a utility, has the potential to transform a large part of it industry, making sw even more attractive aaS and shaping the way it hw is designed and purchased; 
developers with innovative ideas for new internet services no longer require the large capital outlays in hw to deploy their service or the human expense to operate it, they need not be concerned about overprovisioning for a service whose popularity does not meet their predictions, hus wasting costly resources, or underprovisioning for one that becomes wildly popular, thus missing potential customers and revenue; 
moreover, companies with large batch-oriented tasks can get results as quickly as their programs can scale, since using 1000 servers for 1hr costs no more than using 1 server for 1000hrs, this elasticity of resources, without paying a premium for large scale, is unprecedented in history of it.
### Defining Cloud Computing
cloud computing refers to both the applications delivered aaS over the internet and the hw and systems sw in the datacenters that provide those services, the services themselves have long been referred to as SaaS(*here we use the term SaaS to mean applications delivered over the internet, the broadest definition would encompass any on demand sw, including those that run sw locally but control use via remote sw licensing), some vendors use terms such as IaaS, PaaS to describe their products, but we eschew these because accepted definitions for them still vary widely, the line bt low-level infra and a higher-level platform is not crisp, we believe the two are more alike than different, and we consider them together; similarly, the related term grid computing, from the hp computing community, suggests protocols to offer shared computation and storage over long distances, but those protocols did not lead to a sw environment that grew beyond its community; 
the datacenter hw and sw is what we will call a **cloud**, when a cloud is made available in a pay-as-you-go manner to the general public, we call it a **public cloud**, the service being sold is **utility computing**, we use the term **private cloud** to refer to internal datacenters of a business or other org, not made available to the general public, when they are large enough to benefit from the advantages of cloud computing that we discuss here; thus cloud computing is sum of SaaS and utility computing, but does not include small or medium sized datacenters, even if these rely on virtualization for mgmt, people can be users or providers of SaaS, or users or providers of utility computing; we focus on SaaS providers(cloud users) and cloud providers, which have received less attention than SaaS users, [click for making provider-user relationships clear](./img/making-provider-user-relationships-clear.png), in some cases, the same actor can play multiple roles, for example, a cloud provider might also host its own customer-facing services on cloud infra; 
from a hw provisioning and pricing point of view, 3 aspects are new in cloud computing including: the appearance of infinite computing resources available on demand, quickly enough to follow load surges, thereby eliminating need for cloud computing users to plan far ahead for provisioning; elimination of an up-front commitment by cloud users, thereby allowing companies to start small and increase hw resources only when there is an increase in their needs(*note, however, that upfront commitments can still be used to reduce per-usage charges, for exmaple, AWS also offers long-term rental of servers, which they call reserved instances); the ability to pay for use of computing resources on a short-term basis as needed(such as processors by the hr and storage by the day) and release them as needed, thereby rewarding conservation by letting machines and storage go when they are no longer useful; 
we argue that construction and operation of extremely large scale, commodity-computer datacenters at low cost locations was the key necessary enabler of cloud computing, for they uncovered factors of 5-7 decrease in cost of electricity, network bdwidth, ops, sw, and hw available at these very large economies of scale; these factors, combined with statistical multiplexing to increase utilization compared to traditional datacenters, meant that cloud computing could offer services below the costs of a medium sized datacenter and yet still make a good profit; 
out proposed definition allows us to clearly identify certain installations as examples and non-examples of cloud computing, consider a public-facing internet service hosted on an ISP who can allocate more machines to the service given 4hrs notice, since load surges on the public internet can happen much more quickly than that(we saw its load to double every 12 hrs for nearly 3 days), this is not cloud computing, in contrast, consider an internal enterprise datacenter whose applications are modified only with significant advance notice to admins, in this scenario large load surges on the scale of mins are highly unlikely, so as long as allocation can track expected load increases, this scenario fulfills one of the necessary conditions for operating as a cloud, the enterprise datacenter may still fail to meet other conditions for being a cloud, however, such as the appearance of infinite resources for fine-grained billing; a private datacenter may also not benefit from economies of scale that make public clouds financially attractive; 
omitting private clouds from cloud computing has led to considerable debate in the blogosphere, we believe the confusion and skepticism illustrated by Larry Ellison's quote occurs when advantages of public clouds are also claimed for medium sized datacenters, except for extremely large datacenters of hundreds of thousands of machines, such as those that might be operated by Google, Microsoft, [most datacenters enjoy only a subset of the potential advantages of public clouds, click to view](./img/most-data-centers-enjoy-only-a-subset-of-the-potential-advantages-of-public-clouds.png), hence we believe that including traditional datacenters in the definition of cloud computing will lead to exaggerated claims for smaller so-called private clouds, which is why we exclude them, however, here we describe how so-called private clouds can get more of the benefits of public clouds through surge computing, hybrid cloud computing.
### Classes of Utility Computing
any application needs a model of computation, a model of storage, and a model of comm, the statistical multiplexing necessary to achieve elasticity and the appearance of infinite capacity available on demand requires automatic allocation and mgmt, in practice, this is done with virtualization of some sort, we argue that different utility computing offerings will be distinguished based on the cloud sytem sw's level of abstraction and on the level of mgmt of resources; 
Amazon EC2 is at one end of the spectrum, an EC2 instance looks much like physical hw, and users can control nearly the entire sw stack, from the kernel upward, this low level makes it inherently difficult for Amazon to offer automatic scalability and failover because the semantics associated with replication and other state mgmt issues are highly application-dependent; at the other extreme of the spectrum are application domain-specific platforms such as Google AppEngine, which is targeted exclusively at traditional web applications, enforcing an application structure of clean separation bt a stateless computation tier and a stateful storage tier, AppEngine's impressive automatic scaling and high-availability mechanisms, and the proprietary MegaStore data storage available to AppEngine applications, all rely on these constraints; applications for Microsoft's Azure are written using .NET libs, and compiled to the Common Language Runtime, a lang-independent managed environment, the framework is significantly more flexible than AppEngine's, but still constrains user's choice of storage model and application structure, hence Azure is intermediate bt hw vms like EC2 and application frameworks like AppEngine.
### Cloud Computing Economics
we see 3 particularly compelling use cases that favor utility computing over conventional hosting, including: (i)when demand for a service varies with time, for example, provisioning a datacenter for the peak load it must sustain a few days per month leads to underutilization at other times, instead, cloud computing lets an org pay by hr of computing resources, potentially leading to cost savings even if the hourly rate to rent a machine from a cloud provider is higher than the rate to own one; (ii)when demand is unknown in advance, for example, a web startup will need to support a spike in demand when it becomes popular, followed potentially by a reduction once some visitors turn away; (iii)orgs that perform batch analytics can use the cost associativity of cloud computing to finish computations faster :using 1000 EC2 machines for 1hr costs the same as using 1 machine for 1000hrs; 
although the economic appeal of cloud computing is often described as CapEx2OpEx -converting capital expenses to operating expenses, we believe the phrase pay-as-you-go more directly captures the economic benefit to the buyer, hrs purchased via cloud computing can be distributed non-uniformly in time(for example, use 100 server hrs today and no server hrs tomorrow, and still pay only for 100), in the networking community, this way of selling bdwidth is already known as usage-based pricing(*usage-based pricing is not renting, renting a resources involves paying a negotiated cost to have the resource over some time period, whether or not you use the resource, vs., pay-as-you-go involves metering usage and charging based on actual use, independently of the time period over which the usage occurs); in addition, absence of upfront capital expense allows capital to be rediected to core business investment; 
hence, even if Amazon's pay-as-you-go pricing was more expensive than buying and depreciating a comparable server over the same period, we argue that the cost is outwrighed by the extremely important cloud computing economic benefits of elasticity and transference of risk, esp. risks of overprovisioning(underutilization) and underprovisioning(saturation); 
we start with elasticity, key observation is that cloud computing's ability to add or remove resources at a fine grain(1 server at a time with EC2) and with a lead time of mins rather than weeks allows matching resources to wkload much more closely, real world estimates of average server utilization in datacenters range from 5%-20%, this may sound shockingly low, but it is consistent with the observation that for many services the peak wkload exceeds the average by factors of 2-10, since few users deliberately provision for less than the expected peak, resources are idle at nonpeak times, the more pronounced the variation, the more the waste, [click for example in fig2a](./img/elasticity.png), here, we assume our service has a predictable demand where the peak requires 500 servers at noon but the trough requires only 100 servers at midnight, as long as the average utilization over a whole day is 300 servers, the actual cost per day(area under the curve) is 300times24=7200 server hrs, but since we must provision to the peak of 500 servers, we pay for 500times24=12000 server hrs, a factor of 1.7 more; hence, as long as the pay-as-you-go cost per server hr over 3 years(typically amortization time) is less than 1.7 times cost of buying the server, utility computing is cheaper; in fact, this example underestimates benefits of elasticity, because in addition to simple diurnal patterns, most services also experience seasonal or other periodic demand variation(such as e-commerce in December and photo sharing sites after holidays) and some unexpected demand bursts due to external events(such as news events), since it can take weeks to acquire and rack new equipment, to handle such spikes you must provision for them in advance, we already saw that even if service operators predict the spike sizes correctly, capacity is wasted, and if they overestimate the spike they provision for, it is even worse; 
[they may also underestimate spike, click to view in fig2b](./img/elasticity.png), however, accidentally turning away excess users, while cost of overprovisioning is easily measured, cost of underprovisioning is more difficult to measure yet potentially equally serious :not only do rejected users generate 0 revenue, they may never come back; [for example, Friendster's decline in popularity relative to competitors Facebook, MySpace, is believed to have resulted partly from user dissatisfaction with slow repsonse times(up to 40 secs), click to view in fig2c](./img/elasticity.png) :users will desert an underprovisioned service until peak user load equals datacenter's usable capability, at which point users again receive acceptable service; for another simplified example, assume that users of a hypothetical site fall into 2 classes :active users(those who use the site regularly) and defectors(those who abandon the site due to poor perf), further, suppose that 10% of active users who receive poor service due to underprovisioning are permanently lost opportunities(become defectors), i.e., users who would have remained regular visitors with a better experience, the site is initially provisioned to handle an expected peak of 400000 users(1000 users per server times 400 servers), but unexpected positive press drives 500000 users in the first hr, of the 100000 who are turned away or receive bad service, by our assumption 10000 of them are permanently lost, leaving an active user base of 390000, the next hr sees 250000 new unique users, the first 10000 do fine, but the site is still overcapacity by 240000 users, this results in 24000 additional defections, leaving 376000 permanent users, if this pattern continues, after lg(500000) or 19hrs, #new users will approach 0 and the site will be at capacity in steady state, clearly, the service operator has collected less than 400000 users' worth of steady revenue during those 19hrs, however, again illustrating the underutilization argument, to say nothing of the bad reputation from disgruntled users; 
do such scenarios really occur in practice? when we made our service available via Facebook, it experienced a demand surge that resulted in growing from 50 servers to 3500 servers in 3 days, even if the average utilization of each server was low, no one could have foreseen that resource needs would suddenly double every 12hrs for 3 days, after the peak subsided, traffic fell to a lower level, so in this real world example, scale-up elasticity allowed the steady-state expenditure to more closely match the steady-state wkload.
### Top 10 Obstacles and Opportunities for Cloud Computing
[click to view](./img/top-ten-obstacles-and-opportunities-for-cloud-computing.png), the first 3 affect adoption, the next 5 affect growth, the last 2 are policy and business obstacles, each obstacle is paired with an opportunity to overcome that obstacle, ranging from product dev to research projects.
### 拨开云雾，展现云计算的真正潜力，并消除其带来的障碍。
### Oracle 首席执行官 Larry Ellison 表达了他的不满：“云计算的有趣之处在于，我们重新定义了云计算，将我们已经在做的一切都涵盖其中……除了修改一些广告措辞之外，我不知道在云计算的背景下，我们还能做什么不同的事情。”
云计算，作为计算作为一种实用工具的长期梦想，有可能改变 IT 行业的很大一部分，使软件即服务 (aaS) 更具吸引力，并塑造硬件的设计和购买方式；
拥有创新互联网服务理念的开发人员不再需要投入大量的硬件资金来部署他们的服务，也不再需要投入大量的人力来运营它，他们不必担心为受欢迎程度未达预期的服务过度配置资源，从而浪费昂贵的资源，也不必担心为已经非常流行的服务配置资源不足，从而错失潜在客户和收入；
此外，拥有大量批处理任务的公司能够像其程序扩展一样快速获得结果，因为使用 1000 台服务器 1 小时的成本不超过使用 1 台服务器 1000 小时的成本，这种资源弹性在历史上是前所未有的，无需为大规模支付额外费用。
### 定义云计算
云计算既指通过互联网以“即服务”（aaS）形式交付的应用程序，也指数据中心中提供这些服务的硬件和系统软件。这些服务本身长期以来被称为 SaaS（*此处我们使用 SaaS 一词表示通过互联网交付的应用程序，最广泛的定义涵盖任何按需软件，包括在本地运行软件但通过远程软件许可控制使用的软件）。一些供应商使用 IaaS、PaaS 等术语来描述他们的产品，但我们避免使用这些术语，因为它们的公认定义仍然差异很大，低级基础设施和高级平台之间的界限并不明确，我们认为两者的相同之处多于不同之处，我们认为将它们结合在一起；同样，来自惠普计算社区的相关术语“网格计算”提出了提供远距离共享计算和存储的协议，但这些协议并没有催生出一个超越其社区的软件环境；数据中心硬件和软件就是我们所说的**云**，当云以按需付费的方式向公众开放时，我们称之为**公共云**，所出售的服务是**效用计算**，我们使用术语**私有云**来指代企业或其他组织的内部数据中心，这些数据中心不向公众开放，但当它们规模足够大到能够从我们在此讨论的云计算优势中受益时；因此，云计算是 SaaS 和效用计算的总和，但不包括中小型数据中心，即使这些数据中心依赖虚拟化进行管理，人们可以是 SaaS 的用户或提供商，也可以是效用计算的用户或提供商；我们关注的是 SaaS 提供商（云用户）和云提供商，这两者的关注度低于 SaaS 用户。[点击查看提供商-用户关系](./img/making-provider-user-relationships-clear.png)，在某些情况下，同一个参与者可以扮演多个角色，例如，云提供商可能还会在云端基础设施上托管自己的面向客户的服务；
从硬件配置和定价的角度来看，云计算有三个新特点：无限计算资源的出现，可以按需提供，并且能够快速应对负载激增，从而消除了云计算用户提前规划配置的需要；取消了云用户的前期承诺，从而允许公司从小规模开始，仅在需求增加时才增加硬件资源（*但请注意，前期承诺仍然可以用来降低每次使用的费用，例如，AWS 也提供服务器的长期租赁，他们称之为预留实例）；能够根据需要短期支付计算资源的使用费用（例如按小时支付处理器费用和按天支付存储费用），并根据需要释放这些资源，从而通过在不再使用时释放机器和存储来奖励节约；
我们认为，在低成本地点建设和运营超大规模商用计算机数据中心是云计算的关键必要因素，因为它们揭示了在这些非常大的规模经济下，电力、网络带宽、操作、软件和硬件成本降低 5 至 7 倍的因素；这些因素与统计复用相结合，提高了与传统数据中心相比的利用率，意味着云计算可以提供低于中型数据中心成本的服务，同时仍然获得良好的利润；
我们提出的定义使我们能够清楚地识别某些安装作为示例和非云计算的例子，考虑一个托管在 ISP 上的面向公众的互联网服务，该 ISP 可以在提前 4 小时通知的情况下为该服务分配更多机器，因为公共互联网上的负载激增可能发生得更快（我们看到它的负载每 12 小时翻一番，持续了近 3 天），这不是云计算。相反，考虑一个内部企业数据中心，其应用程序只有在提前通知管理员的情况下才会进行修改，在这种情况下，分钟级的大规模负载激增极不可能发生，因此只要分配可以跟踪预期的负载增长，这种情况就满足了作为云运营的必要条件之一。然而，企业数据中心可能仍然无法满足成为云的其他条件，例如出现无限资源以实现细粒度计费；私有数据中心也可能无法从使公共云具有经济吸引力的规模经济中受益；
将私有云从云计算中剔除在博客圈引发了相当大的争议，我们认为，拉里·埃里森的引言所体现的困惑和怀疑，发生在公有云的优势也适用于中型数据中心的情况下，除了拥有数十万台机器的超大型数据中心，例如谷歌、微软运营的那些。[大多数数据中心只享有公有云的部分潜在优势，点击查看](./img/most-data-centers-enjoy-only-a-subset-of-the-potential-advantages-of-public-clouds.png)，因此，我们认为将传统数据中心纳入云计算的定义会导致对规模较小的所谓私有云的夸大宣传，这就是我们将其排除在外的原因。然而，我们在这里描述了所谓的私有云如何通过激增计算、混合云计算获得更多公有云的优势。
### 效用计算的类别
任何应用程序都需要一个计算模型、一个存储模型和一个comm，实现弹性和按需无限容量所需的统计复用需要自动分配和管理，实际上，这是通过某种虚拟化实现的，我们认为不同的效用计算产品将根据云系统软件的抽象级别和资源管理级别进行区分；
Amazon EC2 处于一个极端，EC2 实例看起来很像物理硬件，用户可以控制几乎整个软件堆栈，从内核向上。这种低级别使得 Amazon 很难提供自动可扩展性和故障转移，因为与复制和其他状态管理问题相关的语义高度依赖于应用程序；另一个极端是应用程序领域特定平台，例如 Google AppEngine，它专门针对传统的 Web 应用程序，强制执行无状态计算层和有状态存储层干净分离的应用程序结构，AppEngine 令人印象深刻的自动扩展和高可用性机制，以及 AppEngine 应用程序可用的专有 MegaStore 数据存储，都依赖于这些约束；微软 Azure 的应用程序使用 .NET 库编写，并编译为公共语言运行时（Common Language Runtime，一个独立于语言的托管环境）。该框架比 AppEngine 的框架灵活得多，但仍然限制了用户对存储模型和应用程序结构的选择，因此 Azure 介于 EC2 等硬件虚拟机和 AppEngine 等应用程序框架之间。
### 云计算经济学
我们发现 3 个特别引人注目的用例表明，效用计算优于传统托管，包括：(i) 当服务需求随时间变化时，例如，为数据中心配置每月必须维持几天的峰值负载，会导致其他时间利用率不足。相反，云计算允许组织按小时支付计算资源费用，即使从云提供商租用机器的小时费率高于拥有机器的费率，也有可能节省成本； (ii) 当需求事先未知时，例如，一家网络初创公司需要支持其在流行时出现的需求激增，而一旦一些访问者流失，需求可能会减少；(iii) 执行批量分析的组织可以利用云计算的成本关联性来更快地完成计算：使用 1000 台 EC2 机器 1 小时的成本与使用 1 台机器 1000 小时的成本相同；
虽然云计算的经济吸引力通常被描述为资本支出转化为运营支出（CapEx2OpEx），即将资本支出转化为运营支出，但我们认为“按需付费”这一说法更直接地体现了买方的经济效益，通过云计算购买的服务器小时数可以不均匀地分配（例如，今天使用 100 小时服务器时间，明天不使用，仍然只需支付 100 小时的费用），在网络社区中，这种销售带宽的方式被称为基于使用量的定价（*基于使用量的定价不是租用，租用资源涉及支付协商好的费用以在一段时间内拥有资源，无论是否使用资源；而按需付费则涉及计量使用量并根据实际使用情况收费，与使用发生的时间段无关）；此外，由于没有前期资本支出，资本可以重新用于核心业务投资；因此，即使亚马逊的按需付费定价比在同一时期购买和折旧一台同类服务器的成本更高，我们认为，其成本也被云计算极其重要的经济效益所抵消，即弹性和风险转移，尤其是过度配置（利用不足）和配置不足（饱和）的风险；
我们从弹性开始，关键的观察是云计算能够以细粒度添加或删除资源（使用 EC2 一次添加或删除一台服务器），并且只需几分钟而不是几周的准备时间，就可以更紧密地匹配资源以进行负载均衡，现实世界中数据中心的平均服务器利用率估计在 5% 到 20% 之间，这听起来可能低得惊人，但这与以下观察结果一致：对于许多服务而言，峰值负载是平均值的 2 到 10 倍，因为很少有用户故意提供低于预期峰值的资源，资源在非峰值时段处于闲置状态，变化越明显，浪费就越多，[单击图 2a 中的示例](./img/elasticity.png)，在这里，我们假设我们的服务具有可预测的需求，峰值在中午需要 500 台服务器，但低谷在午夜只需要 100 台服务器，只要全天的平均利用率为 300 台服务器，则每天的实际成本（曲线下面积）为300x24=7200 服务器小时，但由于我们必须为 500 台服务器的峰值进行配置，因此我们需要支付 500x24=12000 服务器小时的费用，比峰值高出 1.7 倍；因此，只要 3 年内（通常是摊销期）每服务器小时的即用即付成本小于购买服务器成本的 1.7 倍，效用计算就会更便宜；事实上，这个例子低估了弹性的优势，因为除了简单的昼夜模式外，大多数服务还会经历季节性或其他周期性需求变化（例如 12 月的电子商务和节假日后的照片分享网站）以及一些由于外部事件（例如新闻事件）导致的意外需求爆发，因为购买和上架新设备可能需要数周时间，为了应对这样的峰值，您必须提前做好准备，我们已经看到，即使服务运营商正确预测了峰值规模，容量也会被浪费，如果他们高估了他们所准备的峰值，情况会更糟；
[他们也可能低估峰值，单击查看图 2b](./img/elasticity.png)，然而，意外地拒绝了多余的用户，虽然过度配置的成本很容易衡量，但配置不足的成本更难衡量，但可能同样严重：被拒绝的用户不仅不会产生任何收入，而且可能永远不会回来；[例如，Friendster 相对于竞争对手 Facebook、MySpace 的受欢迎程度下降被认为部分是由于用户对缓慢的响应时间（长达 40 秒）不满意，单击查看图 2c](./img/elasticity.png)：用户将放弃配置不足的服务，直到峰值用户负载等于数据中心的可用容量，此时用户再次获得可接受的服务；另一个简化的例子，假设一个网站的用户分为两类：活跃用户（经常使用该网站的用户）和流失用户（由于性能不佳而放弃该网站的用户）。进一步假设，由于配置不足而获得较差服务的用户中有 10% 永远失去了机会（成为流失用户），即那些本来可以成为常客并获得更好体验的用户。该网站最初配置为处理预期峰值 400,000 个用户（每台服务器 1,000 个用户乘以 400 台服务器），但意外的正面报道在第一个小时内吸引了 500,000 名用户，在 100,000 名被拒之门外或获得不良服务的用户中，根据我们的假设，其中 10,000 人永久流失，剩下 390,000 个活跃用户。下一个小时将看到 250,000 个新的独立用户，前 10,000 名用户表现良好，但该网站仍然超负荷 240,000 个用户。这导致24000名用户流失，留下376000名永久用户。如果这种模式持续下去，经过lg(500000)或19小时后，新增用户数量将接近0，站点将达到稳定状态。显然，服务运营商在这19个小时内获得的稳定收入还不到400000名用户的收入，但这再次证明了利用率不足的论点，更不用说来自不满用户的坏名声了。
这种情况在实践中真的会发生吗？当我们通过Facebook提供服务时，它经历了需求激增，导致服务器数量在3天内从50台增加到3500台。即使每台服务器的平均利用率很低，也没有人能预见到资源需求会在3天内每12小时突然翻一番。峰值消退后，流量会下降到更低的水平。因此，在这个现实世界的例子中，扩展弹性使稳定状态的支出能够更紧密地匹配稳定状态的工作负荷。
### 云计算的 10 大障碍和机遇
[点击查看](./img/top-ten-obstacles-and-opportunities-for-cloud-computing.png)，前 3 个影响采用，接下来的 5 个影响增长，后 2 个是政策和业务障碍，每个障碍都与克服该障碍的机会配对，从产品开发到研究项目。
### The NIST Definition of Cloud Computing
### [NISTdef2011](https://www.nist.gov/publications/nist-definition-cloud-computing)
### NIST -National Institute of Standards and Technology Definition of Cloud Computing
cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources(such as networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal mgmt effort or service provider interaction; this cloud model is composed of 5 essential characteristics, 3 service models, and 4 deployment models; 
essential characteristics: #1 on-demand self-service -a consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with each service provider, #2 broad network access -capabilities are available over the network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms(such as mobile phones, tablets, laptops, and workstations), #3 resource pooling -provider's computing resources are pooled to serve multiple consumers using a multitenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand, there is a sense of location independence in that the customer generally has no control or knowledge over the exact location of the provided resources but may be able to specify location at a higher level of abstraction(such as country, state, or datacenter), examples of resources include storage, processing, mem, and network bdwidth, #4 rapid elasticity -capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward commensurate with demand, to the consumer, the capabilities available for provisioning often appear to be unlimited and can be appropriated in any quantity at any time, #5 measured service -cloud systems automatically control and optimize resource use by leveraging a metering capability(*typically this is done on a pay-as-you-go or charge-per-use basis) at some level of abstraction appropriate to type of service(such as storage, processing, bdwidth, and active user accounts), resources usage can be monitored, controlled, and reported, providing transparency for both provider and consumer of the utilized service; 
service models: #1 SaaS -the capability provided to the consumer it to use provider's application running on a cloud infra(*a cloud infra is collection of hw and sw that enables the 5 essential characteristics of cloud computing, the cloud infra can be viewed as containing both a physical layer and an abstraction layer, the physical layer consists of the hw resources that are necessary to support the cloud services being provided and typically includes server, storage, and network components, the abstraction layer consists of sw deployed across the physical layer which manifests the essential cloud chracteristics, conceptually the abstraction layer sits above the physical layer), the applications are accessible from various client services through either a thin client interface, such as a web browser(such as web-based email), or a program interface, consumer does not manage or control th eunderlying cloud infra including network, servers, os, storage, or even individual application capabilities, with the possible exception of limited user-specific application config settings; #2 PaaS -the capability provided to the consumer is to deploy onto the cloud infra consumer-created or acquired applications created using programming langs, libs, services, and tools supported by the provider(*this capability does not necessarily preclude use of compatible programming langs, libs, services, and tools from other sources), the consumer does not manage or control the underlying cloud infra including network, servers, os, or storage, but has control over the deployed applications and possibly config settings for the application-hosting environment; #3 IaaS -the capability provided to consumer is to provision processing, storage, networks, and other fundamental computing resources where consumer is able to deploy and run arbitrary sw, which can include os and applications, consumer does not manage or control the underlying cloud infra but has control over os, storage, and deployed applications, and possibly limited control of select networking components(such as host firewalls); 
deployment models: #1 private cloud -the cloud infra is provisioned for exclusive use by a single org comprising multiple consumers(such as business units), it may be owned, managed, and operated by the org, a third party, or some combinition of them, and it may exist on or off premises; #2 community cloud -the cloud infra is provisioned for exclusive use by a specific community of consumers from orgs that have shared concerns(such as mission, security requirements, policy, and compliance considerations), it may be owned, managed, and operated by one or more of the orgs in the community, a third party, or some combinition of them, and it may exist on or off premises; #3 public cloud -the cloud infra is provisioned for open use by the general public, it may be owned, managed, and operated by a business, academic, or government org, or some combinition of them, it exists on the premises of the cloud provider; #4 hybrid cloud -the cloud infra is a composition of two or more distinct cloud infras that remain unique entities, but are bound together by standardized or proprietary technology that enables data and application portability(such as cloud bursting for load balancing bt clouds).<br /><br />
云计算是一种模型，旨在实现对可配置计算资源（例如网络、服务器、存储、应用程序和服务）共享池的无处不在、便捷、按需网络访问，这些资源可以快速配置和发布，且管理工作量或服务提供商交互最少；该云模型由 5 个基本特征、3 个服务模型和 4 个部署模型组成；
基本特征：#1 按需自助服务 - 消费者可以根据需要自动单方面配置计算能力，例如服务器时间和网络存储，而无需与每个服务提供商进行人工交互，#2 广泛的网络访问 - 功能可通过网络获得，并通过标准机制访问，以促进异构瘦客户端或胖客户端平台（如移动电话、平板电脑、笔记本电脑和工作站）的使用，#3 资源池 - 提供商的计算资源池化使用多租户模型为多个消费者提供服务，不同的物理和虚拟资源根据消费者需求动态分配和重新分配，存在位置独立感，因为客户通常无法控制或了解所提供资源的确切位置，但可以在更高的抽象级别（如国家、州或数据中心）指定位置，资源示例包括存储、处理、内存和网络带宽，#4 快速弹性 - 可以弹性地配置和释放功能，在某些情况下可以自动释放，以根据需求快速向外和向内扩展，对于消费者而言，可用的功能资源配置通常看起来是无限的，并且可以随时分配任意数量的资源。#5 可计量服务 - 云系统通过利用计量功能自动控制和优化资源使用（*通常采用按需付费或按使用收费的方式），在适合服务类型（例如存储、处理、带宽和活跃用户账户）的某种抽象级别上，可以监控、控制和报告资源使用情况，从而为所用服务的提供者和消费者提供透明度；
服务模式：#1 SaaS-向消费者提供使用提供商在云基础设施上运行应用程序的能力（*云基础设施是硬件和软件的集合，它实现了云计算的 5 个基本特征，云基础设施可以看作包含物理层和抽象层，物理层由支持所提供的云服务所必需的硬件资源组成，通常包括服务器、存储和网络组件，抽象层由部署在物理层上的软件组成，体现了云的基本特征，从概念上讲，抽象层位于物理层之上），可以通过瘦客户端界面（如 Web 浏览器（如基于 Web 的电子邮件））或程序界面从各种客户端服务访问应用程序，消费者不管理或控制底层云基础设施，包括网络、服务器、操作系统、存储，甚至单个应用程序功能，可能的例外是有限的用户特定的应用程序配置设置； #2 PaaS - 向消费者提供的功能是将消费者使用提供商支持的编程语言、库、服务和工具创建或获取的应用程序部署到云基础设施上（*此功能不一定排除使用来自其他来源的兼容编程语言、库、服务和工具），消费者不管理或控制底层云基础设施，包括网络、服务器、操作系统或存储，但可以控制已部署的应用程序，并可能控制应用程序托管环境的配置设置；#3 IaaS - 向消费者提供的功能是配置处理、存储、网络和其他基本计算资源，消费者可以部署和运行任意软件，其中可能包括操作系统和应用程序，消费者不管理或控制底层云基础设施，但可以控制操作系统、存储和已部署的应用程序，并可能对某些网络组件（例如主机防火墙）进行有限的控制；
部署模型：#1 私有云 - 云基础设施由包含多个消费者（例如业务部门）的单个组织专供使用，它可能由该组织、第三方或它们的某种组合拥有、管理和运营，并且它可能存在于内部或外部；#2 社区云 - 云基础设施由具有共同关注点（例如使命、安全要求、政策和合规性考虑）的组织中的特定消费者社区专供使用，它可能由社区中的一个或多个组织、第三方或它们的某种组合拥有、管理和运营，并且它可能存在于内部或外部; #3 公有云 - 云基础设施由公众开放使用，可能由企业、学术机构、政府机构或以上机构组合拥有、管理和运营，并存在于云提供商的场所内；#4 混合云 - 云基础设施由两个或多个不同的云基础设施组成，这些云基础设施仍然是独立的实体，但通过标准化或专有技术绑定在一起，从而实现数据和应用程序的可移植性（例如，通过云爆发实现负载平衡）。
### Dynamically Scaling Applications in the Cloud
### [Vaquero11](https://dl.acm.org/doi/pdf/10.1145/1925861.1925869)
### Scalability is said to be one of the major advantages brought by the cloud paradigm and, more specifically, the one that makes it different to an advanced outsourcing solution, however, there are some important pending issues before making the dreamed automated scaling for applications come true.
cloud computing is commonly associated to offering of new mechanisms for infra porvisioning including: the illusion of a virtually infinite computing infra, the employment of advanced billing mechanisms allowing a pay-per-use model on shared multitenant resources, the simplified programming mechanisms(platform); among these features, those introduced by adding scalability and automated on-demand self-service are responsible for making any particular service something more than just an outsourced service with a prettier marketing face; 
as a result of its relevance, the wealth of systems dealing with cloud application scalability is slowing gaining weight in the available literature; automation is typically achieved by using a set of service provider-defined rules that govern how the service scales up or down to adapt to a var load, these rules are themselves composed of a condition, which when met triggers some action on infra or platform, the degree of automation, abstraction for the user(service provider) and customization of rules governing the service vary, some systems offer users chance of building rather simple conditions based on fixed infra or platform matrics(such as cpu, mem, etc.), while others employ server-level metrics(such as cost to benefit ration) and allow for more complex conditions(such as arithmetic and logic combinations of simple rules) to be included in the rules, regarding the subsequent actions launched when conditions are met, available efforts focus on service horizontal scaling(i.e. adding new server replicas and load balancers to distribute load among all available replicas) or vertical scaling(i.e. on-th-fly changing of assigned resources to an already running instance, such as letting more physical cpu to a running vm), unfortunately, the most common os do not support on-the-fly(without rebooting) changes on the available cpu or mem to support this vertical scaling; 
beyond mere server scalability, some other elements need to be taken into account that affect the overall application scaling potential, for example, LBs -load balancers need to support aggregation of new servers(typically, but not necessarily, in the form of new vms) in order to distribute load among several servers, Amazon already provides strategies for load balancing your replicated vms via its elastic load balancing capabilities, hence LBs and the algorithms that distribute load to different servers are essential elements in achieving application scalability in the cloud; 
having several servers and the mechanisms to distribute load among them is a definitive step towards scaling a cloud application, however, there is another element of the datacenter infra to be considered towards complete application scalability, network scalability is an often neglected element that should also be considered; in a consolidated datacenter scenario, several vms share the same network, potentially producing a huge increase in the equired bdwidth(potentially collapsing the network), hence it is necessary to extend infra clouds to other kinds of underlying resources beyond servers, LBs, and storage; cloud applications should be able to request not only virtual servers at multiple points in the network, but also bdwidth-provisioned network pipes and other network resources to interconnect them -NaaS; 
clouds that offer simple virtual hw infra such as vms and networks are usually denoted IaaS, vs., PaaS clouds provide sets of online libs and service for supporting application design, implementation, and maintenance, despite being somewhat less flexible than IaaS clouds, PaaS clouds are becoming important elements for building applications in a faster manner and many important it players scuh as Google, Microsoft, have developed new PaaS clouds systems such as Google AppEngine, Microsoft Azure, we discuss scalability in PaaS clouds at 2 different levels -container level and db level; 
[click for an overview of mechanisms handy to accomplish the goal of the whole application scalability](./img/overview-of-mechanisms-handy-to-accomplish-the-goal-of-the-whole-application-scalability.png).
### Server Scalability
most of the available IaaS clouds deal with single vm mgmt primitives(such as elements for adding/removing vms), lacking mechanisms for treating applications as a whole single entity and dealing with relations among different application components, for example, relationships bt vms are often not considered, ordered deployment of vms contaning sw for different tiers of an appliation is not automated(such as the db's ip is only known at deployment time hence the db needs to be deployed first in order to get its ip and configure the web server connecting to it), etc., application providers typically want to deal with their application only, being released from burden of dealing with (virtual) infra terms; 
**towards increased abstraction and automation :the elasticity controller:** such a fine-grained mgmt(vm-based) may come on handy for few services or domestic users, but it may become intractable with a big number of deployed applications composed of several vms each, the problem gets worse if application providers aim at having their application automatically scaled according to load, they would need to monitor every vm for every application in the cloud and make decisions on whether or not every vm should be scaled, its LB re-configured, its network resources resizes, etc., 2 different approaches are possible: increasing the abstraction level of provided APIs and/or advancing towards a higher automation degree; 
automated scaling features are being included by some vendors, but the rules and policies they allow to express still deal with individual vms only, one cannot easily relate the scaling of vms at tier-1 with its load balancers or the scaling of vms at tier-3; 
on the other hand, more abstract frameworks are proposed(they allow users to deal with applications as a whole rather than per individual vm) that also convey automation, unavoidably, any scalability mgmt system(or [elasticity controller, click to view](./img/elasticity-controller.png)) is bound to the underlying cloud API(the problem of discrete actuators), hence one essential task for any application-level elasticity controller is mapping user scaling policies from the appropriate level of abstraction for the user to actual mechanisms provided by IaaS clouds(depending on specific underlying API, in practice, most of the available cloud APIs expose vm-level mgmt capabilities, so that controller-actuator pairs are limited to adding/removing vms); the implementation of elasticity controller can be done in several different ways with regard to the provided abstraction level: (i)a per-tier controller, so that there is a need for coordination and sync among multiple controller-actuator pairs, treating each tier as an independent actuator with its own control policy can cause shifting of the perf bottleneck bt tiers, a tier can only release resources when an interlock is not being held by the other tiers, (ii)a single controller for the whole application(for all tiers), which let users specify how an application should scale in a global manner, for example, application provider could specify(based on its accurate knowledge of application) to scale the application logic tier whenever #incoming reqs at the web tier is beyond a given threshold; 
**expressing how and when to scale :feeding controller with rules and policies:** all the works above rely on traditional control theory in which several sensors feed a decision making module(elasticity controller) with data to operate on an actuator(cloud API), [click to view](./img/elasticity-controller.png), similarly, all the systems above answer the question on how to automate scalability(i.e. how to implement elasticity controller) in a similar manner either for vm-level or for application-level scalability mgmt systems; 
user-defined rules are the chosen mechanism(as opposed to preconfigured equations sets) to express the policies controlling scalability as shown below: *RULE: if CONDITION(s) then ACTION(s) CONDITION: (1..*) (metric.value MODIFIER) ACTION: (*) IaaS cloud-enabled actions(such as deploy new vm)*, a rule is composed of a series of conditions that when met trigger some action over the underlying cloud, every condition itself is composed of a series of metrics or events that may be present in the system, such as money available, authorization received, etc.(either provided by infra or obtained by application provider itself), which are used together with a modifier(either a comparison against a threshold, the presence of such events) to trigger a set of cloud-provided actions; 
if we focus on application-level scalability(rather than dealing with per vm scaling), a mathematical foumulation in which user just configures threshold for replicating storage servers so as to increase the cloud storage capability is proposed(*called proportional thresholding mechanism, which considers the fact that going from 1 to 2 machines can increase capacity by 100% but going from 100 to 101 machines increases capacity by no more than 1%), a leave full control for users to express their rules and use a rule engine as a controller is also proposed, the OVF -open virtual format is extended to define the application, its components, its contextualization needs, and the rules for scaling, this ways, service providers can generate a description of their application components, the way their application behaves with regard to scalability and the relevant metrics that will trigger action expressed in such rules; 
[as shown in fig1](./img/overview-of-mechanisms-handy-to-accomplish-the-goal-of-the-whole-application-scalability.png), in addition to dynamically adding more vm replicas, application behavior could also include many other aspects determining application scalability: adding load balancers to distribute load among vm replicas is also an important point, in an IaaS cloud, #vms balanced by a single LB can hugely increase, hence overloading the balancer(*although we recognize the importance of load balancing algorithms, the wealth of available literature on this matter places them well beyond of the scope of this work); 
LB scalability requires total time taken to forward each req to the corrsponding server to be negligible for small loads and should grow no faster than O(p)(here p being small loads of balanced vms) when the load is big and #balanced vms is large, however, although mechanisms to scale the load balancing tier would benefit any cloud system, they are missing in most current cloud implementations; 
Amazon already provides its elastic load balancer service aimed at delivering users with a single LB to distribute load among vms, however, Amazon does not provide mechanisms to scale LBs themselves; virtualization of load balancing mechanisms offers chance to define scalability rules by using previously presented systems, recently, a rule of thumb procedure to configure presenration-tier(web server) scalability is proposed :for cpu-intensive web applications, it is better to use a LB to split computation among many instances, for network intensive applications, it may be better to use a cpu-powerful standalone instance to maximize network throughput, yet, for even more network intensive applications, it may be necessary to use dns load balancing to get around a single instance bdwidth limitation, the question emerges whether dns-based load balancing can be scalable enough or not, practical experiences with large well-known services such as Google's search engine and recent experimental works on cloud settings seem to point in that direction, also, for many enterprise applications, hw LB is the most common approach.
### Scaling the Network
properly replicated/sized vms led us to think about LBs as a possible bottleneck, assuming this problem is also resolved takes us to think about the link that keeps application elements stuck together, even across different cloud infra services :the network, which is often overlooked in cloud computing; 
networking over virtualized resources is typically done in 2 different manners: ethernet virtualization and overlay networks and tcp/ip virtualization, these techniques are respectively focused in usage of VLAN -virtual local area network tags (L2) to separate traffic or public key infras to build L2/L3 overlays; 
separating users' traffic is not enough for reaching complete application scalability :the need to scale the very network arises in consolidated datacenters hosting several vms per physical machine, this scalability is often achieved by over-provisioning resources to suit this increased demand; this approach is expensive and induces network instability while infra is being updated, also, it is static and does not take into account that not all applications consume all required bdwidth during all time; improved mechanisms taking into account actual network usage are required; on the other hand, one could periodically measure actual network usage per application and let applications momentarily use other applications' allocated bdwidth; on the other hand, applications could request more bdwidth on demand over the same links, instantiate bdwidth-provisioned network resources together with the vms composing the service across several cloud providers is proposed, similar to the OVF extensions mentioned above, these authors employ NDL -network description lang -based ontologies for expressing required network characteristics; these abstract requirements are mapped to the concrete underlying network peculiarities(such as dynamically provisioned circuits vs. ip overlays), a complete architecture to perform this mapping has also been proposed, unfortunately, there is no known production-ready system that fully accomplishes need for dynamically managing the network in synchrony with vms provisioning; 
these techniques to increase utilization of the network by virtually slicing it have been dubbled as NaaS, this *a la cloud* network provision paradigm can be supported by flow control, distirbuted rate limiting, and network slicing techniques, by applying this mechanism the actual bdwidth can be dynamically allocated to applications on demand, which would benefit from a dynamic resource allocation scheme in which all users pay for the actual bdwidth consumption; to optimize network usage statistical multiplexing is used to compute the final bdwidth allocated to each application, statistical multiplexing helps to allocate more bdwidth to some applications while some others are not using it(most system admins usually provision on a worst-case scenario and never use all requested resources), this way, cloud provider can make a more rational use of its network resources, while still meeting applications' needs.
### Scaling the Platform
IaaS clouds are handy for application providers to control resources used by their systems, however, IaaS clouds demand application developers or system admins to install and configure all sw stack the application components need, in contrast, PaaS clouds offer a ready to use exec environment, along with convenient services, for applications, hence when using PaaS clouds developers can focus on programming their components rather than on setting up environment those components require, but as PaaS clouds can be subject to an extensive usage(many concurrent users calling to hosted applications), PaaS providers must be able to scale exec environment accordingly, in this section, we explore how scalability impacts on the 2 core layers of PaaS platforms: the container and the DBMS, as they are the backbone of any PaaS platform :combination of container+db is the chosen stack to implement many networked(such as internet) applications, which are the ones PaaS platforms are oriented to; here, container is the sw platform where users' components will be deployed and run, different PaaS clouds can be based on different platforms, for example, GAE and its open source counterpart AppEngine provide containers for servlets(part of J2EE specification) and python scripts, while Azure, Aneka offer an environment for .NET applications, each platform type can define different lifecycles, services, and APIs for the components it hosts; db provides data persistence support, the db storage service must address demand for data transactions support combined with big availability and scalability requirements, as it is explained later in this section, this can be addressed in different manners; [click for an overview of possible architecture of a PaaS platform](./img/PaaS.png), where both container and db layer achieve scalability through replication of container and dbms(horizontal scaling), this is the scenario this work focuses on, as it is the only one the authors deem feasible in clouds with certain scalability requirements, vs., applying vertical scaling by using more powerful hw would soon fail, as many clouds will typically face loads that one single machine cannot handle whatever its capacity; other services can be offered by a PaaS platform apart from container and db, which also will need to be scaled to adapt to demand, for example, Azure offers services for inter-component communication(bus) or access control, unfortunately, in this work it would only be possible to study scalability issues of all those services in a too superficial manner, instead, a most thorough analysis of the most important services, the container and the db, has been preferred; 
**container-level scalability:** at container level, a better scalability can be achieved by enabling multitenant containers(having the ability to run components belonging to different users), this imposes strong isolation requirements which maybe not all platforms can achieve by default, for example, the standard java platform is known to carry important security limitations that hinder implementation of safe multitenant environments, a more straightforward option is to run each user's components on non-shared containers, which is the approach taken by GAE; 
in both cases scaling is implemented by instantiating/releasig containers where several replicas of the developer components can be run(i.e. it is a form of horizontal scaling), this should automatically be done by the platform, so developers are not forced to constantly monitor their services' state; either automatic scaling IaaS systems(such as those mentioned in section service availability) can be used, or the platform itself can scale up and down the resources, the latter approach is the one used by both AppEngine, Aneka, AppEngine can run in any Xen based cloud, private(built for example with Eucalyptus) or public(EC2), however, it is not clear from if AppEngine supports transparent cloud federation so that a private PaaS cloud could deploy its containers in vms running in third party owned IaaS clouds to face sudden load peaks, in any case AppScale could apply Eucalyptus ability to be integrated with other clouds, on the other hand, Aneka supports cloud federation natively, so federated Aneka clouds can borrow resources among them, or a given Aneka cloud can use containers hosted in vms running in different IaaS clouds; 
automatic scaling of containers has several implications for developers regarding component design, if the PaaS platform can stop container replicas at any moment(such as due to low load), components could be designed to be as stateless as possible(as GAE recommends for the applications it hosts); in order to ease application dev, support for stateful components should be offered by the platform, in this case with stateful components, LB and container replica mgmt modules must be aware of state, this way, LBs know which container replicas hold session data to forward reqs accordingly, and container instances are not removed as long as they hold some session; 
if more than one container instance holds data from the same session(for better scalability and failure resilience), then some mechanism is necessary that allows to keep the different data replicas updated; transparent session data(such as shopping cart) replication, usually denoted soft state replication can be offered through systems such as Tempest toll or SSM; it is also possible to use distributed cache systems such as memcached for explicit data sharing among component replicas; each solution will have a different impact on component dev, roughly speaking, distributed caches work at application level i.e. they are explicitly accessed by hosted components code to store/retrieve info shared among component replicas, while soft state/session replication systems work in a transparent manner for application developer; 
**db scalability:** PaaS systems must expect very high reqs rates as they can host many applications that demand intense data traffice, 3 mechanisms can be used(and combined) to handle this: distributed caching, nosql dbs, and db clustering, here, (i)caching systems, such as memcached, are used to store intermediate data to speed up frequent data queries, a req for some data item will first check cache to see if the data is present, and will query db only if the data is not in cache(or it has expired), distributed caching systems provide the same cache across several nodes, which is useful in clustered applications, for exmaple, GAE offers [memcache](http://code.google.com/appengine/docs/java/memcache/) as a service for application developers; (ii)nosql refers to a wide family of storage solutions for structured data that are different from traditional relational fully sql-compliant dbs, nosql systems offer high scalability and availability, which seems a good fit in cloud environments with potentially many applications hosted under high demand, on the other hand, the replica mgmt mechanisms they use, provide less guarantees than traditional systems, usually, updates on data copies are not immediately done after each write op, they will be eventually done at some point in the future, this causes 3 systems to be unable to implement support for transparent and fully acid-complaint transactions, hence imposing some limitations on how transactions can be used by developers, besides, the fact that they only support(the equivalent to) a subset of sql can be a hurdle for some applications, such as BigTable used by GAE to provide its object oriented data storage service, HBase an open source implementation of BigTable used by AppEngine; (iii)finally, if fully relational and sql-complaint dbs are to be provided(as in the case of Microsoft's Azure), clusters can be built to provide better scalability, availability, and fault tolerance to typical dbms systems, unfortunately, 3 clusters must be built so several or all nodes contain a replica of each data item, which is known to compromise perf even for moderate loads when transactions are supported, present db replication systems from every major relational dbms have several limitations, and further research is needed to achieve desired perf, the major problem comes from the fact that transactions require protecting data involved while transaction lasts, often making that data unavailable to other transactions, the more transactions running at the same time, the more conflicts will be raised with the corrsponding impact on the application perf; 
yet, some db replication solutions exist that offer some degree of scalability, these can be part of the dbms itself(in-core) or be implemented as a mw layer bt db and application, most of the mw based solutions use a proxy driver used by client applications to access db, this proxy redirects reqs to the replication mw, which forwards them to db, the mw layer handles reqs and transforms then in db ops to ensure that all data copies are updated, also, it takes care of load balancing tasks, exmaples of such solution are C-JDBC, Middle-R, DBFarm; 
it can be concluded from this section that replication of dbs/components is the most important issue to consider when scaling a PaaS platform, [click for main replication ideas presented](./img/replication.png), at container level, the same component can be run on different container instances to achieve better scalability and failure tolerance, but then the platform should make available some mechanism for consistent data replication among components, at db level, copies of application data can be hosted in different dbms instances again for better scalability and failure tolerance, unfortunately, keeping consistency can lead to transaction conflicts; 
[click for summary of most relevant works related to holistic application scaling in cloud environments at the 3 different levels: server, network, and platform level, the most relevant features are highlighted and appropriate references are given](./img/summary-of-most-relevant-works-related-to-holistic-application-scaling-in-cloud-environments-at-the-three-different-levels.png).
### 可扩展性被认为是云范式带来的主要优势之一，更具体地说，是其有别于先进外包解决方案的优势。然而，在实现应用程序自动化扩展的梦想之前，仍存在一些重要的待解决的问题。
云计算通常与提供新的基础设施配置机制相关，包括：虚拟的无限计算基础设施、允许在共享多租户资源上采用按使用付费模式的高级计费机制的使用、简化的编程机制（平台）；在这些特性中，通过增加可扩展性和自动化按需自助服务而引入的特性，使得任何特定服务都不仅仅是一种带有漂亮营销幌子的外包服务；由于其相关性，大量处理云应用程序可扩展性的系统在现有文献中越来越受到重视；自动化通常是通过使用一组服务提供商定义的规则来实现的，这些规则控制着服务如何扩展或缩减以适应可变负载，这些规则本身由一个条件组成，当满足该条件时会触发基础设施或平台上的某些操作，自动化程度、用户（服务提供商）的抽象以及管理服务的规则的定制各不相同，一些系统为用户提供了基于固定基础设施或平台矩阵（例如 CPU、内存等）构建相当简单条件的机会，而其他系统则采用服务器级指标（例如成本效益比），并允许在规则中包含更复杂的条件（例如简单规则的算术和逻辑组合），关于满足条件时启动的后续操作，可用的努力集中在服务水平扩展（即添加新的服务器副本和负载平衡器以在所有可用副本之间分配负载）或垂直扩展（即动态更改分配给已经运行的实例的资源，例如让更多的物理 CPU 分配给正在运行的虚拟机），不幸的是，最常见的操作系统不支持即时（无需重启）更改可用的 CPU 或内存以支持这种垂直扩展；
除了单纯的服务器可扩展性之外，还需要考虑其他一些影响整体应用程序扩展潜力的因素，例如，负载均衡器 (LB) 需要支持新服务器的聚合（通常但不一定以新虚拟机的形式），以便在多个服务器之间分配负载。Amazon 已经通过其弹性负载均衡功能提供了对复制虚拟机进行负载均衡的策略，因此，LB 和将负载分配到不同服务器的算法是实现云中应用程序可扩展性的关键要素；
拥有多台服务器并在它们之间分配负载的机制是实现云应用程序扩展的关键一步，然而，要实现完整的应用程序可扩展性，还需要考虑数据中心基础设施的另一个要素，即网络可扩展性，这是一个经常被忽视的要素，也应该考虑；在整合数据中心场景中，多个虚拟机共享同一网络，这可能会导致所需带宽大幅增加（可能导致网络崩溃），因此有必要将基础云扩展到服务器、负载均衡器和存储之外的其他类型的底层资源；云应用程序不仅应该能够请求网络中多个点的虚拟服务器，还应该能够请求已配置带宽的网络管道和其他网络资源以将它们互连（即网络即服务 (NaaS)）；
提供简单虚拟硬件基础设施（例如虚拟机和网络）的云通常被称为 IaaS，而 PaaS 云则提供一系列在线库和服务，用于支持应用程序的设计、实现和维护。尽管 PaaS 云的灵活性略逊于 IaaS 云，但它正成为以更快方式构建应用程序的重要元素，许多重要的 IT 参与者，例如谷歌、微软，都开发了新的 PaaS 云系统，例如 Google AppEngine、Microsoft Azure。我们将从两个不同层面讨论 PaaS 云的可扩展性——容器级和数据库级；
[点击查看实现整个应用程序可扩展性目标的便捷机制概述](./img/overview-of-mechanisms-handy-to-accomplish-the-goal-of-the-whole-application-scalability.png)。
### 服务器可扩展性
大多数可用的 IaaS 云都处理单个虚拟机管理原语（例如用于添加/删除虚拟机的元素），缺乏将应用程序视为一个整体实体并处理不同应用程序组件之间关系的机制。例如，通常不考虑虚拟机之间的关系，包含应用程序不同层级软件的虚拟机的有序部署并非自动化（例如，数据库的 IP 仅在部署时才为人所知，因此需要先部署数据库才能获取其 IP 并配置连接到它的 Web 服务器），等等。应用程序提供商通常希望只处理其应用程序，从而摆脱处理（虚拟）基础设施方面的负担；
**提升抽象化和自动化程度：弹性控制器**：这种基于虚拟机的细粒度管理可能对少数服务或家庭用户有用，但对于部署大量由多个虚拟机组成的应用程序来说，可能会变得难以处理。如果应用程序提供商希望其应用程序根据负载自动扩展，问题将变得更加严重。他们需要监控云中每个应用程序的每个虚拟机，并决定是否应该扩展每个虚拟机、重新配置其负载均衡器、调整其网络资源大小等。有两种不同的方法可供选择：提高所提供 API 的抽象级别和/或提高自动化程度；
一些供应商正在提供自动扩展功能，但它们允许表达的规则和策略仍然只针对单个虚拟机，很难将 Tier-1 虚拟机的扩展与其负载均衡器或 Tier-3 虚拟机的扩展联系起来；
另一方面，提出了更抽象的框架（它们允许用户将应用程序作为一个整体而不是每个单独的虚拟机来处理），这些框架也传达了自动化，不可避免地，任何可扩展性管理系统（或[弹性控制器，单击查看](./img/elasticity-controller.png)）都绑定到底层云 API（离散执行器的问题），因此任何应用程序级弹性控制器的一项基本任务是将用户扩展策略从适合用户的抽象级别映射到 IaaS 云提供的实际机制（取决于特定的底层 API，实际上，大多数可用的云 API 都公开了虚拟机级管理功能，因此控制器-执行器对仅限于添加/删除虚拟机）；根据提供的抽象级别，弹性控制器的实现可以通过几种不同的方式完成：（i）每个层都有一个控制器，因此需要在多个控制器-执行器对之间进行协调和同步，将每个层视为具有其自身控制策略的独立执行器可能会导致性能瓶颈转移到各个层，只有当其他层未保持互锁时，某个层才能释放资源；（ii）整个应用程序（适用于所有层）使用一个控制器，这允许用户指定应用程序应如何以全局方式扩展，例如，应用程序提供商可以指定（基于其对应用程序的准确了解）在 Web 层的传入请求数量超过给定阈值时扩展应用程序逻辑层；
**表达如何以及何时扩展：向控制器提供规则和策略：** 以上所有工作都依赖于传统的控制理论，其中多个传感器向决策模块（弹性控制器）提供数据，以在执行器（云 API）上进行操作，[点击查看](./img/elasticity-controller.png)，同样，以上所有系统都以类似的方式回答了如何针对虚拟机级或应用程序级可扩展性管理系统实现自动化可扩展性（即如何实现弹性控制器）的问题；
用户定义规则是用于表达控制可扩展性的策略的机制（与预配置方程组相对），如下所示：*规则：如果条件满足，则操作条件：（1..*）（指标值修饰符）操作：（*）支持 IaaS 云的操作（例如部署新虚拟机）*，规则由一系列条件组成，当满足这些条件时，会触发底层云上的某些操作，每个条件本身都由系统中可能存在的一系列指标或事件组成，例如可用资金、已收到授权等（由基础设施提供或由应用程序提供商自己获取），它们与修饰符（与阈值的比较或此类事件的存在）一起使用，以触发一组云提供的操作；
如果我们关注应用程序级的可扩展性（而不是处理每个虚拟机的扩展），则提出了一个数学公式，其中用户只需配置复制存储服务器的阈值即可增加云存储能力（*称为比例阈值机制，它考虑从 1 台机器增加到 2 台机器可以将容量增加 100%，但从 100 台机器增加到 101 台机器只会增加 1% 的容量）；此外，还提出了一种让用户完全控制规则表达方式并使用规则引擎作为控制器的方法；OVF 开放虚拟格式被扩展以定义应用程序、其组件、其上下文需求以及扩展规则。这样，服务提供商可以生成其应用程序组件的描述、其应用程序在可扩展性方面的行为方式以及将触发此类规则中表达的操作的相关指标；
[如图 1 所示](./img/overview-of-mechanisms-handy-to-accomplish-the-goal-of-the-whole-application-scalability.png)，除了动态添加更多虚拟机副本之外，应用程序行为还可能包括许多其他决定应用程序可扩展性的方面：添加负载均衡器以在虚拟机副本之间分配负载也是一个重要方面。在 IaaS 云中，由单个负载均衡器平衡的虚拟机数量可能会大幅增加，从而导致平衡器过载（*尽管我们认识到负载均衡算法的重要性，但关于这方面的大量文献使其远远超出了本文的讨论范围）；
负载均衡器的可扩展性要求，在小负载情况下，将每个请求转发到相应服务器所需的总时间可以忽略不计；在负载较大且平衡虚拟机数量较大的情况下，时间增长速度不应超过 O(p)（其中 p 表示平衡虚拟机的小负载）。然而，尽管扩展负载均衡层的机制对任何云系统都有益，但在大多数当前的云实现中，这些机制都缺失；
亚马逊已经提供了弹性负载均衡器服务，旨在为用户提供单个负载均衡器以在虚拟机之间分配负载，但是，亚马逊并没有提供扩展负载均衡器本身的机制；负载均衡机制的虚拟化提供了利用先前提出的系统来定义可扩展性规则的机会。最近，提出了一种配置表示层（Web 服务器）可扩展性的经验法则：对于 CPU 密集型 Web 应用程序，最好使用负载均衡器在多个实例之间分配计算；对于网络密集型应用程序，最好使用 CPU 强大的独立实例来最大化网络吞吐量；然而，对于网络密集型应用程序，可能需要使用 DNS 负载均衡来绕过单个实例的带宽限制。一个问题是基于 DNS 的负载均衡是否具有足够的可扩展性。谷歌搜索引擎等大型知名服务的实际经验以及最近在云设置上的实验工作似乎都指向了这一方向。此外，对于许多企业应用程序来说，硬件负载均衡是最常见的方法。
### 扩展网络
适当复制/调整虚拟机大小让我们认为负载均衡器 (LB) 可能是一个瓶颈，假设这个问题也得到解决，我们就会思考将应用程序元素连接在一起的纽带，即使跨越不同的云基础设施服务也是如此：网络，这在云计算中经常被忽视；
虚拟化资源上的网络连接通常以两种不同的方式实现：以太网虚拟化和覆盖网络以及 TCP/IP 虚拟化，这些技术分别侧重于使用 VLAN（虚拟局域网标签 (L2)）来分离流量或使用公钥基础设施来构建 L2/L3 覆盖网络；
分离用户流量不足以实现完整的应用程序可扩展性：在整合数据中心中，每个物理机托管多个虚拟机时，需要扩展网络本身，这种可扩展性通常通过过度配置资源来满足不断增长的需求；这种方法成本高昂，并且在基础设施更新时会导致网络不稳定。此外，它是静态的，没有考虑到并非所有应用程序都始终消耗所有所需的带宽；需要改进的机制来考虑实际的网络使用情况；另一方面，可以定期测量每个应用程序的实际网络使用情况，并让应用程序暂时使用其他应用程序分配的带宽；另一方面，应用程序可以在相同的链路上按需请求更多带宽，并与跨多个云提供商组成服务的虚拟机一起实例化已配置带宽的网络资源。类似于上面提到的OVF扩展，这些作者使用基于NDL网络描述语言的本体来表达所需的网络特性；这些抽象需求被映射到具体的底层网络特性（例如动态配置的电路与IP覆盖），一个完整的架构来执行这种映射也已被提出，但遗憾的是，目前还没有已知的可用于生产的系统能够完全满足与虚拟机配置同步动态管理网络的需求；
这些技术可以提高网络的利用率通过虚拟切片，它被称为 NaaS，这种“云”式网络供应范例可以通过流量控制、分布式速率限制和网络切片技术支持，通过应用这种机制，可以根据需求将实际带宽动态分配给应用程序，这将受益于一种动态资源分配方案，在该方案中，所有用户都按实际带宽消耗付费；为了优化网络使用情况，使用统计复用计算分配给每个应用程序的最终带宽，统计复用有助于为某些应用程序分配更多带宽，而其他一些应用程序则不使用它（大多数系统管理员通常在最坏的情况下进行配置，并且从不使用所有请求的资源），这样，云提供商可以更合理地利用其网络资源，同时仍然满足应用程序的需求。
### 平台扩展
IaaS 云方便应用提供商控制其系统使用的资源，然而，IaaS 云要求应用开发人员或系统管理员安装和配置应用组件所需的所有软件栈。相比之下，PaaS 云为应用提供了即用型执行环境以及便捷的服务。因此，使用 PaaS 云时，开发人员可以专注于组件编程，而不是搭建组件所需的环境。但由于 PaaS 云的使用场景非常广泛（许多并发用户调用托管应用程序），PaaS 提供商必须能够相应地扩展执行环境。在本节中，我们将探讨可扩展性如何影响 PaaS 平台的两个核心层：容器和 DBMS，因为它们是任何 PaaS 平台的骨干：容器+数据库的组合是实现许多联网（例如互联网）应用程序的首选技术栈，而这些应用程序正是 PaaS 平台所面向的对象；这里的容器是用户组件部署和运行的软件平台，不同的PaaS云可以基于不同的平台，例如GAE和它的开源版本AppEngine为servlet（J2EE规范的一部分）和python脚本提供了容器，而Azure、Aneka为.NET应用程序提供了环境，每种平台类型可以为其承载的组件定义不同的生命周期、服务和API；db提供数据持久化支持，db存储服务必须满足数据事务支持的需求以及高可用性和可扩展性的要求，正如本节后面所解释的，这可以通过不同的方式来解决； [点击查看 PaaS 平台可能的架构概览](./img/PaaS.png)，其中容器层和数据库层都通过复制容器和数据库管理系统（水平扩展）实现可扩展性，这是本文关注的场景，因为这是作者认为在具有一定可扩展性要求的云中唯一可行的方案，相比之下，使用更强大的硬件进行垂直扩展很快就会失败，因为许多云通常会面临单台机器无论其容量如何都无法处理的负载；除了容器和数据库之外，PaaS 平台还可以提供其他服务，这些服务也需要进行扩展以适应需求，例如，Azure 提供组件间通信（总线）或访问控制服务，遗憾的是，本文只能以过于肤浅的方式研究所有这些服务的可扩展性问题，相反，我们更倾向于对最重要的服务——容器和数据库——进行最彻底的分析；
**容器级可扩展性**：在容器级别，通过启用多租户容器（能够运行属于不同用户的组件）可以实现更好的可扩展性，这带来了严格的隔离要求，并非所有平台都能默认实现。例如，众所周知，标准 Java 平台存在严重的安全限制，阻碍了安全多租户环境的实现。更直接的选择是在非共享容器中运行每个用户的组件，GAE 就采用了这种方法；
在这两种情况下，扩展都是通过实例化/释放容器来实现的，在容器中可以运行开发人员组件的多个副本（即，这是一种水平扩展），平台应该自动完成扩展，因此开发人员不必时刻监控其服务的状态；可以使用自动扩展的 IaaS 系统（例如在服务可用性部分提到的系统），或者平台本身可以扩展或缩减资源，后者是 AppEngine 和 Aneka 使用的方法。AppEngine 可以在任何基于 Xen 的云中运行，无论是私有云（例如使用 Eucalyptus 构建的）还是公共云（EC2）。然而，目前尚不清楚 AppEngine 是否支持透明云联合，以便私有 PaaS 云可以将其容器部署在第三方拥有的 IaaS 云中运行的虚拟机中，以应对任何 AppSclae 可以应用 Eucalyptus 的功能与其他云集成的情况下的突发负载峰值；另一方面，Aneka 原生支持云联合，因此联合的 Aneka 云可以在它们之间借用资源，或者某个 Aneka 云可以使用托管在不同 IaaS 云中的虚拟机中的容器；
容器的自动扩展对开发人员的组件设计有几个影响，如果 PaaS 平台可以随时停止容器副本（例如由于负载过低），则组件可以设计得尽可能无状态（就像 GAE 对其托管的应用程序所推荐的那样）；为了简化应用程序开发，平台应该提供对有状态组件的支持，在这种情况下，对于有状态组件，负载均衡器和容器副本管理模块必须了解状态，这样，负载均衡器就知道哪些容器副本保存了会话数据，以便相应地转发请求，并且只要容器实例还保存着会话，就不会被删除；
如果多个容器实例保存着来自同一会话的数据（为了更好的可扩展性和故障恢复能力），则需要某种机制来保持不同数据副本的更新；透明会话数据（例如购物车）复制，通常称为软状态复制，可以通过 Tempest toll 或 SSM 等系统提供；也可以使用分布式缓存系统（例如 memcached）在组件副本之间显式共享数据；每种解决方案对组件开发的影响各不相同，粗略地说，分布式缓存工作在应用程序级别，即托管组件代码可以显式访问它们，以存储/检索组件副本之间共享的信息，而软状态/会话复制系统则以对应用程序开发人员透明的方式工作；
**数据库可扩展性：** PaaS 系统必须预期非常高的请求率，因为它们可以托管许多需要密集数据流量的应用程序，可以使用 3 种机制（组合使用）来处理这个问题：分布式缓存、nosql dbs 和 db 集群，这里，（i）缓存系统，例如 memcached，用于存储中间数据以加速频繁的数据查询，对某些数据项的请求将首先检查缓存以查看数据是否存在，并且仅当数据不在缓存中（或已过期）时才会查询数据库，分布式缓存系统在多个节点上提供相同的缓存，这在集群应用程序中很有用，例如，GAE 为应用程序开发人员提供 [memcache](http://code.google.com/appengine/docs/java/memcache/) 作为服务； (ii)nosql 是指用于结构化数据的广泛存储解决方案系列，与完全符合 SQL 的传统关系数据库不同，nosql 系统提供高可扩展性和可用性，这似乎非常适合在高需求下托管许多应用程序的云环境，另一方面，它们使用的副本管理机制提供的保证比传统系统少，通常，数据副本的更新不会在每次写入操作后立即完成，它们最终将在未来的某个时间点完成，这导致 3 个系统无法实现对透明和完全酸性投诉事务的支持，因此对开发人员如何使用事务施加了一些限制，此外，它们仅支持（相当于）sql 子集的事实可能会对某些应用程序造成障碍，例如 GAE 用于提供其面向对象数据存储服务的 BigTable，HBase 是 AppEngine 使用的 BigTable 的开源实现； (iii) 最后，如果要提供完全关系型数据库和兼容 SQL 的数据库（例如 Microsoft Azure），可以构建集群，以便为典型的 DBMS 系统提供更好的可扩展性、可用性和容错能力。遗憾的是，必须构建 3 个集群，以便多个或所有节点都包含每个数据项的副本，这已知会在支持事务的情况下即使在中等负载下也会损害性能。目前，每个主要关系型 DBMS 的数据库复制系统都存在一些局限性，需要进一步研究才能达到所需的性能。主要问题在于，事务需要在事务持续期间保护所涉及的数据，这通常会使其他事务无法访问这些数据。同时运行的事务越多，冲突就会越多，从而对应用程序性能产生相应的影响；
然而，一些数据库复制解决方案提供了一定程度的可扩展性，它们可以是数据库管理系统本身的一部分（核心内），也可以作为数据库和应用程序之间的数据复制层来实现。大多数基于数据复制的解决方案使用代理驱动程序，客户端应用程序使用该代理驱动程序来访问数据库，此代理将请求重定向到复制数据复制层，再由复制数据复制层将其转发到数据库，数据复制层处理请求并在数据库操作中进行转换，以确保所有数据副本都得到更新，此外，它还负责负载平衡任务，此类解决方案的示例包括 C-JDBC、Middle-R 和 DBFarm；
由此部分可以得出结论，数据库/组件的复制是扩展 PaaS 平台时需要考虑的最重要的问题。[点击查看主要的复制思路](./img/replication.png)；在容器级别，相同的组件可以在不同的容器实例上运行，以实现更好的可扩展性和容错能力，但平台应该提供某种机制来在组件之间实现一致的数据复制。在数据库级别，应用程序数据的副本可以再次托管在不同的 DBMS 实例中，以实现更好的可扩展性和容错能力。遗憾的是，保持一致性可能会导致事务冲突；[点击查看与云环境中整体应用程序扩展相关的三个不同级别（服务器、网络和平台级别）的相关著作摘要，其中重点介绍了最相关的特性并给出了相应的参考文献](./img/summary-of-most-relevant-works-related-to-holistic-application-scaling-in-cloud-environments-at-the-three-different-levels.png)。
****
### Building a Carnegie Mellon Cloud and Openstack
### Virtual Infrastructure Management in Private and Hybrid Clouds
### [Sotomayor2009](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5233608)
### One of the many definitions of cloud is that of an IaaS system, which it infra is deployed in a provider's datacenter as vms, with IaaS clouds' growing popularity, tools and technologies are emerging that can transform an org's existing infra into a private or hybrid cloud; OpenNebula is an open source virtual infra manager that deploys virtualized servers on both a local pool of resources and external IaaS clouds; Haizea is a resource lease manager that can act as a scheduling backend for OpenNebula, providing features not found in other cloud sw or virtualization-based datacenter mgmt sw.
cloud computing is to use a cloud-inspired pun, a nebulously defined term, however, it was arguably first popularized in 2006 by Amazon's [EC2](www.amazon.com/ec2/) -elastic compute cloud, which started offering vms for $0.10 an hr using both a simple web interface and a programmer-friendly API, although not the first to propose a utility computing model, EC2 contributed to popularizing IaaS paradigm, which became closely tied to the notion of cloud computing; an IaaS cloud enables on-demand provisioning of computational resources in the form of vms deployed in a cloud provider's datacenter(such as Amazon's), minimizing or even eliminating associated capital costs for cloud consumers and letting those consumers add or remove capacity from their it infra to meet peak or fluctuating service demands while paying only for the actual capacity load; 
overtime, an ecosystem of providers, users, and technologies has coalesced around this IaaS cloud model, more IaaS cloud providers such as GoGrid, FlexiScale, ElasticHosts, etc., have emerged, and a strong number of companies base their it strategy on cloud-based resources, spending little or no capital to manage their own it infra([click for more examples](https://aws.amazon.com/solutions/case-studies/)); some providers such as Elastra, RightScale, focus on deploying and managing services on top of IaaS clouds, including web and db servers that benefit from such clouds' elastic capacity, and let their clients provision services directly instead of having no provision and set up infra themselves; other providers offer products that facilitate working with IaaS clouds, such as [rPath's rBuilder](www.rpath.org), which enables users to dynamically create sw environments to run on a cloud; 
although this ecosystem has evolved around public clouds -commercial cloud providers that offer a publicly accessible remote interface for creating and managing vm instances within their proprietary infra -internet is growing in open source cloud computing tools that let orgs build their own IaaS clouds using their internal infras, these private cloud deployments' primary aim is not to sell capacity over internet through publicly accessible interfaces but to give local users a flexible and agile private infra to run service wkloads within their administrative domains, private clouds can also support a hybrid cloud model by supplementing local infra with computing capacity from an external public cloud, private and hybrid clouds are not exclusive with being public clouds, a priavte or hybrid cloud can allow remote access to its resources over internet using remote interfaces, such as web services that Amazon EC2 uses, here, we look at 2 open source projects that facilitate mgmt of such private or hybrid cloud models.
### Virtual Infra Mgmt
to provide users with the same features found in commercial public clouds, private or hybrid cloud sw must: (i)provide a uniform and homogeneous view of virtualized resources, regardless of underlying virtualization platform(such as Xen, KVM -kernel-based vm, VMware, etc.), (ii)manage a vm's full life cycle, including setting up networks dynamically for groups of vms and managing their storage requirements, such as vm disk image deployment or on-the-fly sw environment creation, (iii)support configurable resource allocation policies to meet org's specific goals(high availability, server consolidation to minimize power usage, etc.), (iv)adapt to org's changing resource needs, including peaks in which local resource are insufficient, and changing resources including addition or failure of physical resources; 
hence a key component in private or hybrid clouds will be VI -virtual infra mgmt, the dynamic orchestration of vms that meets the requirements we have just outlined, here, we discuss vi mgmt's relevance not just for creating private or hybrid clouds but also within the emerging cloud ecosystem; our 2 open source projects, [OpenNebula](http://www.opennebula.org/), [Haizea](http://haizea.cs.uchicago.edu/), are complementary and can be used to manag vis in private or hybrid clouds, here, OpenNebula is a vi manager that orgs can use to deploy and manage vms, either individually or in groups that must be coscheduled on local resources or external public clouds, it automates vm setup(preparing disk images, setting up networking, etc.) regardless of underlying virtualization layer(Xen, KVM, VMware) or external cloud(EC2, ElasticHosts), Haizea is a resource lease manager that can act as a scheduling backend for OpenNebula, providing leasing capabilities not found in other cloud systems, such as ARs -advance researvations, and resource preemption, which are particularly relevant for private clouds.
### The Cloud Ecosystem
vi mgmt tools for datacenters have been around since before cloud computing became industry's new buzzword, several of these, such as the [Platform VM Orchestrator](https://www.platform.com/Products/platform-vm-orchestrator), [VMware vSphere](http://www.vmware.com/products/vsphere/), [Ovirt](http://ovirt.org/), meet many of the vi mgmt requirements we outlined earlier, providing features such as dynamic placement and vm mgmt on a pool of physical resources, automatic load balancing, server consolidation, and dynamic infra resizing and partitioning; although creating what we now call a private cloud was already possible with existing tools, these tools lack other features that are relevant for building IaaS clouds, such as public cloud-like interfaces, mechanisms for adding such interfaces easily, and the ability to deploy vms on external clouds; 
on the other hand, projects such as [Globus Nimbus](http://workspace.globus.org), [Eucalyptus](www.eucalyptus.com), which we term cloud toolkits, can help transform existing infra into an IaaS cloud with cloud-like interfaces, here, Eucalyptus is compatible with Amazon EC2 interface and is designed to support additional client-side interfaces, Globus Nimbus exposes EC2 and WSRF -web services resource framework interfaces and offers self-configuring virtual cluster support, however, although these tools are fully functional with respect to providing cloud-like interfaces and higher-level functionality for security, contextualization, and vm disk image mgmt, their vi mgmt capabilities are limited and lack features of solutions that specialize in vi mgmt; hence [an ecosystem of cloud tools is starting to form, click to view](./img/ecosystem-of-cloud-tools.png), in which cloud toolkits attempt to span both cloud mgmt and vi mgmt but, by focusing on the former, do not deliver the same functionality as sw written specifically for vi mgmt; although integrating cloud mgmt solutions with existing vi managers would seem like obvious solution, this is complicated by lack of open and standard interfaces bt the 2 layers, and lack of certain key features in existing vi managers, hence our aim is to produce a vi mgmt solution with a flexible and open architecture that orgs can employ to build private or hybrid clouds; with this goal in mind, we started developing OpenNebula and continue to enhance it as part of [EU's Reservoir porject](http://www.reservoir-fp7.eu/), which aims to develop open source technologies to enable deployment and mgmt of complex it services across different administrative domains; OpenNebula provides similar functionality to that found in existing vi managers but also aims to over those solutions' shortcomings, namely: (i)the inability to scale to external clouds, (ii)monolithic and closed architectures that are hard to extend or interface with other sw, not allowing seamless integration with existing storage and network mgmt solutions deployed in datacenters, (iii)a limited choice of preconfigured placement policies(first fit, round robin, etc.), (iv)a lack of support for scheduling, deploying, and configuring groups of vms(such as a group of vms representing a cluster where all the nodes either deploy entirely or do not deploy at all and where some vms' config depends on others' such as the head-worker replationship in compute clusters), [click for a more detailed comparison bt OpenNebula and several well-known vi managers, including cloud toolkits that perform vi mgmt](./img/detailed-comparison-between-OpenNebula-and-several-well-known-virtual-infrastructure-managers.png); 
a key feature of OpenNebula's architecture, which we describe more in the next section, is its highly modular design, which facilitates integration with any virtualization platform and third-party component in the cloud ecosystem, such as cloud toolkits, virtual image managers, service managers, and vm schedulers, for example, it specifies all actions pretaining to setting up a vm disk image(transferring the image, installing sw on it, etc.) in terms of well-defined hooks, although OpenNebula includes a default transfer manager that uses these hooks, it can also leverage contextualizers just by writing code that interfaces bt hooks and thir-party sw; 
Haizea which we developed independently from OpenNebula, was the first to leverage such an architecture in a way that was beneficial to both projects, Haizea originally cloud simulate vm scheduling only for research purposes, but we modified it to act as a drop-in replacement for OpenNebula's default scheduler, with few changes required in Haizea code and none in the OpenNebula code, by working together, OpenNebula could offer resource leases as a fundamental provisioning abstraction, and Haizea could operate with real hw through OpenNebula; 
in fact, integraing OpenNebula and Haizea provides the only vi mgmt solution offering advance reservation of capacity, [as tabel1 shows](./img/detailed-comparison-between-OpenNebula-and-several-well-known-virtual-infrastructure-managers.png), other vi managers use immediate provisioning, or best-effort provisioning, which we discuss in more detail later, however, private clouds -specifically those with limited resources in which not all reqs are satisfiable immediately owing to lack of resources -stand to benefit from more sophisticated vm placement strategies supporting queues, priorities, and ars; additionally, service provisioning clouds, such as the one being developed in Reservoir project, have requirements that are insupportable with only an immediate provisioning model -for example, they need capacity reservations at specific times to meet SLAs -service-level agreements, or peak capacity requirements.
### The OpenNebula Architecture
[click to view](.//img/opennebula.png), encompasses several components specialized in different aspects of vi mgmt; to control a vm's life cycle, OpenNebula core orchestrates 3 different mgmt areas including: image and storage technologies(i.e. virtual appliance tools or distributed file systems) for preparing disk images for vms, network fabric(such as DHCP -dynamic host config protocol servers, firewalls, or switches) for providing vms with a virtual network environment, and underlying hypervisors for creating and controlling vms; the core performs specific storage, network, or virtualization ops through pluggable drivers, hence OpenNebula is not tied to any specific environment, providing a uniform mgmt layer regardless of underlying infra; 
besides managing individual vm's life cycle, we also designed the core to support services deployment, such services typically include a set of interrelated components(such as a web server and db backend) requiring several vms, hence we can treat a group of related vms as a first-class entity in OpenNebula; besides managing vms as a unit, the core also handles delivery of context info(such as web server's ip addr, digital certificates, and sw licenses) to the vms; 
a separate scheduler component makes vm placement decisions, more specifically, the scheduler has access to info on all reqs OpenNebula receives and, based on these reqs, keeps track of current and future allocations, creating and updating a resource schedule and sending appropriate deployment commands to OpenNebula core; OpenNebula default scheduler provides a rank scheduling policy that places vms on physical resources according to a ranking algorithm that the administrator can configure, it relies on realtime data from both the running vms and available physical resources; 
OpenNebula offers mgmt interfaces to integrate core's functionality within other datacenter mgmt tools, such as accounting or monitoring frameworks, to this end, OpenNebula implements the [libvirt API](http://libvirt.org/), an open interface for vm mgmt, as well as a CLI, adiitionally, a subset of this functionality is exposed to external users through a cloud interface; 
finally, OpenNebula can support a hybrid cloud model by using cloud drivers to interface with external clouds, this lets orgs supplement local infra with computing capacity from a public cloud to meet peak demands, better serve their access reqs(such as by moving the service closer to user), or implement high availablity strategies; OpenNebula currently includes an EC2 driver, which can submit reqs to EC2, Eucalyptus, as well as an ElasticHosts driver.
### Haizea Lease Manager
Haizea is an open source lease manager and can act as a vm scheduler for OpenNebula or be used on its own as a simulator to evaluate different scheduling strategies' perf over time; the fundamental resource provisioning abstraction in Haizea is the lease, intuitively, a lease is a form of contract in which one party agrees to provide a set of resources to another, when a user wants to request computational resources from Haizea, it does so in the form of a lease, leases are then implemented as vms managed by OpenNebula, the lease terms Haizea supports include hw resources, sw environments, and the period during which hw and sw resources must be avialable; currently, Haizea supports ar leases which resources must be available at a specific time; best-effort leases, in which resources are provisioned as soon as possible, and reqs are placed in a queue, if necessary; immediate leases, in which resources are provisioned when requested or not at all; 
to satisfy use cases where resources must be guaranteed to be available at certain times, various researchers have studied advance reservation of computational resources in the context of parallel computing; in the absence of suspension/resumption capabilities, this approach produces resource underutilization due to need to vacate resources before an ar starts; by using vms to implement leases, an org can support ars more efficiently through resource preemption, suspending vms of lower-priority leases before a reservation starts, resuming them after reservation ends, and potentially migrating them to other available nodes or even other clouds, we can do this without having to make the applications inside the vm aware that they are going to be suspended, resumed, or even migrated, however, using vms introduces runtime overhead, which poses additional scheduling challenges, as does the preparation overhead of deploying the vm disk images that the lease needs, such overheads can noticeably affect perf if they are not adequately managed; Haizea's approach is to separately schedule preparation overhead rather than assuming it should just be deducted from a user's allocation, however, this is complicated when Haizea must support multiple lease types with conflicting requirements that it has to reconcile -for example, transfers for a lease starting at 2pm could require delaying transfers for best-effort leases, resulting in longer wait times, Haizea uses several optimizations, such as reusing disk images across leases, to minimize preparation overhead's impact, similarly, Haizea also schedules runtime overhead of suspending, resuming, and migrating vms; 
Haizea bases its scheduling on a resource slot table that represents all physical nodes it manages over time, it schedules best-effort leases using a first-come-first-serve queue with backfilling(a common optimization in queue-based systems), whereas ar leases use a greedy algorithm to select physical resources that minimize #preemptions, although the resource selection algorithm is currently hardcoded, future versions will include a policy decision module to let developers specify their own resource selection policies(such as policies to prioritize leases based on user, group, project, etc.), this policy decision module will also specify the conditions under which Haizea should accept or reject a lease.
### 云的众多定义之一是 IaaS 系统，其基础设施以虚拟机的形式部署在提供商的数据中心中。随着 IaaS 云的日益普及，可以将组织现有的基础设施转变为私有云或混合云的工具和技术正在涌现；OpenNebula 是一个开源虚拟基础设施管理器，可在本地资源池和外部 IaaS 云上部署虚拟化服务器；Haizea 是一个资源租赁管理器，可以充当 OpenNebula 的调度后端，提供其他云软件或基于虚拟化的数据中心管理软件所没有的功能。
云计算是一个受云启发的双关语，一个定义模糊的术语，然而，它可以说是在 2006 年由亚马逊的 [EC2](www.amazon.com/ec2/) 弹性计算云首次推广的，它开始以每小时 0.10 美元的价格提供虚拟机，使用简单的 Web 界面和程序员友好的 API。虽然 EC2 不是第一个提出效用计算模型的，但它为 IaaS 范式的普及做出了贡献，该范式与云计算的概念紧密相关；IaaS 云支持以部署在云提供商数据中心（例如亚马逊的数据中心）中的虚拟机的形式按需配置计算资源，从而最大限度地减少甚至消除云消费者的相关资本成本，并允许这些消费者从其 IT 基础设施中添加或删除容量，以满足峰值或波动的服务需求，同时只需为实际的容量负载付费；
随着时间的推移，围绕 IaaS 云模型，一个由提供商、用户和技术组成的生态系统逐渐形成，越来越多的 IaaS 云提供商（例如 GoGrid、FlexiScale、ElasticHosts 等）应运而生，大量公司将其 IT 战略建立在云资源之上，几乎不投入任何资金来管理自己的 IT 基础设施 ([点击查看更多示例](https://aws.amazon.com/solutions/case-studies/))；一些提供商（例如 Elastra 和 RightScale）专注于在 IaaS 云之上部署和管理服务，包括受益于此类云的弹性容量的 Web 和数据库服务器，并允许其客户直接提供服务，而无需自行配置和设置基础设施；其他提供商提供方便使用 IaaS 云的产品，例如 [rPath 的 rBuilder](www.rpath.org)，它使用户能够动态创建在云上运行的软件环境；
尽管这个生态系统是围绕公共云发展起来的——商业云提供商提供可公开访问的远程接口，用于在其专有基础设施内创建和管理虚拟机实例——但开源云计算工具在互联网上不断发展，这些工具允许组织使用其内部基础设施构建自己的 IaaS 云，这些私有云部署的主要目的不是通过可公开访问的接口在互联网上销售容量，而是为本地用户提供灵活、敏捷的私有基础设施，以便在其管理域内运行服务负载，私有云还可以通过使用来自外部公共云的计算能力补充本地基础设施来支持混合云模型，私有云和混合云并不排斥公共云，私有云或混合云可以允许使用远程接口通过互联网远程访问其资源，例如 Amazon EC2 使用的 Web 服务，在这里，我们来看看两个有助于管理此类私有云或混合云模型的开源项目。
### 虚拟基础设施管理
为了向用户提供与商业公有云相同的功能，私有云或混合云软件必须：(i) 提供统一且同构的虚拟化资源视图，无论底层虚拟化平台是什么（例如 Xen、基于 KVM 内核的虚拟机、VMware 等）；(ii) 管理虚拟机的整个生命周期，包括为虚拟机组动态设置网络并管理其存储需求，例如虚拟机磁盘映像部署或动态创建软件环境；(iii) 支持可配置的资源分配策略，以满足组织的特定目标（高可用性、服务器整合以最大限度地降低功耗等）；(iv) 适应组织不断变化的资源需求，包括本地资源不足的峰值以及资源变化（例如物理资源的添加或故障）；
因此，私有云或混合云中的一个关键组件将是 VI - 虚拟基础设施管理，即满足我们刚才概述的要求的虚拟机的动态编排，在这里，我们讨论 VI 管理的相关性不仅在于创建私有云或混合云，还在于其在新兴云生态系统中的应用；我们的两个开源项目 [OpenNebula](http://www.opennebula.org/) 和 [Haizea](http://haizea.cs.uchicago.edu/) 是互补的，可用于管理私有云或混合云中的 VI。在这里，OpenNebula 是一个 VI 管理器，组织可以使用它来部署和管理虚拟机，无论是单独部署还是成组部署，这些虚拟机必须在本地资源或外部公共云上同步调度，它可以自动设置虚拟机（准备磁盘映像、设置网络等），而不管底层虚拟层（Xen, KVM, VMware）或外部云（EC2, ElasticHosts）如何，Haizea 是一个资源租赁管理器，可以作为 OpenNebula 的调度后端，提供其他云系统所不具备的租赁功能，例如 ARs（提前预留）和资源抢占，这些功能对于私有云尤为重要。
### 云生态系统
早在云计算成为业界新热词之前，数据中心的虚拟机管理工具就已经存在，其中一些工具，例如 [Platform VM Orchestrator](https://www.platform.com/Products/platform-vm-orchestrator)、[VMware vSphere](http://www.vmware.com/products/vsphere/)、[Ovirt](http://ovirt.org/)，满足了我们之前概述的许多虚拟机管理需求，提供了诸如在物理资源池上动态放置和虚拟机管理、自动负载平衡、服务器整合以及动态基础设施大小调整和分区等功能；虽然使用现有工具已经可以创建我们现在所说的私有云，但这些工具缺乏构建 IaaS 云所需的其他功能，例如类似公有云的接口、轻松添加此类接口的机制以及在外部云上部署虚拟机的能力；
另一方面，[Globus Nimbus](http://workspace.globus.org)、[Eucalyptus](www.eucalyptus.com) 等项目（我们称之为云工具包）可以帮助将现有的基础设施转变为具有类云接口的 IaaS 云。其中，Eucalyptus 与 Amazon EC2 接口兼容，并旨在支持其他客户端接口。Globus Nimbus 公开了 EC2 和 WSRF -Web 服务资源框架接口，并提供自配置虚拟集群支持。然而，尽管这些工具在提供类云接口和更高级别的安全性、情境化和虚拟机磁盘映像管理功能方面功能齐全，但它们的 vi 管理功能有限，缺乏专门从事 vi 管理的解决方案的功能；因此 [云工具生态系统正在开始形成，点击查看](./img/ecosystem-of-cloud-tools.png)，其中云工具包试图涵盖云管理和 vi 管理，但由于专注于前者，无法提供与专门为 vi 管理编写的软件相同的功能；尽管将云管理解决方案与现有的 vi 管理器集成似乎是显而易见的解决方案，但由于这两层之间缺乏开放和标准的接口，以及现有 vi 管理器缺乏某些关键功能，这个问题变得复杂，因此我们的目标是提供一个具有灵活开放架构的 vi 管理解决方案，组织可以使用它来构建私有云或混合云；带着这个目标，我们开始开发 OpenNebula，并继续将其作为 [欧盟 Reservoir 项目](http://www.reservoir-fp7.eu/) 的一部分进行增强，该项目旨在开发开源技术，以实现跨不同管理域部署和管理复杂的 IT 服务； OpenNebula 提供与现有 vi 管理器类似的功能，但也旨在克服这些解决方案的缺点，即：（i）无法扩展到外部云，（ii）单片和封闭的架构，难以扩展或与其他软件接口，无法与数据中心部署的现有存储和网络管理解决方案无缝集成，（iii）预配置放置策略的选择有限（首次适应、循环等），（iv）缺乏对虚拟机组（例如，一组虚拟机代表一个集群，其中所有节点要么完全部署，要么根本不部署，并且某些虚拟机的配置依赖于其他虚拟机的配置，例如计算集群中的主从关系）的支持，[单击查看 OpenNebula 与一些知名 vi 管理器（包括执行 vi 的云工具包）的更详细比较mgmt](./img/detailed-comparison-between-OpenNebula-and-several-well-known-virtual-infrastructure-managers.png);
OpenNebula 架构的一个关键特性（我们将在下一节中详细介绍）是其高度模块化的设计，这有助于与云生态系统中的任何虚拟化平台和第三方组件集成，例如云工具包、虚拟映像管理器、服务管理器和虚拟机调度程序。例如，它通过明确定义的钩子指定与设置虚拟机磁盘映像（传输映像、在其上安装软件等）相关的所有操作。尽管 OpenNebula 包含一个使用这些钩子的默认传输管理器，但它也可以通过编写用于连接钩子和第三方软件的代码来利用上下文工具；
Haizea 是我们独立于 OpenNebula 开发的，它是第一个利用这种架构并同时对两个项目都有益的项目。Haizea 最初仅用于研究目的的云模拟虚拟机调度，但我们对其进行了修改，使其成为 OpenNebula 默认调度程序的直接替代品，Haizea 代码几乎不需要更改，OpenNebula 代码也无需更改，通过合作，OpenNebula 可以提供资源租赁作为基本的配置抽象，而 Haizea 可以通过 OpenNebula 使用真实的硬件；事实上，OpenNebula 和 Haizea 的集成提供了唯一提供提前预留容量的虚拟基础设施管理解决方案，[如表 1 所示](./img/detailed-comparison-between-OpenNebula-and-several-well-known-virtual-infrastructure-managers.png)，其他虚拟基础设施管理器使用立即配置或尽力而为的配置，我们稍后会详细讨论，然而，私有云——特别是那些资源有限、由于缺乏资源而并非所有需求都能立即满足的私有云——将受益于支持队列、优先级和 ARS 的更复杂的虚拟机放置策略；此外，服务供应云（例如 Reservoir 项目中正在开发的云）具有仅靠即时供应模型无法支持的需求 - 例如，它们需要在特定时间预留容量以满足 SLA（服务级别协议）或峰值容量需求。
### OpenNebula 架构
[点击查看](.//img/opennebula.png) 包含多个专门负责虚拟机管理不同方面的组件；为了控制虚拟机的生命周期，OpenNebula 核心协调了 3 个不同的管理领域，包括：用于为虚拟机准备磁盘映像的映像和存储技术（即虚拟设备工具或分布式文件系统）、用于为虚拟机提供虚拟网络环境的网络结构（例如 DHCP - 动态主机配置协议服务器、防火墙或交换机），以及用于创建和控制虚拟机的底层虚拟机管理程序；核心通过可插拔驱动程序执行特定的存储、网络或虚拟化操作，因此 OpenNebula 不依赖于任何特定环境，无论底层基础设施如何，都能提供统一的管理层；
除了管理单个虚拟机的生命周期外，我们还设计了核心来支持服务部​​署。此类服务通常包含一组相互关联的组件（例如 Web 服务器和数据库后端），这些组件需要多个虚拟机，因此我们可以将一组相关的虚拟机视为 OpenNebula 中的一级实体；除了将虚拟机作为一个单元进行管理外，核心还负责向虚拟机传递上下文信息（例如 Web 服务器的 IP 地址、数字证书和软件许可证）；
一个单独的调度器组件负责虚拟机的放置决策，更具体地说，调度器可以访问 OpenNebula 收到的所有请求信息，并基于这些请求跟踪当前和未来的分配情况，创建和更新资源调度表，并向 OpenNebula 核心发送适当的部署命令；OpenNebula 默认调度器提供了一种排序调度策略，该策略根据管理员可以配置的排序算法将虚拟机放置在物理资源上，该策略依赖于正在运行的虚拟机和可用物理资源的实时数据；
OpenNebula 提供管理接口，以便将核心功能集成到其他数据中心管理工具（例如会计或监控框架）中。为此，OpenNebula 实现了 [libvirt API](http://libvirt.org/)，这是一个用于虚拟机管理的开放接口，以及命令行界面 (CLI)。此外，该功能的一部分通过云接口向外部用户开放；
最后，OpenNebula 可以通过使用云驱动程序与外部云进行交互来支持混合云模型。这使得组织可以使用来自公共云的计算能力来补充本地基础设施，以满足峰值需求，更好地满足其访问需求（例如，通过将服务移近用户），或实施高可用性策略；OpenNebula 目前包含一个 EC2 驱动程序，它可以将请求提交到 EC2、Eucalyptus 以及 ElasticHosts 驱动程序。
### Haizea 租赁管理器
Haizea 是一个开源租赁管理器，可以作为 OpenNebula 的虚拟机调度器，也可以单独用作模拟器，评估不同调度策略随时间推移的性能；Haizea 中资源配置的基本抽象是租赁。直观地说，租赁是一种合同形式，一方同意向另一方提供一组资源。当用户想要向 Haizea 请求计算资源时，它会以租赁的形式进行请求，然后租赁将实现为由 OpenNebula 管理的虚拟机。Haizea 支持的租赁条款包括硬件资源、软件环境以及硬件和软件资源必须可用的时间段；目前，Haizea 支持三种租赁模式，即资源必须在特定时间可用；尽力而为租赁模式，即尽快提供资源，并在必要时将请求放入队列；立即租赁模式，即在请求时提供资源，否则根本不提供资源；
为了满足必须保证资源在特定时间可用的用例，许多研究人员研究了并行计算中计算资源的提前预留；在没有暂停/恢复能力的情况下，这种方法由于需要在 ar 启动之前腾出资源，导致资源利用不足；通过使用 vm 实施租约，组织可以通过资源抢占更有效地支持 ars，在预留开始之前暂停低优先级租约的 vm，在预留结束后恢复它们，并可能将它们迁移到其他可用节点甚至其他云，我们可以做到这一点，而不必让 vm 内的应用程序知道它们将被暂停、恢复甚至迁移，但是，使用 vm 会引入运行时开销，这会带来额外的调度挑战，部署租约所需的 vm 磁盘映像的准备开销也会带来额外的挑战，如果管理不善，这些开销会明显影响性能； Haizea 的方法是单独安排准备开销，而不是假设它应该从用户的分配中扣除，但是，当 Haizea 必须支持多种租约类型且这些租约具有相互冲突的要求时，这种方法会变得复杂 - 例如，从下午 2 点开始的租约传输可能需要延迟尽力而为租约的传输，从而导致更长的等待时间，Haizea 使用多种优化措施，例如在租约之间重用磁盘映像，以最大限度地减少准备开销的影响，同样，Haizea 还会安排暂停、恢复和迁移虚拟机的运行时开销；
Haizea 的调度以资源槽表为基础，该表代表了它在一段时间内管理的所有物理节点，它使用先到先服务队列和回填（基于队列的系统中常见的优化）来调度尽力而为的租约，而 ar 租约使用贪婪算法来选择最小化#preemptions 的物理资源，虽然资源选择算法目前是硬编码的，但未来版本将包括一个策略决策模块，让开发人员指定自己的资源选择策略（例如根据用户、组、项目等对租约进行优先排序的策略），该策略决策模块还将指定 Haizea 应接受或拒绝租约的条件。
****
### Encapsulating Computation (Intel Labs, guest)
### Xen and the Art of Virtualization
### [Barham03](https://dl.acm.org/doi/pdf/10.1145/1165389.945462)
### [Project Page](http://www.cl.cam.ac.uk/netos/xen)
numerous systems have been designed which use virtualization to subdivide ample resources of a modern computer, some require specialized hw, or cannot support commodity os, some target 100% binary compatibility at the expense of perf, others sacrifice security of functionality for speed, few offer resource isolation or perf guarantees, most provide only best-effort provisioning, risking denial of service; 
we present Xen, an x86 vm monitor which allows multiple commodity os to share conventional hw in a safe and resource managed fashion, but without sacrificing their perf or functionality, this is achieved by providing an idealized vm abstraction to which os such as linux, bsd, windows xp, can be ported with minimal effort; 
our design is targeted at hosting up to 100 vm instances simultaneously on a modern server, the virtualization approach taken by Xen is extremely efficient -we allow os such as linux, windows xp, to be hosted simultaneously for a negligible perf overhead -at most a few percent compared with the unvirtualized case, we considerably outperform competing commercial and freely available solutions in a range of microbenchmarks and system-wide tests. 
modern computers are sufficiently powerful to use virtualization to present illusion of many smaller vms, each running a separate os instance, this has lead to a resurgence of interest in vm technology, here we present Xen, a high perf resource-managed VMM -vm monitor which enables applications such as server consolidation, co-located hosting facilities, distributed web services, secure computing platforms, and application mobility; 
successful partitioning of a machine to support concurrent exec of multiple os poses several challenges including: (i)vms must be isolated from one another -it is not acceptable for exec of one to adversely affect perf of another, this is particularly true when vms are owned by mutually untrusting users, (ii)it is necessary to support a variety of different os to accomodate the heterogeneity of popular applications, (iii)the perf overhead introduced by virtualization should be small; 
Xen hosts commodity os, albeit with some source modifications, the prototype described and evaluated here can support multiple concurrent instances of our XenoLinux guest os, each instance exports an application binary interface identical to a non-virtualized linux2.4, our port of windows xp to Xen is not yet complete but is capable of running simple user-space processes, work is also progressing in porting NetBSD; 
Xen enables users to dynamically instantiate an os to execute whether they desire, in the XenoServer project we are deploying Xen on standard server hw at economically strategic locations within ISPs or at internet exchanges, we perform admission control when starting new vms and expect each vm to pay in some fashion for resources it requires, we discuss our ideas and approach in this direction elsewhere, here we focus on vmm; 
there are a number of ways to build a system to host multiple applications and servers on a shared machine, perhaps the simplest is to deploy one or more hosts running a standard os such as linux, windows, and then to allow users to install files and start processes -protection bt applications being provided by conventional os techniques, experience shows that system administration can quickly become a time-consuming task due to complex config interactions bt supposedly disjoint applications; 
more importantly, such systems do not adequately support perf isolation; the scheduling policy, mem demand, network traffic, and disk accesses of one process impact perf of others, this may be acceptable when there is adequate provisioning and a closed user group(such as in the case of computational grids, or the experimental PlanetLab platform), but not when resources are oversubscribed, or users uncooperative; one way to address this problem is to retrofit support for perf isolation to os, this has been demonstrated to a greater or lesser degree with resource containers, linux/rk, qlinux, and silk, one difficulty with such approaches is ensuring that all resource usage is accounted to the correct process -consider, for example, the complext interactions bt applications due to buffer cache or page replacement algorithms, this is effectively the problem of QoS crosstalk within the os, performing multiplexing at a low level can mitigate this problem, as demonstrated by the Exokernel and Nemesis os, unintentional or undesired interactions bt tasks are minimized; 
we use this same basic approach to build Xen, which multiplexes physical resources at the granularity of an entire os and is able to provide perf isolation bt them, in contrast to process-levlel multiplexing this also allows a range of guest os to gracefully coexist rather than mandating a specific application binary interface, there is a price to pay for this flexibility -running a full os is more heavyweight than running a process, both in terms of initialization(such as booting or resuming versus fork and exec), and in terms of resource consumption; 
for our target of up to 100 hosted os instances, we believe this price is worth paying -it allows individual users to run unmodified binaries, or collections of binaries, in a resource controlled fashion(such as an Apache server along with a PostgreSQL backend), furthermore it provides an extremely high level of flexibility since the user can dynamically create the precise exec environment their sw requires, unfortunate config interactions bt various services and applications are avoided(such as each windows instance maintains its own strategy).
### Xen :Approach and Overview
in a traditional vmm the virtual hw exposed is functionally identical to underlying machine, although full virtualization has obvious benefits of allowing unmodified os to be hosted, it also has a number of drawbacks, this is particularly true for the prevalent IA-32, x86, architecture; 
support for full virtualization was never part of the x86 architectural design, certain supervisor instructions must be handled by vmm for correct virtualization, but executing these with insufficient privilege fails silently rather than causing a convenient trap, efficiently virtualizing the x86 mmu is also difficult, these problems can be solved but only at the cost of increased complexity and reduced perf; VMware's ESX Server dynamically rewrites portions of hosted machine code to insert traps wherever vmm intervention might be required, this translation is applied to the entire guest os kernel(with associated translation, exec, and caching costs) since all non-trapping priviledged instructions must be caught and handled, ESX Server implements shadow versions of system structures such as page tables and maintains consistency with the virtual tables by trapping every update attempt -this approach has a high cost for update-intensive ops such as creating a new application process; 
notwithstanding the intricacies of x86, there are other arguments against full virtualization, in particular, there are situations in which it is desirable for hosted os to see real as well as virtual resources -providing both real and virtual time allows a guest os to better support time-sensitive tasks, and to correctly handle tcp timeouts and rtt estimates, while exposing real machine addr allows a guest os to improve perf by using superpages or page coloring; 
we avoid drawbacks of full virtualization by presenting a vm abstraction that is similar but not identical to the underlying hw -an approach which has been dubbed paravirtualization, this promises improved perf, although it does require modifications to the guest os, however, we do not require changes to the ABI -application binary interface, and hence no modifications are required to guest applications, we distill the discussion so far into a set of design principles including: (i)suport for unmodified application binaries is essential, or users will not transition to Xen, hence we must virtualize all architectural features required by existing standard ABIs, (ii)supporting full multiapplication os is important, as this allows complex server configs to be virtualized within a single guest os instance, (iii)paravirtualization is necessary to obtain high perf and strong resource isolation on uncooperative machine architestures such as x86, (iv)even on cooperative machine structures, completely hiding effects of resource virtualization from guest os risks both correctness and perf; 
note that our paravirtualized x86 abstraction is quite different from that proposed by the recent Denali project, Denali is designed to support thousands of vms running network services, the vast majority of which are small-scale and unpopular, in contrast, Xen is intended to scale to approximately 100 vms running industry standard applications and services, given these very different goals, it is instructive to contrast Denali's design choices with our own principles including: (i)Denali does not target existing ABIs, and so can elide certain architectural features from their vm interface, for example, Denali does not fully support x86 segmentation although it is exported(and widely used, such as segments are frequently used by thread libs to address thread-local data) in the ABIs of NetBSD, linux, windows xp; (ii)Denali implementation does not address the problem of supporting application multiplexing, nor multiple addr spaces, within a single guest os, rather, applications are linked explicitly against an instance of the Ilwaco guest os in a manner rather reminiscent of a libOS in the Exokernel, hence each vm essentially hosts a single-user single-application unprotected os, by contrast, in Xen a single vm hosts a real os which may itself securely multiplex thousands of unmodified user-level processes, although a prototype virtual mmu has been developed which may help Denali in this area, we are unaware of any published technical details or evaluation; (iii)in the Denali archiecture the vmm performs all paging to and from disk, this is perhaps related to lack of mem-mgmt support at virtualization layer, paging within vmm is contrary to our goal of perf isolation -malicious vms can encourage thrashing behaviour, unfairly depriving others of cpu time and disk bdwidth, in Xen we expect each guest os to perform its own paging using its own guaranteed mem reservation and disk allocation(an idea previously exploited by self-paging); (iv) Denali virtualizes the namespaces of all machine resources, taking the view that no vm can access the resource allocations of another vm if it cannot name them(such as vms have no knowledge of hw addrs but only the virtual addrs created for them by Denali), in contrast, we believe that secure access control within the hypervisor is sufficient to ensure protection -furthermore, as discussed previously, there are strong correctness and perf arguments for making physical resources directly visible to guest os; in the following section we describe vm abstraction exported by Xen and discuss how a guest os must be modified to conform to this, note that here we reserve the term guest os to refer to one of the os that Xen can host and we use the term domain to refer to a running vm within which a guest os executes, the distinction is analogous to that bt a program and a process in a conventional system, we call Xen itself the hypervisor since it operates at a higher privilege level than the supervisor code of the guest os that it hosts; 
**vm interface:** [click for an overview of the paravirtualized x86 interface](./img/paravirtualized-x86-interface.png). factored into 3 broad aspects of the system: mem mgmt, cpu, device io, in the following we address each machine subsystem in turn, and discuss how each is presented in our paravirtualized architecture, note that although certain parts of our implementation, such as mem mgmt, are specific to x86, many aspects(such as our virtual cpu and io devices) can be readily applied to other machine architectures, furthermore, x86 represents a worst case in the areas where it differs significantly from risc-style processors, such as efficiently virtualizing hw page tables is more difficult than virtualizing a sw-managed tlb; 
**mem mgmt:** virtualizing mem is undoubtedly the most difficult part of paravirtualizing an architecture, both in terms of mechanisms required in the hypervisor and modifications required to port each guest os, the task is easier if architecture provides a sw-managed tlb as these can be efficiently virtualized in a simple manner, a tagged tlb is another useful feature supported by most server-class risc architectures including: Alpha, MIPS, SPARC, associating an addr-space identifier tag with each tlb entry allows the hypervisor and each guest os to efficiently coexist in separate addr spaces because there is no need to flush the entire tlb when transferring exec; unfortunately, x86 does not have a sw-managed tlb, instead, tlb misses are serviced automatically by the processor by walking the page table structure in hw, hence to achieve the best possible perf, all valid page translators for the current addr space should be present in the hw-accessible page table, moreover, because tlb is not targeted, addr space switches typically require a complete tlb flush, given these limitations, we made 2 decisions: (i)guest os are responsible for allocating and managing the hw page tables, with minimal involvement from Xen to ensure safety and isolation, (ii)Xen exists in a 64MB section at the top of every addr space, hence avoiding a tlb flush when entering and leaving the hypervisor; each time a guest os requires a new page table, perhaps because a new process is being created, it allocates and initializes a page from its own mem reservation and registers it with Xen, at this point os must relinquish direct write privileges to the page-table mem, all subsequent updates must be validated by Xen, this restrictes updates in a number of ways, including only allowing an os to map pages that it owns and disallowing writable mappings of page tables; guest os may batch update reqs to amortize overhead of entering the hypervisor; the top 64MB region of each addr space, which is reserved for Xen, is not accessible or remappable by guest os, however, this addr region is not used by any of the common x86 ABIs, hence this restriction does not break application compatibility; segmentation is virtualized in a similar way, by validating updates to hw segment descriptor tables, the only restrictions on x86 segment descriptors are: (i)they must have lower privilege than Xen, (ii)they may not allow any access to Xen-reserved portion of the addr space; 
**cpu:** virtualizing cpu has several implications for guest os, principally, insertion of a hypervisor below os violates usual assumption that os is the most privileged entity in the system, in order to protect the hypervisor from os misbehavior(and domains from one another), guest os must be modified to run at a lower privilege level; many processor archiectures only provide 2 privilege levels, in these cases guest os would share the lower privilege level with applications, guest os would then protect itself by running in a separate addr space from its applications, and indirectly pass control to and from applications via hypervisor to set the virtual privilege level and change current addr space, again, if processor's tlb supports addr-space tags then expensive tlb flushes can be avoided; efficient virtualization of privilege levels is possible on x86 because it supports 4 distinct privilege levels in hw, the x86 privilege levels are generally described as rings, and are numbered from 0(most privileged) to 3(least privileged), os code typically executes in ring0 because no other ring can execute privileged instructions, while ring3 is generally used for application code, to our knowledge, rings1&2 have not been used by any well-known x86 os since os/2, any os which follows this common arrangement can be ported to Xen by modifying it to execute in ring1, this prevents guest os from directly executing privileged instructions, yet it remains safely isolated from applications running in ring3; privileged instructions are paravirtualized by requiring them to be validated and executed within Xen -this applies to ops such as installing a new page table, or yielding the processor when idle(rather than attempting to hlt it), any guest os attempt to directly execute a privileged instruction is failed by the processor, either silently or by taking a fault, since only Xen executes at a sufficiently privileged level; exceptions, including mem faults and sw traps, are virtualized on x86 very straightforwardly, a table describing the handler for each type of exception is registered with Xen for validation, the handlers specified in this table are generally identical to those for real x86 hw, this is possible because the exception stack frames are unmodified in our paravirtualized architecture, the sole modifications is to the page fault handler, which would normally read the faulting addr from a privileged processor register(CR2), since this is not possible, we write it into an extended stack frame(*in hindsight, writing the value into a pre-agreed shared mem location rather than modifying the stack frame would have simplified the xp port), when an exception occurs while executing outside ring0, Xen's handler creates a copy of the exception stack frame on guest os stack and returns control to the appropriate registered handler; typically only 2 types of exception occur frequently enough to affect system perf -system calls(which are usually implemented via a sw exception) and page faults, we improve the perf of system calls by allowing each guest os to register a fast exception handler which is accessed directly by the processor without indirecting via ring0, this handler is validated before installing it in the hw exception table, unfortunately it is not possible to apply the same technique to the page fault handler because only code executing in ring0 can read the faulting addr from reg CR2, page faults must therefore always be delivered via Xen so that this reg value can be saved for access in ring1; safely is ensured by validating exception handlers when they are presented to Xen, the only required check is that the handler's code segment does not specify exec in ring0, since no guest os can create such a segment it suffices to compare the specified segment selector to a small number of static values which are reserved by Xen, apart from this, any other handler problems are fixed up during exception propagation -for example, if the handler's code segment is not present or if the handler is not paged into mem then an appropriate fault will be taken when Xen executes the iret instruction which returns to the handler, Xen detects these double faults by checking the faulting program counter value -if addr resides within the exception-virtualizing code then the offending guest os is terminated; note that this lazy checking is safe even for the direct system-call handler -access faults will occur when cpu attempts to directly jump to guest os handler, in this case, the faulting addr will be outside Xen(since Xen will never execute a guest os system call) and so the fault is virtualized in the normal way, if propagation of the fault causes a further double fault then guest os is terminated as described above; 
**device io:** rather than emulating existing hw devices, as is typically done in fully virtualized environments, Xen exposes a set of clean and simple device abstractions, this allows us to design an interface that is both efficient and satisfies our requirements for protection and isolation, to this end, io data is transferred to and from each domain via Xen, using shared-mem, asynchronous buffer-descriptor rings, these provide a high-perf comm mechanism for passing buffer info vertically through the system, while allowing Xen to efficiently perform validation checks(such as checking that buffers are contained within a domain's mem reservation); similar to hw interrupts, Xen supports a lightweight event-delivery mechanism which is used for sending asynchronous notifications to a domain, these notifications are made by updating a bitmap of pending event types and, optionally, by calling an event handler specified of guest os(such as to avoid extra costs incurred by frequent wake-up notifications).
### The Cost of Porting an OS to Xen
[click for cost, in lines of code, of porting commodity os to Xen's paravirtualized x86 environment](./img/cost-of-porting-commodity-operating-systems-to-Xen-paravirtualized-x86-environment.png), note that our NetBSD port is at a very early stage, and hence we report no figures here, the xp port is more advanced, but still in progress, it can execute a number of user-space applications from a ram disk, but it currently lacks any virtual io drivers, for this reason, figures for xp's virtual device drivers are not presented, however, as with linux, we expect these drivers to be small and simple due to the idealized hw abstraction presented by Xen; 
windows xp required a surprising number of modifications to its architecture independent os code because it uses a variety of structures and unions for accessing PTEs -page-table entries, each page-table access had to be separately modified, although some of this process was automated with scripts, in contrast, linux needed far fewer modifications to its generic mem system as it uses pre-processor macros to access PTEs -the macro definitions provide a convenient place to add translation and hypervisor calls required by paravirtualization; in both os, the architecture-specific sections are effectively a port of x86 code to our paravirtualized architecture, this involved rewriting routines which used privileged instructions, and removing a large amount of low-level system initialization code, again, more changes were required in windows xp, mainly due to presence of legacy 16-bit emulation code and need for a somewhat different boot-loading mechanism, note that the x86-specific code base in xp is substantially larger than in linux and hence a larger porting effort should be expected.
### Control and Mgmt
throughout the design and implementation of Xen, a goal has been to separate policy from mechanism wherever possible, although the hypervisor must be involved in data-path aspects(such as scheduling cpu bt domains, filtering network packets before transmission, or enforcing access control when reading data blocks), there is no need for it to be involved in, or even aware of, higher level issues such as how cpu is be to shared, or which kinds of packet each domain may transmit; 
the resulting architecture is one in which the hypervisor itself provides only basic control ops, these are exported through an interface accessible from authorized domains, potentially complex policy decisions such as admission control are best performed by mgmt sw running over a guest os rather than in privileged hypervisor code; 
[click for the overall system structure](./img/overall-system-structure-of-a-machine-running-the-xen-hypervisor.png), note that a domain is created at boot time which is permitted to use the control interface, the initial domain, termed Domain0, is responsible for hosting the application-level mgmt sw, the control interface provides the ability to create and terminate other domains and to control their associated scheduling params, physical mem allocations, and the access they are given to the machine's physical disks and network devices; 
in addition to processor and mem resources, the control interface supports creation and delection of VIFs -virtual network interfaces and VBDs -virtual block devices, these virtual io devices have associated access-control info which determines which domains can access them, and with what restrictions(such as a read-only vbd may be created, or a vif may filter ip packets to prevent source-addr spoofing); 
this control interface, together with profiling stats on current state of the system, is exported to a suite of application-level mgmt sw running in Domain0, this complement of administrative tools allow convenient mgmt of the entire server -current tools can create and destroy domains, set network filters and routing rules, monitor per-domain network activity at packet and flow granularity, and create and delete virtual network interfaces and virtual block devices, we anticipate dev of higher-level tools to further automate the application of administrative policy.
### Detailed Design
in this section we introduce design of the major subsystems that make up a Xen-based server, in each case we present both Xen and guest os functionality for clarity of exposition, the current discussion of guest os focuses on XenoLinux as this is the most mature, nonetheless our ongoing porting of windows xp and netbsd gives us confidence that Xen is guest os agnostic.
### Control Transfer :Hypercalls and Events
2 mechanisms exist for control interactions bt Xen and an overlying domain :synchronous calls from a domain to Xen may be made using a hypercall, while notifications are delivered to domains from Xen using an asynchronous event mechanism; 
the hypercall interface allows domains to perform a synchronous sw trap into hypervisor to perform a privileged op, analogous to use of system calls in conventional os, an example use of a hypercall is to request a set of page-table updates, in which Xen validates and applies a list of updates, returning control to the calling domain when this is completed; 
comm from Xen to a domain is provided through an asynchronous event mechanism, which replaces usual delivery mechanisms for device interrupts and allows lightweight notification of important events such as domain-termination reqs, akin to traditionl unix signals, there are only a small number of events, each acting to flag a particular type of occurence, for example, events are used to indicate that new data has been received over the network, or that a virtual disk req has completed; 
pending events are stored in a per-domain bitmask which is updated by Xen before invoking an event-callback handler specified by guest os, the callback handler is responsible for resetting the set of pending events, and responding to the notifications in an appropriate manner, a domain may explicitly defer event handling by setting a Xen-readable sw flag :this is analogous to disabling interrupts on a real processor.
### Data Transfer :I/O Rings
the presence of a hypervisor means there is an additional protection domain bt guest os and io devices, so it is crucial that a data transfer mechanism be provided that allows data to move vertically through the system with as little overhead as possible; 
2 main factors have shaped the design of our io-transfer mechanism :resource mgmt and event notification, for resource accountability, we attempt to minimize the work required to demultiplex data to a specific domain when an interrupt is received from a device -the overhead of managing buffers is carried out later where computation may be accounted to the appropriate domain, similarly, mem committed to device io is provided by relevant domains wherever possible to prevent crosstalk inherent in shared buffer pools, io buffers are protected during data transfer by pinning underlying page frames within Xen; 
[click for structure of our io descriptor rings](./img/structure-of-asynchronous-io-rings.png), here a ring is a circular queue of descriptors allocated by a domain but accessible from within Xen, descriptors do not directly contain io data, instead, io data buffers are allocated out-of-band by guest os and indirectly referenced by io descriptors, access to each ring is based around 2 pairs of producer-consumer pointers :domains place reqs on a ring, advancing a req producer pointer, and Xen removes these reqs for handling, advancing an associated req consumer pointer, responses are placed back on the ring similarly, save with Xen as the producer and guest os as the consumer, there is no requirement that reqs be processed in order :guest os associates a unique identifier with each req which is reproduced in the associated response, this allows Xen to unambiguously reorder io ops due to scheduling or priority considerations; 
this structure is sufficiently generic to support a number of different device paradigms, for example, a set of reqs can provide buffers for network packet reception, subsequent responses then signal arrival of packets into these buffers, reordering is useful when dealing with disk reqs as it allows them to be scheduled within Xen for efficiency, and use of descriptors with out-of-band buffers makes implementing zero-copy transfer easy; 
we decouple production of reqs or responses from the notification of the other party -in the case of reqs, a domain may enqueue multiple entries before invoking a hypercall to alert Xen, in the case of responses, a domain can defer delivery of a notificatioon event by specifying a threshold number of responses, this allows each domain to trade-off latency and throughput requirements, similarly to the flow-aware interrupt dispatch in the ArseNIC Gigabit Ethernet interface.
### Building a New Domain
the task of building the initial guest os structures for a new domain is mostly delegated to Domain0 which uses its privileged control interfaces(see section Control and Mgmt) to access the new domain's mem and inform Xen of initial reg state, this approach has a number of advantages compared with building a domain entirely within Xen, including reduced hypervisor complexity and improved rebustness(accesses to privileged interface are sanity checked which allowed us to catch many bugs during initial deployment); however, most important is the case with which the building process can be extended and specialized to cope with new guest os, for example, the boot-time addr space assumed by the linux kernel is considerably simpler than that expected by windows xp, it would be possible to specify a fixed initial mem layout for all guest os, but this would require additional bootstrap code within every guest os to lay things out as required by the rest of os, unfortunately this type of code is tricky to implement correctly, for simplicity and robustness it is therefore better to implement it within Domain0 which can provide much richer diagnostics and debugging support than a bootstrap environment.<br \>
众多系统已设计出使用虚拟化技术来细分现代计算机的丰富资源，有些系统需要专用硬件，或者无法支持商用操作系统；有些系统则以牺牲性能为代价，力求实现 100% 二进制兼容性；有些系统则为了追求速度而牺牲功能安全性；很少有系统提供资源隔离或性能保证，大多数系统仅提供尽力而为的配置，存在拒绝服务的风险；
我们推出了 Xen，一款 x86 虚拟机监视器，它允许多个商用操作系统以安全且资源可控的方式共享传统硬件，同时又不牺牲其性能或功能。这是通过提供一种理想化的虚拟机抽象来实现的，Linux、BSD、Windows XP 等操作系统可以轻松移植到该抽象中；
我们的设计目标是在现代服务器上同时托管多达 100 个虚拟机实例。Xen 采用的虚拟化方法极其高效——我们允许同时托管 Linux、Windows XP 等操作系统，且性能开销几乎可以忽略不计——与未虚拟化的情况相比，最多只高出几个百分点。在一系列微基准测试和系统级测试中，我们的性能显著优于竞争对手的商业和免费解决方案。
现代计算机的性能已足够强大，可以使用虚拟化技术来呈现许多小型虚拟机的幻象，每个虚拟机都运行着一个独立的操作系统实例，这重新引发了人们对虚拟机技术的兴趣。我们在此介绍 Xen，这是一款高性能资源管理型 VMM - 虚拟机监视器，它支持服务器整合、主机托管设施、分布式 Web 服务、安全计算平台和应用程序移动性等应用；
成功划分一台机器以支持多个操作系统的并发执行面临诸多挑战，包括：(i) 虚拟机必须彼此隔离 - 一个虚拟机的执行对另一个虚拟机的性能产生不利影响是不可接受的，尤其是在虚拟机由互不信任的用户拥有时；(ii) 有必要支持各种不同的操作系统以适应流行应用程序的异构性；(iii) 虚拟化带来的性能开销应该很小；
Xen 托管商用操作系统，尽管经过一些源代码修改，但本文描述和评估的原型可以支持我们的 XenoLinux 客户操作系统的多个并发实例，每个实例都导出一个与非虚拟化 Linux2.4 相同的应用程序二进制接口。我们将 Windows XP 移植到 Xen 的工作尚未完成，但能够运行简单的用户空间进程；NetBSD 的移植工作也在进行中；
Xen 允许用户动态实例化一个操作系统，并根据需要执行。在 XenoServer 项目中，我们将 Xen 部署在 ISP 或互联网交换中心等经济战略位置的标准服务器硬件上。我们在启动新虚拟机时执行准入控制，并期望每个虚拟机以某种方式为其所需的资源付费。我们在其他地方讨论了这方面的想法和方法，这里我们重点介绍 VMM；
构建一个在共享计算机上托管多个应用程序和服务器的系统有很多方法，最简单的方法可能是部署一台或多台运行标准操作系统（例如 Linux、Windows）的主机，然后允许用户安装文件和启动进程。传统操作系统技术无法保护应用程序，经验表明，由于复杂的配置交互和所谓的不相交应用程序，系统管理很快就会成为一项耗时的任务；
更重要的是，这样的系统无法充分支持性能隔离；一个进程的调度策略、内存需求、网络流量和磁盘访问会影响其他进程的性能，当资源配置充足且用户群体封闭时（例如计算网格或实验性的 PlanetLab 平台），这种情况尚可接受，但当资源超额认购或用户不合作时则不可接受；解决这个问题的一种方法是改进操作系统对性能隔离的支持，这已在资源容器、linux/rk、qlinux 和 silk 等技术中或多或少地得到证明。这种方法的一个难点是确保所有资源使用情况都归于正确的进程——例如，考虑由于缓冲区缓存或页面替换算法而导致的应用程序之间的复杂交互，这实际上是操作系统内 QoS 串扰的问题，在低级别执行多路复用可以缓解此问题，正如 Exokernel 和 Nemesis 操作系统所证明的那样，可以最大限度地减少任务之间的无意或不必要的交互；
我们使用相同的基本方法来构建 Xen，它以整个操作系统的粒度复用物理资源，并能够为它们提供性能隔离，与进程级复用相比，它还允许一系列客户操作系统优雅地共存，而不是强制使用特定的应用程序二进制接口，这种灵活性是有代价的 - 运行完整的操作系统比运行进程更重，无论是在初始化方面（例如启动或恢复与 fork 和 exec 的区别），以及资源消耗方面；
对于我们最多 100 个托管操作系统实例的目标，我们认为这个代价是值得的——它允许单个用户以资源受控的方式运行未修改的二进制文件或二进制文件集合（例如，Apache 服务器和 PostgreSQL 后端），此外，它提供了极高的灵活性，因为用户可以动态创建其软件所需的精确执行环境，避免了繁琐的配置交互，同时避免了各种服务和应用程序（例如，每个 Windows 实例都维护自己的策略）。
### Xen：方法和概述
在传统的虚拟机中，暴露的虚拟硬件在功能上与底层机器相同。虽然完全虚拟化具有允许托管未修改操作系统的明显优势，但它也存在一些缺点，对于流行的 IA-32、x86 架构尤其如此；
x86 架构设计从未支持完全虚拟化。某些管理指令必须由 VMM 处理才能正确实现虚拟化，但如果权限不足，执行这些指令时会默默失败，而不是触发陷阱。此外，高效地虚拟化 x86 mmu 也很困难。这些问题可以解决，但代价是复杂性增加和性能降低。VMware 的 ESX Server 会动态重写部分托管机器代码，以便在可能需要 VMM 干预的地方插入陷阱。由于所有非陷阱特权指令都必须被捕获和处理，因此此转换将应用于整个客户操作系统内核（并产生相关的转换、执行和缓存成本）。ESX Server 实现了系统结构（如页表）的影子版本，并通过捕获每次更新尝试来保持与虚拟表的一致性——这种方法对于更新密集型操作（如创建新的应用程序进程）的成本很高。
尽管 x86 架构错综复杂，但仍有其他反对完全虚拟化的观点，特别是在某些情况下，托管操作系统需要能够查看真实资源和虚拟资源——同时提供真实时间和虚拟时间可以让客户操作系统更好地支持时间敏感型任务，并正确处理 TCP 超时和 RTT 预估，而暴露真实机器地址可以让客户操作系统通过使用超级页面或页面着色来提升性能；
我们通过提出与底层硬件相似但不完全相同的虚拟机抽象来避免完全虚拟化的缺点——这种方法被称为半虚拟化，这有望提高性能，尽管它确实需要修改客户操作系统。然而，我们不需要更改 ABI（应用程序二进制接口），因此客户应用程序无需进行任何修改。我们将目前的讨论提炼为一组设计原则，包括：（i）必须支持未修改的应用程序二进制文件，否则用户将不会过渡到 Xen，因此我们必须虚拟化现有标准 ABI 所需的所有架构特性；（ii）支持完整的多应用程序操作系统非常重要，因为这允许在单个客户操作系统实例中虚拟化复杂的服务器配置；（iii）半虚拟化对于在非协作的机器架构（例如 x86）上获得高性能和强大的资源隔离是必要的；（iv）即使在协作的机器结构上，完全隐藏客户操作系统对资源虚拟化的影响也会危及正确性和性能；
请注意，我们的半虚拟化 x86 抽象与最近的 Denali 项目提出的抽象有很大不同，Denali 旨在支持运行网络服务的数千个虚拟机，其中绝大多数是小规模且不受欢迎的，相比之下，Xen 旨在扩展到运行行业标准应用程序和服务的大约 100 个虚拟机，鉴于这些截然不同的目标，将 Denali 的设计选择与我们自己的原则进行对比是有益的，包括：（i）Denali 不针对现有的 ABI，因此可以从其虚拟机接口中省略某些架构特性，例如，Denali 不完全支持 x86 分段，尽管它在 NetBSD、linux、windows xp 的 ABI 中被导出（并且被广泛使用，例如线程库经常使用段来处理线程本地数据）； (ii)Denali 实现并未解决在单个客户操作系统中支持应用程序多路复用或多个地址空间的问题，而是将应用程序明确链接到 Ilwaco 客户操作系统的一个实例，这种方式让人联想到 Exokernel 中的 libOS，因此每个虚拟机本质上都托管一个单用户单应用程序不受保护的操作系统，相比之下，在 Xen 中，单个虚拟机托管一个真正的操作系统，该操作系统本身可以安全地多路复用数千个未修改的用户级进程，尽管已经开发出一个原型虚拟 mmu 可能有助于 Denali 在这方面的发展，但我们不知道任何已发布的技术细节或评估；（iii）在 Denali 架构中，vmm 执行所有磁盘分页，这可能与虚拟化层缺乏 mem-mgmt 支持有关，vmm 内的分页与我们的性能隔离目标相悖 - 恶意虚拟机会鼓励抖动行为，不公平地剥夺其他虚拟机的 CPU 时间和磁盘带宽，在 Xen 中，我们希望每个客户操作系统使用自己保证的内存预留和磁盘分配（以前由自分页利用的想法）执行自己的分页；（iv）Denali 虚拟化所有机器资源的命名空间，认为如果虚拟机无法命名其他虚拟机的资源分配（例如，虚拟机不知道硬件地址，只知道 Denali 为其创建的虚拟地址），则任何虚拟机都无法访问这些资源分配，相反，我们认为虚拟机管理程序内的安全访问控制足以确保保护 - 此外，如前所述，有很强的正确性和性能论据可以使物理资源对客户操作系统直接可见；在下一节中，我们将描述 Xen 导出的虚拟机抽象，并讨论如何修改客户操作系统以使其符合此抽象。请注意，这里我们保留术语“客户操作系统”来指代 Xen 可以托管的操作系统之一，并使用术语“域”来指代客户操作系统在其中运行的虚拟机。两者的区别类似于传统系统中程序和进程的区别。我们将 Xen 本身称为虚拟机管理程序，因为它的运行权限级别高于其托管的客户操作系统的管理代码；
**虚拟机接口**：[点击查看半虚拟化 x86 接口概览](./img/paravirtualized-x86-interface.png)。该系统主要分为三大方面：内存管理、CPU、设备 IO。接下来，我们将依次讨论每个机器子系统，并讨论它们在我们的半虚拟化架构中的呈现方式。需要注意的是，尽管我们实现的某些部分（例如内存管理）特定于 x86，但许多方面（例如我们的虚拟 CPU 和 IO 设备）可以很容易地应用于其他机器架构。此外，x86 在与 RISC 风格处理器存在显著差异的领域中表现最差，例如，高效虚拟化硬件页表比虚拟化软件管理的 TLB 更困难；
**mem mgmt：** 毫无疑问，虚拟化内存是架构半虚拟化中最困难的部分，无论是从虚拟机管理程序所需的机制还是移植每个来宾操作系统所需的修改来看，如果架构提供了 sw 管理的 tlb，则任务会更容易，因为这些可以以简单的方式有效地虚拟化，标记的 tlb 是大多数服务器级 risc 架构支持的另一个有用功能，包括：Alpha、MIPS、SPARC，将地址空间标识符标记与每个 tlb 条目关联起来，允许虚拟机管理程序和每个来宾操作系统在单独的地址空间中有效地共存，因为在传输 exec 时无需刷新整个 tlb；不幸的是，x86 没有由软件管理的 TLB，相反，处理器通过遍历硬件中的页表结构自动处理 TLB 未命中，因此为了实现最佳性能，当前地址空间的所有有效页面转换器都应该存在于硬件可访问的页表中，此外，因为 TLB 不是目标，所以地址空间切换通常需要完整的 TLB 刷新，考虑到这些限制，我们做出了两个决定：（i）客户操作系统负责分配和管理硬件页表，Xen 的参与最少，以确保安全和隔离，（ii）Xen 存在于每个地址空间顶部的 64MB 部分中，因此避免在进入和离开虚拟机管理程序时进行 TLB 刷新；每次客户操作系统需要新的页表时（可能是因为正在创建新进程），它会从自己的内存预留中分配并初始化一个页面，然后将其注册到 Xen，此时操作系统必须放弃对页表内存的直接写入权限，所有后续更新都必须经过 Xen 验证，这会以多种方式限制更新，包括仅允许操作系统映射其拥有的页面，而不允许可写的页表映射；客户操作系统可以批量更新请求，以摊销进入虚拟机管理程序的开销；每个地址空间的前 64MB 区域是为 Xen 保留的，客户操作系统无法访问或重新映射，但是，任何常见的 x86 ABI 都不使用此地址区域，因此此限制不会破坏应用程序兼容性；分段的虚拟化方式类似，通过验证硬件段描述符表的更新，对 x86 段描述符的唯一限制是：(i) 它们的权限必须低于 Xen；(ii) 它们不得允许访问 Xen 保留的地址空间部分；
**CPU：** 虚拟化 CPU 对客户操作系统有几个影响，主要是，在操作系统下方插入虚拟机管理程序违反了通常的假设，即操作系统是系统中权限最高的实体，为了保护虚拟机管理程序免受操作系统不当行为（以及彼此之间的域）的影响，必须修改客户操作系统以在较低的特权级别运行；许多处理器架构仅提供 2 个特权级别，在这种情况下，客户操作系统将与应用程序共享较低的特权级别，客户操作系统将通过在与其应用程序不同的地址空间中运行来保护自己，并通过虚拟机管理程序间接地将控制权传递给应用程序和从应用程序传递控制权以设置虚拟特权级别并更改当前地址空间，同样，如果处理器的 tlb 支持地址空间标签，则可以避免昂贵的 tlb 刷新； x86 上可以实现特权级别的高效虚拟化，因为它在硬件中支持 4 种不同的特权级别，x86 特权级别通常被描述为环，并从 0（最高特权）到 3（最低特权）编号，操作系统代码通常在 ring0 中执行，因为没有其他 ring 可以执行特权指令，而 ring3 通常用于应用程序代码，据我们所知，自 os/2 以来，rings1&2 还没有被任何知名的 x86 操作系统使用过，任何遵循这种常见安排的操作系统都可以通过修改它以在 ring1 中执行来移植到 Xen，这可以防止客户操作系统直接执行特权指令，但它仍然与在 ring3 中运行的应用程序安全地隔离；特权指令是通过要求在 Xen 中验证和执行来实现半虚拟化的 - 这适用于诸如安装新页表或在空闲时让出处理器（而不是尝试将其关闭）之类的操作，任何客户操作系统直接执行特权指令的尝试都会因处理器而失败，无论是默默地还是通过发生故障，因为只有 Xen 在足够特权的级别上执行；异常（包括内存错误和软件陷阱）在 x86 上非常直接地虚拟化，描述每种类型异常的处理程序的表在 Xen 中注册以进行验证，此表中指定的处理程序通常与真实 x86 硬件的处理程序相同，这是可能的，因为异常堆栈帧在我们的半虚拟化架构中未经修改，唯一的修改是页面错误处理程序，它通常会从特权处理器寄存器（CR2）读取错误地址，由于这是不可能的，我们将其写入扩展堆栈帧（*事后看来，将值写入预先商定的共享内存位置而不是修改堆栈帧会简化 xp 端口），当在 ring0 之外执行时发生异常时，Xen 的处理程序会在客户操作系统堆栈上创建异常堆栈帧的副本，并将控制权返回给适当的注册处理程序；通常只有 2 种类型的异常发生频率足以影响系统性能 - 系统调用（通常通过 sw 异常实现）和页面错误，我们通过允许每个客户操作系统注册一个快速异常处理程序来提高系统调用的性能，该处理程序可由处理器直接访问，而无需通过 ring0 间接访问，此处理程序在安装到 hw 异常表中之前经过验证，不幸的是，无法将相同的技术应用于页面错误处理程序，因为只有在 ring0 中执行的代码才能从 reg CR2 中读取错误地址，因此页面错误必须始终通过 Xen 传递，以便可以保存此 reg 值以供在 ring1 中访问；通过在异常处理程序提交给 Xen 时对其进行验证来确保安全，唯一需要检查的是处理程序的代码段未在 ring0 中指定 exec，因为没有客户操作系统可以创建这样的段，只需将指定的段选择器与 Xen 保留的少量静态值进行比较即可，除此之外，在异常传播期间修复任何其他处理程序问题 - 例如，如果处理程序的代码段不存在或处理程序未分页到 mem，则当 Xen 执行返回到处理程序的 iret 指令时将发生适当的故障，Xen 通过检查故障程序计数器值来检测这些双重故障 - 如果 addr 驻留在异常虚拟化代码中，则终止有问题的客户操作系统；请注意，即使对于直接系统调用处理程序，这种惰性检查也是安全的。当 CPU 尝试直接跳转到客户操作系统处理程序时，会发生访问错误。在这种情况下，出错的地址位于 Xen 之外（因为 Xen 永远不会执行客户操作系统的系统调用），因此错误会以正常方式虚拟化。如果错误的传播导致进一步的双重错误，则客户操作系统将如上所述终止；
**设备 IO**：Xen 并不像通常在完全虚拟化环境中那样模拟现有的硬件设备，而是公开了一组简洁的设备抽象，这使我们能够设计一个既高效又能满足我们对保护和隔离要求的接口。为此，IO 数据通过 Xen 在每个域之间传输，使用共享内存、异步缓冲区描述符环，这些环提供了高性能通信机制来在系统中垂直传递缓冲区信息，同时允许 Xen 高效地执行验证检查（例如检查缓冲区是否包含在域的内存预留内）；与硬件中断类似，Xen 支持轻量级事件传递机制，用于向域发送异步通知，这些通知通过更新待处理事件类型的位图以及可选地调用客户操作系统指定的事件处理程序来发送（例如，以避免频繁唤醒通知产生的额外成本）。
### 将操作系统移植到 Xen 的成本
[点击查看将商用操作系统移植到 Xen 半虚拟化 x86 环境的成本（以代码行数为单位）](./img/cost-of-porting-commodity-operating-systems-to-Xen-paravirtualized-x86-environment.png)，请注意，我们的 NetBSD 移植尚处于非常早期的阶段，因此我们在此未提供任何数据。Xp 移植更为先进，但仍在进行中，它可以从内存磁盘执行多个用户空间应用程序，但目前缺少任何虚拟 IO 驱动程序，因此，我们未提供 xp 虚拟设备驱动程序的数据。然而，与 Linux 一样，由于 Xen 提供的理想化的硬件抽象，我们预计这些驱动程序将会小巧而简单；
Windows XP 需要对其独立于体系结构的操作系统代码进行大量修改，因为它使用各种结构和联合来访问 PTE - 页表条目，每个页表访问都必须单独修改，尽管其中某些过程可以通过脚本自动完成，相比之下，Linux 需要对其通用内存系统进行少得多的修改，因为它使用预处理器宏来访问 PTE - 宏定义提供了一个方便的位置来添加半虚拟化所需的转换和虚拟机管理程序调用；在这两个操作系统中，特定于体系结构的部分实际上是 x86 代码到我们的半虚拟化体系结构的移植，这涉及重写使用特权指令的例程，并删除大量低级系统初始化代码，同样，Windows XP 需要进行更多更改，主要是因为存在遗留的 16 位仿真代码并且需要稍微不同的引导加载机制，请注意，XP 中特定于 x86 的代码库比 Linux 中的大得多，因此应该预计移植工作会更大。 ### 控制和管理
在 Xen 的整个设计和实现过程中，我们始终致力于尽可能地将策略与机制分离。尽管虚拟机管理程序必须参与数据路径方面（例如调度 CPU 和 BT 域、在传输前过滤网络数据包或在读取数据块时强制执行访问控制），但它无需参与甚至无需了解更高级别的问题，例如 CPU 如何共享，或者每个域可以传输哪些类型的数据包；
最终的架构是，虚拟机管理程序本身仅提供基本的控制操作，这些操作通过授权域可访问的接口导出，而诸如准入控制之类的潜在复杂策略决策最好由运行在客户操作系统上的管理软件来执行，而不是由特权虚拟机管理程序代码来执行；
[点击查看整体系统结构](./img/overall-system-structure-of-a-machine-running-the-xen-hypervisor.png)，请注意，系统启动时会创建一个域，该域允许使用控制接口。初始域（称为 Domain0）负责托管应用程序级管理软件。控制接口提供创建和终止其他域的功能，并控制其相关的调度参数、物理内存分配以及它们对计算机物理磁盘和网络设备的访问权限；
除了处理器和内存资源外，控制接口还支持创建和删除 VIF（虚拟网络接口）和 VBD（虚拟块设备）。这些虚拟 I/O 设备具有相关的访问控制信息，这些信息决定了哪些域可以访问它们，以及访问时会受到哪些限制（例如，可以创建只读的 VBD，或者 VIF 可以过滤 IP 数据包以防止源地址欺骗）；
该控制接口与系统当前状态的分析统计数据一起导出到Domain0中运行的一套应用级管理软件。这套管理工具可以方便地管理整个服务器——当前工具可以创建和销毁域、设置网络过滤器和路由规则、以数据包和流粒度监控每个域的网络活动，以及创建和删除虚拟网络接口和虚拟块设备。我们期待开发更高级别的工具来进一步自动化管理策略的应用。
### 详细设计
在本节中，我们将介绍构成基于Xen的服务器的主要子系统的设计。在每种情况下，为了清晰起见，我们都将介绍Xen和客户操作系统的功能。当前关于客户操作系统的讨论主要集中在XenoLinux上，因为它是最成熟的，当然，尽管如此，我们正在进行的 Windows XP 和 NetBSD 移植工作让我们确信 Xen 与客户操作系统无关。
### 控制转移：超级调用和事件
Xen 和上层域之间存在两种控制交互机制：域可以使用超级调用进行对 Xen 的同步调用，而 Xen 则使用异步事件机制向域发送通知；
超级调用接口允许域执行同步软件陷阱，进入虚拟机管理程序以执行特权操作，类似于传统操作系统中的系统调用。超级调用的一个示例是请求一组页表更新，其中 Xen 验证并应用更新列表，并在完成后将控制权返回给调用域；
Xen 与域之间的通信是通过异步事件机制实现的，该机制取代了通常的设备中断传递机制，并允许对重要事件（例如域终止请求）进行轻量级通知，类似于传统的 Unix 信号。事件数量很少，每个事件都用于标记特定类型的事件，例如，事件用于指示已通过网络接收到新数据，或虚拟磁盘请求已完成；
待处理事件存储在每个域的位掩码中，该位掩码由 Xen 在调用客户操作系统指定的事件回调处理程序之前进行更新。回调处理程序负责重置待处理事件集，并以适当的方式响应通知。域可以通过设置 Xen 可读的 sw 标志来显式推迟事件处理：这类似于在真实处理器上禁用中断。
### 数据传输：I/O 环
虚拟机管理程序的存在意味着客户操作系统和 I/O 设备之间存在一个额外的保护域，因此，提供一种数据传输机制至关重要，该机制允许数据以尽可能少的开销在系统中垂直移动；
我们的 I/O 传输机制的设计主要受两个因素的影响：资源管理和事件通知。为了实现资源可追溯性，我们尝试在设备收到中断时，尽量减少将数据解复用到特定域所需的工作——管理缓冲区的开销稍后再进行，此时计算可以归属于相应的域；同样，提交给设备 I/O 的内存尽可能由相关域提供，以防止共享缓冲池中固有的串扰；在数据传输过程中，通过固定 Xen 中的底层页框来保护 I/O 缓冲区；
[点击查看我们的 io 描述符环的结构](./img/structure-of-asynchronous-io-rings.png)，这里的环是由域分配但可从 Xen 内部访问的描述符循环队列，描述符不直接包含 io 数据，相反，io 数据缓冲区由客户操作系统带外分配，并由 io 描述符间接引用，对每个环的访问基于两对生产者-消费者指针：域将请求放置在环上，推进请求生产者指针，Xen 移除这些请求进行处理，推进相关的请求消费者指针，响应以类似方式放回环上，保存 Xen 作为生产者，客户操作系统作为消费者，不要求请求按顺序处理：客户操作系统为每个请求关联一个唯一标识符，该标识符在相关的响应中重现，这使得 Xen 可以根据调度或优先级考虑明确地重新排序 io 操作；
这种结构足够通用，可以支持多种不同的设备范例。例如，一组请求可以为网络数据包接收提供缓冲区，后续响应则向这些缓冲区发出数据包到达的信号；重新排序在处理磁盘请求时非常有用，因为它允许它们在 Xen 内部进行调度以提高效率；使用带有带外缓冲区的描述符可以轻松实现零拷贝传输；
我们将请求或响应的生成与对方的通知分离开来——对于请求，域可以在调用超级调用（hypercall）来通知 Xen 之前将多个条目入队；对于响应，域可以通过指定响应的阈值数量来推迟通知事件的传递。这允许每个域权衡延迟和吞吐量需求，类似于 ArseNIC 千兆以太网接口中的流感知中断调度。
### 构建新域
为新域构建初始客户操作系统结构的任务主要委托给 Domain0，Domain0 使用其特权控制接口（参见控制和管理部分）访问新域的内存并通知 Xen 初始注册状态。与完全在 Xen 内部构建域相比，此方法具有许多优势，包括降低虚拟机管理程序的复杂性和提高可重用性（对特权接口的访问经过健全性检查，这使我们能够在初始部署期间捕获许多错误）；然而，最重要的是构建过程可以进行扩展和专门化以应对新的客户操作系统的这种情况，例如，Linux 内核假设的启动时地址空间比 Windows XP 预期的要简单得多，可以为所有客户操作系统指定一个固定的初始内存布局，但这需要每个客户操作系统中都有额外的引导代码来按照其余操作系统的要求进行布局，不幸的是，这种类型的代码很难正确实现，因此，为了简单性和稳健性，最好在 Domain0 中实现它，因为它可以提供比引导环境更丰富的诊断和调试支持。
****
### Programming Models and Frameworks
### MapReuce :Simplified Data Processing on Large Clusters
### [Dean2004](https://www.usenix.org/legacy/events/osdi04/tech/full_papers/dean/dean.pdf)
MapReduce is a programming model and an associated implementation for processing and generating large datasets, users specify a map function that processes a key-value pair to generate a set of immediate key-value pairs, and a reduce function that merges all intermediate values associated with the same immediate key, many real world tasks are expressible in this model; 
programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines, the runtime system takes care of details of partitioning input data, scheduling the program's exec across a set of machines, handling machine failures, and managing required inter-machine comm, this allows programmers without any experience with parallel and distributed systems to easily utilize resources of a large distributed system; 
our implementation of MapReduce runs on a large cluster of commidity machines and it is highly scalable :a typical MapReduce computation processes many terabytes of data on thousands of machines, programmers find the system easy to use :hundreds of MapReduce programs have been implemented and 1000+ MapReduce jobs are executed on Google's clusters everyday. 
over the past 5 years, Google have implemented hundreds of special-purpose computations that process large amounts of raw data, such as crawled documents, web req logs, etc., to compute various kinds of derived data, such as inverted indices, various implementations of the graph structure of web documents, the summaries of #pages crawled per host, the set of most frequent queries in a given day, etc., most such computations are conceptually straightforward, however, input data is usually large and computatins have to be distributed across hundreds of thousands of machines in order to finish in a reasonable amount of time, the issues of how to parallelize computation, distribute data, and handle failures conspire to obscure original simple computation with large amounts of complex code to deal with these issues; 
as a reaction to this complexity, we designed a new abstraction that allows us to express the simple computations we were trying to perform but hides the messy details of parallelization, fault-tolerance, data distribution, and load balancing in a lib, our abstraction is inspired by map, reduce primitives present in Lisp and many other functional langs, we realized that most of our computations involved applying a map op to each logical record in our input in order to compute a set of intermediate key-value pairs, and then applying a reduce op to all values that shared the same key in order to combine derived data appropriately, our use of a functional model with user-specified map, reduce ops allows us to parallelize large computations easily and to use re-exec as the primary mechanism for fault tolerance; 
the major contributions of this work are a simple and powerful interface that enables automatic parallelization and distribution of large-scale computations, combined with an implementation of this interface that achieves high perf on large clusters of commidity PCs.
### Programming Model
the computation takes a set of input key-value pairs, and produces a set of output key-value pairs, the user of MapReduce lib expresses the computation as 2 functions :Map, Reduce, here, Map, written by user takes an input pair and produces a set of intermediate key-value pairs, the MapReduce lib groups together all intermediate values associated with the same intermediate key *I* and passes them to the Reduce function, Reduce, also written by user accepts an intermediate key *I* and and a set of values for that key, it merges together these values to form a possibly smaller set of values, typically just 0 or 1 output value is produced per Reduce invocation, the intermediate values are supplied to user's reduce function via an iterator, this allows us to handle lists of values that are too large to fit in mem. 
**example:** consider the problem of counting #occurrences of each word in a large collection of documents, user would write code similar to the following pseudo-code:
```
map(String key, String value):
  // key: document time
  // value: document contents
  for each word w in value:
    EmitIntermediate(w, "1");
reduce(String key, Iterator values):
  // key: a word
  // values: a list of counts
  int result = 0;
  for each v in values:
    result += ParseInt(v);
  Emit(AsString(result));
```
here, map function emits each word plus an associated count of occurrences(just 1 in this simple example), reduce function sums together all counts emitted for a particular word; 
in addition, user writes code to fill a mapreduce specification object with names of input and output files, and optional tuning params, user then invokes MapReduce function, passing it the specification obejct; user's code is linked together with MapReduce lib(implemented in C++);
**types:** even though the previous pseudo-code is written in terms of string inputs and outputs, conecptually the map and reduce functions supplied by user have associated types:
```
map    (k1,v1)       ->list(k2,v2)
reduce (k2,list(v2)) ->list(v2)
```
i.e., the input keys and values are drawn from a different domain than the output keys and values, furthermore, the intermediate keys and values are from the same domains as the output keys and values; 
our C++ implementation passes strings to and from user-defined functions and leaves it to user code to convert bt strings and appropriate types;
**more examples(that can be easily expressed as MapReduce computations):** distributed grep -map function emits a line if it matches a supplied pattern, reduce function is an identity function that just copies supplied intermediate data to the output; count of url access freq -map function processes logs of web page reqs and outputs <URL,1>, reduce function adds together all values for the same url and emits a <URL,totalcount> pair; reverse web-link graph -map function outputs <target,source> pairs for each link to a target url found in a page named source, reduce function concatenates the list of all source urls associated with a given target url and emits the pair <target,list(source)>; term-vector per host -a term vector summarizes the most important words that occur in a document or a set of documents as a list of <word,frequency> pairs, map function emits a <hostname,termvector> pair for each input document(where hostname is extracted from the url of the document), reduce function is passed all per-document term vectors for a given host, it adds these term vectors together, throwing away infrequent terms, and then emits a final <hostname,termvector> pair; inverted index -map function parses each document and emits a sequence of <word,documentID> pairs, reduce function accepts all pairs for a given word and sorts the corresponding document ids and emits a <word,list(documentID)> pair, the set of all output pairs forms a simple inverted index, it is easy to agument this computation to keep track of word positions; distributed sort -map function extracts the key from each record and emits a <key,record> pair, reduce function emits all pairs unchanges.
### Implementation
many different implementations of MapReduce interface are possible, the right choice depends on the environment, for example, one implementation may be suitable for a small shared-mem machine, another for a large numa multiprocessor, and yet another for an even larger collection of networked machines; 
this section describes an implementation targeted to the computing environment in wide use at Google -large clustes of commodity PCs connected together with switched ethernet, it is: (i)machines are typically dualprocessor x86 processers running linux, with 2-4GB of mem per machine, (ii)commodity networking hw is used -typically either 100 megabits/sec or 1 gigabit/sec at the machine level, but averaging considerably less in overall bisection bdwidth, (iii)a cluster consists of hundreds or thousands of machines, and therefore machine failures are common, (iv)storage is provided by inexpensive ide disks attached directly to individual machines, a distributed file system developed in-house is used to manage the data stored on these disks, the file system users replication to provide availability and reliability on top of unreliable hw, (v)users submit jobs to a scheduling system, each job consists of a set of tasks, and is mapped by scheduler to a set of available machines within a cluster; 
**exec overview:** Map invocations are distributed across multiple machines by automatically partitionalling input data into a set of *M* splits, input splits can be processed in parallel by different machines, Reduce invocations are distributed by partitioning the intermediate key space into *R* pieces using a partitioning function(such as *hash(key)* **mod** *R*), #partition(*R*) and the partitioning function are specified by user; [click for overall flow a MapReduce op in our implementation](./img/mapreduce.png), when our user program calls MapReduce function, the following sequence of actions occur(here the numbered labels correspond to the numbers in the list below): (i)MapReduce lib in the user program first splits input files into *M* pieces of typically 16MB to 64MB per piece(controllable by user via an optional param), it then starts up many copies of the program on a cluster of machines, (ii)one of the copies of the program is special -the master, the rest are workers that are assigned work by master, there are *M* map tasks and *R* reduce tasks to assign, master picks idle workers and assigns each one a map task or a reduce task, (iii)a worker who is assigned a map task reads the contents of the corresponding input split, it parses key-value pairs out of input data and passes each pair to user-defined Map function, the intermediate key-value pairs produced by Map function are buffered in mem, (iv)periodically, buffered pairs are written to local disk, partitioned into *R* regions by the partitioning function, locations of these buffered pairs on local disk are passed back to master, who is responsible for forwarding these locations to the reduce workers, (v)when a reduce worker is notified by master about these locations, it uses rpcs to read buffered data from local disks of the map workers, when a reduce worker has read all intermediate data, it sorts it by the intermediate keys so that all occurrences of the same key are grouped together, the sorting is needed because typically many different keys map to the same reduce taks, if the amount of intermediate data is too large to fit in mem, an external sort is uses, (vi)the reduce worker iterates over the sorted intermediate data and for each unique intermediate key encountered, it passes the key and the corrsponding set of intermediate values to user's Reduce function, output of Reduce function is appended to a final output file for this reduce partition, (vii) when all map tasks and reduce tasks have been completed, master wakes up user program, at this point, MapReduce call in the user program returns back to user code; 
after successful completion, output of the mapreduce exec is avialable in the *R* output files(1 per reduce task, with file names as specified by user), typically, users do not need to combine these *R* output files into 1 file -they often pass these files as input to another MapReduce call, or use them from another distributed application that is able to deal with input that is partitioned into multiple files; 
**master data structures:** master keeps several data structures, for each map task and reduce task, it stores the state(idle, in-progress, or complete), and the identity of worker machine(for non-idle tasks); master is the conduit through which the location of intermediate file regions is propagated from map tasks to reduce tasks, hence for each completed map taks, master stores locations and sizes of the *R* intermediate file regions produced by map task, updates to this location and size info are received as map tasks are completed, the info is pushed incrementally to workers that have in-progress reduce tasks; 
**fault tolerance:** since MapReduce lib is designed to help process very large amounts of data using hundreds of thousands of machines, the lib must tolerate machine failures gracefully: (i) worker failure: master pings every worker periodically, if no response is received from a worker in a certain amount of time, master marks the worker as failed, any map tasks completed by worker are reset back to their initial idel state, hence become eligible for scheduling on other workers, similarly, any map task or reduce task in-progress on a failed worker is also reset to idle and becomes eligible for rescheduling; completed map tasks are re-executed on a failure because their output is stored on local disks of the failed machine and is therefore inaccessible, completed reduce tasks do not need to be re-executed since their output is stored in a global file system; when a map task is executed first by worker *A* and then later executed by worker *B*(because *A* failed), all workers executing reduce tasks are notified of the exec, any reduce task that has not already read the data from worker *A* will read data from worker *B*; MapReduce is resilient to large-scale worker failures, for example, during 1 MapReduce op, network maintenance on a running cluster was causing groups of 80 machines at a time to become unreachable for several mins, MapReduce master simply re-executed the work done by unreachable worker machines, and continued to make forward progress, eventually completing MapReduce op; 
(ii) master failure: it is easy to make master write periodic checkpoints of master data structures described above, if master task dies, a new copy can be stored from the last checkpointed state, however, given that there is only a single master, its failure is unlikely, hence our current implementation aborts MapReduce computation if master fails, clients can check for this condition and retry MapReduce op if they desire; 
(iii) semantics in presence of failures: when the user-supplied map and reduce operators are deterministic functions of their input values, our distributed implementation produces the same output as would have been produced by a non-faulting sequential exec of the entire program; we rely on atomic commits of map and reduce task outputs to achieve this property, each in-progress tasks writes its output to private temporary files, a reduce task produces 1 such file, and a map task produces *R* such files(1 per reduce task), when a map task completes, worker sends a msg to master and includes names of the *R* temporary files in the msg, if master receives a completion msg for an already completed map task, it ignores the msg, otherwise, it records names of *R* files in a master data structure; when a reduce task completes, reduce worker atomically renames its temporary output file to the final output file, if the same reduce task is executed on multiple machines, multiple rename calls will be executed for the same final output file, we reply on the atomic rename op provided by underlying file system to guarantee that the final file system state contains just the data produced by 1 exec of the reduce task; the vast majority of our map and reduce operators are deterministic, and the fact that out semantics are equivalent to a sequential exec in this case makes it very easy for programmers to reason about their program's behavior, when map and/or reduce operators are non-deterministic, we provide weaker but still reasonable semantics, in the presence of non-deterministic operators, output of a particular reduce task *R1* is equivalent to output for *R1* produced by a sequential exec of the non-deterministic program, however, output for a different reduce task *R2* may correspond to output for *R2* produced by a different sequential exec of the non-deterministic program; consider map task *M* and reduce tasks *R1, R2*, let *e(Ri)* be the exec of *Ri* that committed(there is exactly 1 such exec), the weaker semantics arise because *e(R1)* may have read output produced by 1 exec of *M* and *e(R2)* may have read output produced by a different exec of *M*; 
**locality:** network bdwidth is a relatively scarce resource in our computing environment, we conserve network bdwidth by taking advantage of the fact that input data(managed by GFS) is stored on local disks of the machines that make up our cluster, GFS delivers each file into 64MB blocks, and stores several copies of each block(typically 3 copies) on different machines; MapReduce master takes location info of input files into account and attempts to schedule a map task on a machine that contains a replica of the corrsponding input data; failing that, it attempts to schedule a map task near a replica of that task's input data(such as on a worker machine that is on the same network switch as the machine containing the data); when running large MapReduce ops on a significant fraction of the workers in a cluster, most input data is read locally and consumes no network bdwidth; 
**task granularity:** we subdivide the map phase into *M* pieces and the reduce phase into *R* pieces, ideally, *M, R* should be much larger than #worker machines; having each worker perform many different tasks improves dynamic load balancing, and also speeds up recovery when a worker fails :the many map tasks it has completed can be spread out across all other worker machines; there are practical bounds on how large *M, R* can be in our implementation, since master must make *O(M+R)* scheduling decisions and keeps *O(M\*R)* state in mem(the constant factors for mem usage are small however :the *O(M\*R)* piece for the state consists of approximately 1 byte of data per map task-reduce task pair); furthermore, *R* is often constrained by users because output of each reduce task ends up in a separate output file, in practice, we tend to choose *M* so that each individual task is roughly 16-64MB of input data(so that the locality optimization described above is most effective), and we make *R* a small multiple of #worker machines we expect to use, we often perform MapReduce computations with *M=200000, R=5000, using 2000 worker machines*; 
**backup tasks:** one of the common causes that lengthens local time taken for a MapReduce op is a straggler :a machine that takes an unusually long time to complete one of the last few map or reduce tasks in the computation; straggler can arise for a whole host of reasons, for example, a machine with a bad disk may experience frequent correctable errors that slow its read perf from 30MB/sec to 1MB/sec, the cluster scheduling system may have scheduled other tasks on the machine, causing it to execute MapReduce code more slowly due to competition for cpu, mem, local disk, or network bdwidth, a recent problem we experienced was a bug in machine initialization code that caused processor caches to be disabled :computations on affected machines slowed down by over a factor of 100; we have a general mechanism to alleviate the problem of stragglers, when a MapReduce op is close to completion, master schedules backup exec of the remaining in-progress tasks, the task is marked as completed whenever either the primary or the backup exec completes, we have tuned this mechanism so that it typically increases the computational resources used by the op by no more than a few percent, we have found that this significally reduces the time to complete large MapReduce ops.<br \>
MapReduce 是一种用于处理和生成大型数据集的编程模型及其相关实现。用户指定一个 map 函数，用于处理键值对并生成一组即时键值对；以及一个 reduce 函数，用于合并与同一即时键关联的所有中间值。许多现实世界中的任务都可以用此模型来表达；
以这种函数式风格编写的程序会自动并行化，并在大型商用机器集群上执行。运行时系统负责处理输入数据的分区、在​​一组机器之间调度程序的执行、处理机器故障以及管理所需的机器间通信等细节。这使得没有任何并行和分布式系统经验的程序员也可以轻松地利用大型分布式系统的资源；
我们的 MapReduce 实现运行在由商用机器组成的大型集群上，并且具有高度的可扩展性：典型的 MapReduce 计算会在数千台机器上处理数 TB 的数据，程序员发现该系统易于使用：数百个 MapReduce 程序已经实现，并且每天在 Google 集群上执行 1000 多个 MapReduce 作业。
在过去的五年里，谷歌实现了数百个专用计算，用于处理大量原始数据，例如爬取的文档、网页请求日志等，并计算各种派生数据，例如倒排索引、网页文档图结构的各种实现、每个主机爬取的页面数摘要、特定日期内最频繁的查询集合等。大多数此类计算在概念上都很简单，然而，输入数据通常很大，计算必须分布在数十万台机器上才能在合理的时间内完成。如何并行化计算、分发数据和处理故障等问题，使得原本简单的计算变得难以理解，处理这些问题需要大量复杂的代码；
为了应对这种复杂性，我们设计了一种新的抽象，它使我们能够表达我们试图执行的简单计算，但将并行化、容错、数据分布和负载平衡等复杂的细节隐藏在一个库中。我们的抽象受到 Lisp 和许多其他函数式语言中 map、reduce 原语的启发。我们意识到，我们的大多数计算都涉及将 map 操作应用于输入中的每个逻辑记录，以计算一组中间键值对，然后对共享相同键的所有值应用 reduce 操作，以便适当地组合派生数据。我们使用带有用户指定 map、reduce 操作的函数式模型，使我们能够轻松地并行化大型计算，并使用 re-exec 作为主要的容错机制；
这项工作的主要贡献是一个简单而强大的接口，可以实现大规模计算的自动并行化和分布，并结合该接口的实现，在大型商用 PC 集群上实现了高性能。
### 编程模型
计算过程接受一组键值对作为输入，并生成一组键值对作为输出。MapReduce 库的用户将计算过程表达为两个函数：Map 和 Reduce。其中，用户编写的 Map 函数接受一个输入键值对，并生成一组中间键值对。MapReduce 库将与同一个中间键 *I* 关联的所有中间值组合在一起，并将它们传递给 Reduce 函数。Reduce 函数同样由用户编写，它接受一个中间键 *I* 和该键对应的一组值。Reduce 函数将这些值合并在一起，形成一个可能更小的集合。通常，每次 Reduce 调用只生成 0 或 1 个输出值。中间值通过迭代器传递给用户的 Reduce 函数，这使我们能够处理内存中无法容纳的庞大值列表。
**示例**：考虑计算大量文档中每个单词出现的次数的问题，用户编写的代码类似于以下伪代码：
```
map(String key, String value):
  // key: document time
  // value: document contents
  for each word w in value:
    EmitIntermediate(w, "1");
reduce(String key, Iterator values):
  // key: a word
  // values: a list of counts
  int result = 0;
  for each v in values:
    result += ParseInt(v);
  Emit(AsString(result));
```
这里，map 函数发出每个单词及其相关的出现次数（在这个简单的例子中只有 1），reduce 函数将针对特定单词发出的所有计数相加；
此外，用户编写代码，用输入和输出文件的名称以及可选的调整参数填充 mapreduce 规范对象，然后用户调用 MapReduce 函数，并将规范对象传递给它；用户的代码与 MapReduce 库（用 C++ 实现）链接在一起；
**类型：**尽管前面的伪代码是用字符串输入和输出编写的，但从概念上讲，用户提供的 map 和 reduce 函数具有关联类型：
```
map (k1,v1) ->list(k2,v2)
reduce (k2,list(v2)) ->list(v2)
```
也就是说，输入的键和值与输出的键和值来自不同的域，而且中间的键和值与输出的键和值来自相同的域；
我们的 C++ 实现将字符串传递给用户定义的函数，并将转换为字符串和适当类型的任务留给用户代码；
**更多示例（可以很容易地表示为 MapReduce 计算）：**分布式 grep -map 函数如果与提供的模式匹配，则会输出一行；reduce 函数是一个恒等函数，它只是将提供的中间数据复制到输出； url 访问频率计数 -map 函数处理网页请求日志并输出 <URL,1>，reduce 函数将同一 url 的所有值加在一起并发出 <URL,totalcount> 对；反向网页链接图 -map 函数输出 <target,source> 对，用于指向名为 source 的页面中找到的目标 url 的每个链接，reduce 函数连接与给定目标 url 关联的所有源 url 列表并发出对 <target,list(source)>；每个主机的术语向量 - 术语向量将文档或一组文档中出现的最重要单词总结为 <word,frequency> 对的列表，map 函数为每个输入文档发出一个 <hostname,termvector> 对（其中 hostname 从文档的 url 中提取），reduce 函数传递给定主机的所有每个文档术语向量，它将这些术语向量加在一起，丢弃不常见的术语，然后发出最终的 <hostname,termvector> 对；倒排索引 -map 函数解析每个文档并输出一个 <word,documentID> 对序列；reduce 函数接收给定单词的所有对，并对相应的文档 ID 进行排序，然后输出一个 <word,list(documentID)> 对。所有输出对的集合构成一个简单的倒排索引，通过参数化此计算可以很容易地跟踪单词的位置；分布式排序 -map 函数从每个记录中提取键并输出一个 <key,record> 对；reduce 函数输出所有不变的对。
### 实现
MapReduce 接口有多种不同的实现方式，正确的选择取决于具体环境。例如，一种实现可能适用于小型共享内存机器，另一种实现可能适用于大型 NUMA 多处理器机器，还有一种实现可能适用于更大规模的联网机器集合；
本节描述了一种针对 Google 广泛使用的计算环境的实现 - 通过交换式以太网连接在一起的大型商用 PC 集群，它：(i) 机器通常是运行 Linux 的双处理器 x86 处理器，每台机器具有 2-4GB 内存；(ii) 使用商用网络硬件 - 通常在机器级别上为 100 Mbps 或 1 Gbps，但整体二分带宽的平均值要低得多；(iii) 一个集群由数百或数千台机器组成，因此机器故障很常见；(iv) 存储由直接连接到各个机器的廉价 IDE 磁盘提供，内部开发的分布式文件系统用于管理存储在这些磁盘上的数据，文件系统用户使用复制来在不可靠的硬件之上提供可用性和可靠性；(v) 用户将作业提交给调度系统，每个作业由一组任务组成，并由调度程序映射到集群内的一组可用机器；
**exec 概述：** Map 调用通过自动将输入数据划分为一组 *M* 个分片分布在多台机器上，输入分片可由不同的机器并行处理，Reduce 调用通过使用分区函数（例如 *hash(key)* **mod** *R*）将中间键空间划分为 *R* 个部分来分布，#partition(*R*) 和分区函数由用户指定；[单击查看我们实现中的 MapReduce 操作的整体流程](./img/mapreduce.png)，当我们的用户程序调用 MapReduce 函数时，将发生以下操作序列（这里编号标签对应于以下列表中的数字）：（i）用户程序中的 MapReduce 库首先将输入文件拆分为 *M* 个块，每个块通常为 16MB 到 64MB（用户可以通过可选参数控制），然后在机器集群上启动该程序的多个副本，（ii）程序的其中一个副本是特殊的 - master，其余的是由 master 分配工作的 workers，有 *M* 个 map 任务和 *R* 个 reduce 任务需要分配，master 挑选空闲的 workers 并为每个 workers 分配一个 map 任务或一个 reduce 任务，（iii）被分配了 map 任务的 worker 读取相应输入拆分的内容，它从输入数据中解析出键值对并将每个对传递给用户定义的 Map 函数，Map 函数生成的中间键值对在内存中缓冲，（iv）定期将缓冲对写入本地磁盘，通过分区函数将其分区为 *R* 个区域，这些缓冲对在本地磁盘上的位置被传回给 master，master 负责将这些位置转发给 Reduce 工作进程；(v) 当 Reduce 工作进程收到 Master 通知这些位置后，它会使用 RPC 从 Map 工作进程的本地磁盘读取缓冲数据；当 Reduce 工作进程读取完所有中间数据后，它会根据中间键对数据进行排序，以便将相同键的所有出现位置分组在一起。排序是必要的，因为通常许多不同的键会映射到相同的 Reduce 任务。如果中间数据量太大，无法放入内存，则使用外部排序；(vi) Reduce 工作进程迭代排序后的中间数据，对于遇到的每个唯一中间键，它会将键和相应的中间值集合传递给用户的 Reduce 函数，Reduce 函数的输出将附加到此 Reduce 分区的最终输出文件中；(vii) 当所有 Map 任务和 Reduce 任务都完成后，Master 唤醒用户程序，此时，用户程序中的 MapReduce 调用返回给用户代码；
成功完成后，mapreduce exec 的输出可在 *R* 个输出文件中获取（每个 Reduce 任务一个，文件名由用户指定）。通常，用户不需要将这些 *R* 个输出文件合并为一个文件 - 他们通常会将这些文件作为输入传递给另一个 MapReduce 调用，或者在另一个能够处理分割成多个文件的输入的分布式应用程序中使用它们；
**master 数据结构**：master 保存多个数据结构，对于每个 Map 任务和 Reduce 任务，它存储状态（空闲、进行中或完成）以及工作机器的标识（对于非空闲任务）；master 是将中间文件区域的位置从 Map 任务传播到 Reduce 任务的管道，因此对于每个已完成的 Map 任务，master 存储 Map 任务生成的 *R* 个中间文件区域的位置和大小，在 Map 任务完成时接收对这些位置和大小信息的更新，并将这些信息逐步推送到正在执行 Reduce 任务的工作机器；
**容错性**：由于 MapReduce lib 旨在帮助使用数十万台机器处理大量数据，因此该 lib 必须能够优雅地容忍机器故障：（i）工作者故障：主服务器定期 ping 每个工作者，如果在一定时间内没有收到工作者的响应，主服务器将工作者标记为失败，工作者完成的任何映射任务都将重置回其初始空闲状态，因此有资格在其他工作者上进行调度，类似地，故障工作者上正在进行的任何映射任务或减少任务也将重置为空闲状态并有资格重新安排；已完成的映射任务在发生故障时重新执行，因为它们的输出存储在故障机器的本地磁盘上，因此无法访问，已完成的减少任务不需要重新执行，因为它们的输出存储在全局文件系统中；当一个 map 任务首先由 worker *A* 执行，然后由 worker *B* 执行（因为 *A* 失败了）时，所有正在执行 reduce 任务的 worker 都会收到该执行通知，任何尚未从 worker *A* 读取数据的 reduce 任务都将从 worker *B* 读取数据；MapReduce 能够应对大规模 worker 故障，例如，在 1 个 MapReduce 操作期间，正在运行的集群上的网络维护导致 80 台机器同时无法访问几分钟，MapReduce 主服务器只需重新​​执行无法访问的 worker 机器完成的工作，并继续前进，最终完成 MapReduce 操作；
(ii) 主服务器故障：很容易让主服务器写入上述主数据结构的定期检查点，如果主服务器任务死亡，则可以从最后一个检查点状态存储新的副本，但是，鉴于只有一个主服务器，它发生故障的可能性不大，因此，如果主服务器发生故障，我们当前的实现将中止 MapReduce 计算，客户端可以检查此情况并根据需要重试 MapReduce 操作；
(iii) 出现故障时的语义：当用户提供的 map 和 reduce 运算符是其输入值的确定性函数时，我们的分布式实现产生的输出与整个程序的无故障顺序 exec 产生的输出相同；我们依靠 map 和 reduce 任务输出的原子提交来实现这一属性，每个正在进行的任务将其输出写入私有临时文件，一个 reduce 任务生成 1 个这样的文件，一个 map 任务生成 *R* 个这样的文件（每个 reduce 任务 1 个），当 map 任务完成时，worker 向 master 发送一条消息，并在消息中包含 *R* 个临时文件的名称，如果 master 收到已完成 map 任务的完成消息，则忽略该消息，否则，它会在主数据结构中记录 *R* 个文件的名称；当一个 Reduce 任务完成时，Reduce Worker 会原子地将其临时输出文件重命名为最终输出文件，如果在多台机器上执行相同的 Reduce 任务，则会对相同的最终输出文件执行多次重命名调用，我们依靠底层文件系统提供的原子重命名操作来保证最终的文件系统状态只包含由 Reduce 任务的 1 次 exec 生成的数据；我们的绝大多数 map 和 reduce 运算符都是确定性的，并且在这种情况下，我们的语义相当于顺序执行，这使得程序员可以很容易地推断出他们的程序的行为，当 map 和/或 reduce 运算符是非确定性的时，我们提供较弱但仍然合理的语义，在存在非确定性运算符的情况下，特定 reduce 任务 *R1* 的输出相当于非确定性程序的顺序执行产生的 *R1* 的输出，但是，不同 reduce 任务 *R2* 的输出可能对应于非确定性程序的不同顺序执行产生的 *R2* 的输出；考虑 map 任务 *M* 和 reduce 任务 *R1, R2*，令 *e(Ri)* 为已提交的 *Ri* 的 exec（只有 1 个这样的 exec），由于 *e(R1)* 可能读取了 *M* 中某个 exec 的输出，而 *e(R2)* 可能读取了 *M* 中另一个 exec 的输出，因此语义较弱；
**局部性**：网络带宽在我们的计算环境中是一种相对稀缺的资源，我们利用输入数据（由 GFS 管理）存储在集群各机器本地磁盘上的优势来节省网络带宽。GFS 将每个文件拆分成 64MB 大小的块，并在不同的机器上存储每个块的多个副本（通常为 3 个）；MapReduce 主节点会考虑输入文件的位置信息，并尝试在包含相应输入数据副本的机器上调度 map 任务；如果失败，它会尝试将 map 任务安排在该任务输入数据的副本附近（例如，在与包含数据的机器位于同一网络交换机上的工作机器上）；当在集群中的大部分工作机器上运行大型 MapReduce 操作时，大多数输入数据都在本地读取，不消耗网络带宽；
**任务粒度**：我们将 map 阶段细分为 *M* 个部分，将 reduce 阶段细分为 *R* 个部分，理想情况下，*M, R* 应该远大于工作机器数量；让每个工作机器执行许多不同的任务可以改善动态负载平衡，并在工作机器发生故障时加快恢复速度：它完成的许多 map 任务可以分散到所有其他工作机器上；在我们的实现中，*M，R* 的大小存在实际限制，因为主节点必须做出 *O(M+R)* 的调度决策，并将 *O(M\*R)* 的状态保存在内存中（然而，影响内存使用的常数因素很小：每个 map 任务-reduce 任务对的状态 *O(M\*R)* 部分大约包含 1 个字节的数据）；此外，*R* 通常会受到用户的限制，因为每个 Reduce 任务的输出最终都会存储在单独的输出文件中。在实践中，我们倾向于选择 *M*，以便每个单独的任务大约有 16-64MB 的输入数据（这样上述局部性优化最为有效），并且我们将 *R* 设置为我们预期使用的工作机数量的较小倍数，我们经常使用 *M=200000、R=5000 和 2000 台工作机来执行 MapReduce 计算*；
**备份任务**：导致 MapReduce 操作本地时间延长的常见原因之一是落后者：一台机器花费异常长的时间来完成计算中最后几个 map 或 reduce 任务之一；落后者可能由于多种原因而出现，例如，磁盘损坏的机器可能会频繁出现可纠正的错误，导致其读取性能从 30MB/秒降低到 1MB/秒，集群调度系统可能已在该机器上安排了其他任务，导致其由于 CPU、内存、本地磁盘或网络带宽竞争而执行 MapReduce 代码的速度变慢，我们最近遇到的一个问题是机器初始化代码中的一个错误，导致处理器缓存被禁用：减慢了 100 倍以上的受影响机器上的计算速度；我们有一个通用机制来缓解落后者的问题，当 MapReduce 操作接近完成时，主进程会调度剩余正在进行的任务的备份执行，只要主进程或备份执行完成，该任务就会被标记为已完成，我们已经调整了此机制，因此它通常只会增加操作使用的计算资源不超过百分之几，我们发现这显著减少了完成大型 MapReduce 操作的时间。
### Spark :Cluster Computing with Working Sets
### [Zaharia10](https://www.usenix.org/legacy/event/hotcloud10/tech/full_papers/Zaharia.pdf)
MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters, however, most of these systems are built around an acylic data flow model that is not suitable for other popular applications, here we focus on one such class of applications :those that reuse a working set of data across multiple parallel ops, this includes many iterative ml algorithms as well as interactive data analysis tools, we propose a new framework called Spark that supports these applications while retaining the scalability and fault tolerance of MapReduce, to achieve these goals, Spark introduces an abstraction called RDDs -resilient distributed datasets, an rdd is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost, Spark can outperform Hadoop by 10x in iterative ml jobs and can be used to interactively query a 39GB dataset with sub-second response time. 
a new model of cluster computing has been widely popular, in which data-parallel computations are executed on clusters of unreliable machines by systems that automatically provide locality-aware scheduling, fault tolerance, and load balancing; MapReduce pioneered this model while system like Dryad, MapReudceMerge generalized types of data flows supported, these systems achieve their scalability and fault tolerance by providing a programming model where user creates acyclic data flow graphs to pass input data through a set of operators, this allows underlying system to manage scheduling and to react to faults without user intervention; 
while this data flow programming model is useful for a large class of applications, there are applications that cannot be expressed efficiently as acyclic data flows, here we focus on one such class of applications :those that reuse a working set of data across multiple parallel ops, this includes 2 use cases where we have seen Hadoop users report that MapReduce is deficient: (i)iterative jobs -many common ml algorithms apply a function repeatedly to the same dataset to optimize a param(such as through gradient descent), while each iteration can be expressed as a MapReduce/Dryad job, each job must reload the data from disk, incurring a significant perf penalty, (ii)interactive analytics -Hadoop is often used to run ad-hoc exploratory queries on large datasets, through sql interfaces such as Pig, Hive, ideally, a user would be able to load a dataset of interest into mem across a number of machines and query it repeatedly, however, with Hadoop each query incurs significant latency(tens of secs) because it runs as a separate MapReduce job and reads data from disk; 
here we present a new cluster computing framework called Spark, which supports applications with working sets while providing similar scalability and fault tolerance properties to MapReduce; 
the main abstraction in Spark is that of a rdd, which represents a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost; users can explicitly cache an rdd in mem across a set of machines and reuse it in multiple MapReduce-like parallel ops; rdds achieve fault tolerance through a notion of lineage :if a partition of an rdd is lost, the rdd has enough info about how it was was derived from other rdds to be able to rebuild just that partition; although rdds are not a general shared mem abstraction, trhey represent a sweet-spot bt expressively on the one hand and scalability and reliablity on the other hand, and we have found them well-suited for a variety of applications; 
Spark is implemented in Scala, a statically typed high-level programming lang for java vm, and exposes a functional programming interface similar to DryadLINQ, in addition, Spark can be used interactively from a modified version of the Scala interpreter, which allows user to define rdds, functions, vars, and classes, and use them in parallel ops on a cluster, we believe that Spark is the first system to allow an efficient, general-purpose programming lang to be used interactively to process large datasets on a cluster; 
although our implementation of Spark is still a prototype, early experience with the system is encouraging, we show that Spark can outperform Hadoop by 10x in iterative ml wkloads and can be used interactively to scan a 39GB dataset with sub-second latency.
### Programming Model
to use Spark, developers write a driver program that implements the high-level control flow of their application and launches various ops in parallel, Spark provides 2 main abstractions for parallel programming :rdds and parallel ops, on these datasets(invoked by passing a function to apply on a dataset), in addition, Spark supports 2 restricted types of a shared vars that can be used in functions running on the cluster, which we shall explain later; 
**rdds:** a rdd is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost, elements of an rdd need to exist in physical storage, instead, a handle to an rdd contains enough info to compute the rdd starting from data in reliable storage, this means rdds can always be reconstructed if nodes fail; in Spark, each rdd is represented by a Scala object, Spark lets programmers construct rdds in 4 ways including: (i)from a file in a shared file system such as HDFS -Hadoop distributed file system, (ii)by parallelizing a Scala collection(such as an array) in the driver program which means dividing it into a number of slices that will be sent to multiple nodes, (iii)by transforming an existing rdd, a dataset with elements of type A can be transformed into a dataset with elements of type B using an op called flatMap, which passes each element through a user-provided function of type *A->List\[B\]*(*faltMap has the same semantics as the map in MapReduce, but map is usually used to refer to a one2one function of type *A->B* in Scala), other transformations can be expressed using faltMap, including map(passes elements through a function of type *A->B*) and filter(pick elements matching a predicate), (iv)by changing the persistence of an existing rdd, by default, rdds are lazy and ephemeral i.e. partitions of a dataset are materialized on demand when they are used in a parallel op(such as by passing a block of a file through a map function), and are discarded from mem after use(*this is how distributed collections function in DryadLINQ), however, a user can alter the persistence of an rdd through 2 actions :the cache action leaves the dataset lazy but hints that it shoudl be kept in mem after the first time it is computed because it will be reused, the save action evaluates the dataset and writes it to a distributed file system such as hdfs the saved version is used in future ops on it; we note that our cache action is only a hint :if there is not enough mem in the cluster to cache all partitions of a dataset, Spark will recompute them when they are used, we chose this design so that Spark programs keep working(at reduced perf) if nodes fail or if a dataset is too big, this idea is loosely analogous to vm; we also plan to extend Spark to support other levels of persistence(such as in-mem replication across multiple nodes), our goal is to let users trade off bt the cost of storing an rdd, the speed of accessing it, the prob of losing part of it, and the cost of recomputing it; 
**parallel ops:** several parallel ops can be performed on rdds including: (i)reduce -combines dataset elements using an associative function to produce a result at the driver program, (ii)collect -sends all elements of the dataset to the driver program such as an easy way to update an array in parallel is to parallelize, map, and collect the array, (iii)foreach -passes each element through a user provided functions, this is only done for the side effects of the function(which might be to copy data to another system or to update a shared var as explained below); we note that Spark does not currently support a grouped reduce op as in MapReduce, results are only collected at 1 process(the driver, however, local functions are first performed at each node), we plan to support grouped reductions in the future using a shuffle transformation on distributed datasets, however, even using a single reducer is enough to express a variety of useful algorithms such as a recent paper on MapReduce for ml on multicore systems implemented 10 learning algorithms without supporting parallel reduction; 
**shared vars:** programmers invoke ops like map, filter, and reduce by passing closures(functions) to Spark, as is typical in functional programming, these clusters can refer to vars in the scope where they are created; normally, when Spark runs a closure on a worker node, these vars are copied to the worker, however, Spark also lets programmers create 2 restricted types of shared vars to support 2 simple but common usage patterns including: (i)broadcast vars -if a large read-only piece of data(such as a lookup table) is used in multiple parallel ops, it is preferable to distribute it to workers only once instead of packaging it with every closure, Spark lets programmers create a broadcast var object that wraps the value and ensures that it is only copied to each worker once, (ii)accumulators -these are vars that workers can only add to using an associative op, and that only the driver can read, they can be used to implement counters as in MapReduce and to provide a more imperative syntax for parallel sums, accumulators can be defined for any type that has an add op and a 0 value, due to their add-only semantics, they are easy to make fault-tolerant.
### Examples
we now show some sample Spark programs, note that we omit var types because Scala supports type inference; 
**text search:** suppose that we wish to count lines containing errors in a large log file stored in hdfs, this can be implemented by starting with a file dataset object as follows:
```
val file = spark.textFile("hdfs://...")
val errs = file.filter(_.contains("ERROR"))
val ones = errs.map(_ => 1)
val count = ones.reduce(_+_)
```
we first create a distributed dataset called file that represents the hdfs file as a collection of lines, we transform this dataset to create the set of lines containing ERROR(errs), and then amp each line a 1 and add up these 1s using reduce, the arguments to filter, map, and reduce are Scala syntax for function literals; note that errs and ones are lazy rdds that are never materialized, instead, when reduce is called, each worker node scans input blocks in a streaming manner to evaluate ones, adds these to perform a local reduce, and sends its local count to the driver, when used with lazy datasets in this manner, Spark closely emulates MapReduce; where Spark differs from other frameworks is that it can make some of the intermediate datasets persist across operations, such as if wanted to reuse the errs dataset, we could create a cached rdd from it as follows:
```
val cachedErrs = errs.cache()
```
we would now be able to invoke parallel ops on cachedErrs or on datasets derived from it as usual, but nodes would cache partitions of cachedErrs in mem after the first time they compute them, greatly speeding up subsequent ops on it; 
**logistic regression:** the following program implements logistic regression, an iterative classification algorithm that attempts to find hyperplane *w* that best separates 2 sets of points, the algorithm performs gradient descent :it starts *w* at a random value, and on each iteration, it sums a function of *w* over the data to move *w* in a direction that improves it, it hence benifits from caching the data in mem across iterations:
```
// read points from a text file and cache them
val points = spark.textFile(...).map(parsePoint).cache()
// initiate w to random d-dim vector
var w = Vector.random(D)
// run multiple iterations to update w
for (i <- 1 to ITERATIONS) {
  val grad = spark.accumulator(new Vector(D))
  for (p <- points) { // runs in parallel
    val s = (1/(1+exp(-p.y*(w dot p.x)))-1*p.y)
    grad += s*p.x
  }
  w -= grad.value
}
```
first although we create an rdd called points, we process it by running a for loop over it, the for keyword in Scala is syntactic sugar for invoking the foreach method of a collection with the loop body as a closure i.e. the code *for(p <- points){body}* is equivalent to *points.foreach(p => {body})*, hence we are invoking Spark's parallel foreach op, second to sum up the gradient, we use an accumulator var called gradient(with a value of type Vector), note that the loop adds to gradient using an overloaded += operator, the combination of accumulators and for syntax allows Spark porgrams to look much like imperative serial programs, indeed, this example differs from a serial version of logistic regression in only 3 lines; 
**ALS -alternating least squares:** used for collaborative filtering problems such as predicting users' ratings for movies that they have not seen based on their movie rating history(as in the Netflix Challenge), unlike our previous examples, als is cpu-intensive rather than data-intensive; we briefly sketch als -suppose that we wanted to predict the ratings of *u* users for *m* movies and that we had a partially filled matrix *R* containing known ratings for some user-movie pairs, als models *R* as the product of 2 matrices *M, U* of dims *mtimesk, ktimesu* respectively i.e. each user and each movie has a *k*-dim feature vector describing its characteristics, and a user's rating for a movie is the dot product of its feature vector and the movie's, als solves for *M, U* using the known ratings and then computes *MtimesU* to predict the unknown ones, this is done using the following iterative process: 1.initialize *M* to a random value, 2.optimize *U* given *M* to minimize error on *R*, 3.optimize *M* given *U* to minimize error on *R*, 4.repeat steps2&3 until convergence; als can be parallelized by updating different users/movies on each node in steps2&3, however, because all of the steps use *R*, it is helpful to make *R* a broadcast var so that it does not get re-sent to each node on each step, note that we parallelize the collection 0 until u(a Scala range object) and collect it to update each array:
```
val Rb = spark.broadcast(R)
for (i <- 1 to ITERATIONS) {
  U = spark.parallelize(0 until u).map(j => updateUser(j, Rb, M)).collect()
  M = spark.parallelize(0 until m).map(j => updateUser(j, Rb, U)).collect()
}
```
### Implementation
Spark is built on top of Mesos, a cluster os that lets multiple parallel applications share a cluster in a fine-grained manner and provides an api for applications to launch tasks on a cluster, this allows Spark to run alongside existing cluster computing frameworks such as Mesos ports of Hadoop and MPI, and share data with them, in addition, building on Mesos greatly reduced the programming effort that had to go into Spark; 
the core of Spark is the implementation of rdds, for example, suppose that we define a cached dataset called cachedErrs representing error msgs in a log file, and that we count its elements using map and reduce as in section text search, [these datasets will be stored as a chain of objects capturing the lineage of each rdd, click to view](./img/resilient-distributed-datasets-linear-chain.png), each dataset object contains a pointer to its parent and info about how the parent was transformed; 
internally, each rdd object implementats the same simple interface, which consists of 3 ops including: getPartitions -which returns a list of partition IDs, getIterator(partition) -which iterates over a partition, getPreferredLocations(partition) -which is used for task scheduling to achieve data locality; when a parallel op is invoked on a dataset, Spark creates a task to process each partition of the dataset and sends these tasks to worker nodes, we try to send each task to one of its preferred locations using a technique called delay scheduling, once launched on a worker, each task calls getIterator to start reading its partition; 
the different types of rdds differ only in how they implement the rdd interface, for example, for a HDFS-Textfile, the partitions are block IDs in hdfs, their preferred locations are the block locations, and getIterator opens a stream to read a block, vs., in a MappedDataset, the partitions and preferred locations are the same as for the parent, but the iterator applies the map function to elements of the parent, vs., in a CachedDataset, the getIterator method looks for a locally cached copy of a transformed partition, and each partition's preferred locations start out equal to parent's preferred locations, but get updated after the partition is cached on some node to prefer using that node, this design makes faults easy to handle :if a node fails, its partitions are re-read from their parent datasets andd eventually cached on other nodes; 
shipping tasks to worker requires shipping closures to them -both the closures used to define a distributed dataset, and closures passed to ops such as reduce, to achieve this, we rely on the fact that Scala closures are java objects and can be serialized using java serialization, this is a feature of Scala that makes it relatively straightforward to send a computation to another machine; Scala's built-in closure implementation is not ideal, however, because we have found cases where a closure object references vars in the closure's outer scope that are not actually used in its body, we have filed a bug report about this, but in the meantime, we have solved the issue by performing a static analysis of closure classes' bytecode to detect these unused vars and set the corresponding fields in the closure object to null; 
**shared vars:** the 2 types of shared vars in Spark, broadcast vars and accumulators, are implemented using classes with custom serialization formats, when one creates a broadcast var b with a value v, v is saved to a file in a shared file system, the serialized form of b is a path to this file, when b's value is required on a worker node, Spark first checks whether v is in a local cache, and reads it from the file system if it is not, we initially used hdfs to broadcast vars, but we are developing a more efficient streaming broadcast system; accumulators are implemented using a different serialization trick, each accumulator is given a unique ID when it is created, when the accumulator is saved, its serialized form contains its ID and the 0 value for its type, on the workers, a separate copy of the accumulator is created for each thread that runs a task using thread-local vars, and is set to 0 when a task begins, after each task runs, worker sends a msg to the driver program containing the updates it made to various accumulators, the driver applies updates from each partition of each op only once to prevent double-counting when tasks are re-executed due to failures; 
**interpreter integration:** here we only sketch how we have integrated Spark into Scala interpreter, the Scala interpreter normally operates by compiling a class for each line typed by user, this class includes a singleton object that contains vars or functions on that line and runs the line's code in its constructor, for example, if user types *var x=5* followed by *println(x)*, the interpreter defines a class(say *Linel*) containing *x* and causes the second line to compile to *print(Linel.getInstance().x)*, these classes are loaded into jvm to run each line, to make the interpreter work with Spark, we made 2 changes including: (i)we made the interpreter output the classes it defines to a shared filesystem, from which they can be loaded by workers using a custom java class loader, (ii)we changed the generated code so that the singleton object for each line references the singleton objects for previous lines directly rather than going through the static *getInstance* methods, this allows closures to capture current state of the singletons they reference whenever they are serialized to be sent to a worker, if we had not done this, then updates to the singleton objects(such as a line setting *x=7* above) would not propagate to workers.<br \>
MapReduce 及其变体在商用集群上实现大规模数据密集型应用方面取得了巨大成功，然而，这些系统大多基于循环数据流模型构建，并不适用于其他主流应用。本文我们将重点关注一类应用：那些在多个并行操作中重用工作集的应用，这包括许多迭代机器学习算法以及交互式数据分析工具。我们提出了一个名为 Spark 的新框架，它在保留 MapReduce 可扩展性和容错性的同时，支持这些应用。为了实现这些目标，Spark 引入了一种称为 RDD 的抽象概念——弹性分布式数据集。RDD 是分布在一组机器上的只读对象集合，如果分区丢失，可以重建。Spark 的性能在以下方面比 Hadoop 高出 10 倍：迭代机器学习作业，可用于以亚秒级响应时间交互式查询 39GB 数据集。
一种新的集群计算模型已广受欢迎，其中数据并行计算由自动提供局部感知调度、容错和负载平衡的系统在不可靠机器集群上执行；MapReduce 开创了这种模型，而像 Dryad、MapReudceMerge 这样的系统支持通用类型的数据流，这些系统通过提供一种编程模型来实现其可扩展性和容错性。在这种模型中，用户创建非循环数据流图，将输入数据传递到一组运算符，这使得底层系统能够管理调度并对故障做出反应，而无需用户干预；
虽然这种数据流编程模型对于一大类应用程序很有用，但有些应用程序无法有效地表示为非循环数据流，这里我们重点介绍其中一类应用程序：那些在多个并行操作中重用工作数据集的应用程序，这包括两个用例，我们已经看到 Hadoop 用户报告 MapReduce 存在缺陷：（i）迭代作业 - 许多常见的机器学习算法将函数重复应用于同一数据集以优化参数（例如通过梯度下降），而每次迭代都可以表示为 MapReduce / Dryad 作业，每个作业都必须从磁盘重新加载数据，从而导致显着的性能损失，（ii）交互式分析 - Hadoop 通常用于通过 Pig，Hive 等 SQL 接口对大型数据集运行临时探索性查询，理想情况下，用户能够将感兴趣的数据集加载到多台机器的内存中并重复查询，但是，使用 Hadoop 时，每个查询都会产生显着的延迟（数十秒），因为它作为单独的 MapReduce 作业运行，并且从磁盘读取数据；
我们在此介绍一种名为 Spark 的新型集群计算框架，它支持具有工作集的应用程序，同时提供与 MapReduce 类似的可扩展性和容错特性；
Spark 中的主要抽象是 RDD，它表示跨一组机器分区的只读对象集合，如果分区丢失，可以重建；用户可以将 RDD 显式缓存在跨一组机器的内存中，并在多个类似 MapReduce 的并行操作中重用它；RDD 通过血统概念实现容错：如果 RDD 的某个分区丢失，该 RDD 拥有足够的信息，说明它是如何从其他 RDD 派生出来的，从而能够重建该分区；虽然 RDD 不是通用的共享内存抽象，但它们一方面在表达能力上达到了最佳平衡，另一方面在可扩展性和可靠性方面也达到了最佳平衡，我们发现它们非常适合各种应用程序；
Spark 是用 Scala（一种用于 Java 虚拟机的静态类型高级编程语言）实现的，并公开了类似于 DryadLINQ 的函数式编程接口。此外，Spark 可以通过 Scala 解释器的修改版本以交互方式使用，该解释器允许用户定义 RDD、函数、变量和类，并在集群上的并行操作中使用它们。我们相信，Spark 是第一个允许以交互方式使用高效、通用编程语言在集群上处理大型数据集的系统；虽然我们的 Spark 实现仍处于原型阶段，但该系统的早期经验令人鼓舞，我们表明 Spark 在迭代机器学习负载方面的表现比 Hadoop 高出 10 倍，并且可以交互使用以亚秒级延迟扫描一个 39GB 数据集。
### 编程模型
要使用 Spark，开发者需要编写一个驱动程序来实现应用程序的高级控制流，并并行启动各种操作。Spark 为并行编程提供了两种主要抽象：RDD 和并行操作。针对这些数据集（通过传递一个函数来调用），Spark 支持两种受限类型的共享变量，这些变量可用于集群上运行的函数，我们将在后面解释；
**RDD**：RDD 是跨一组机器分区的只读对象集合，如果分区丢失，可以重建。RDD 的元素需要存在于物理存储中，而 RDD 的句柄包含足够的信息，可以从可靠存储中的数据开始计算 RDD，这意味着即使节点发生故障，RDD 也可以重建；在 Spark 中，每个 rdd 都由一个 Scala 对象表示，Spark 允许程序员以 4 种方式构建 rdd，包括：（i）从共享文件系统（如 HDFS -Hadoop 分布式文件系统）中的文件构建，（ii）通过在驱动程序中并行化 Scala 集合（如数组），这意味着将其分成多个切片，然后发送到多个节点，（iii）通过转换现有的 rdd，可以使用名为 flatMap 的操作将具有类型 A 元素的数据集转换为具有类型 B 元素的数据集，该操作将每个元素传递给用户提供的类型为 *A->List\[B\]* 的函数（*faltMap 具有与 MapReduce 中的 map 相同的语义，但 map 通常用于指 Scala 中类型为 *A->B* 的一对一函数），其他转换可以使用 faltMap 表示，包括 map（通过类型为 *A->B* 的函数传递元素）和 filter（选择与谓词匹配的元素），（iv）通过更改现有rdd，默认情况下，rdds是惰性和短暂的，即数据集的分区在并行操作中使用时按需实现（例如通过 map 函数传递文件块），并在使用后从 mem 中丢弃（*这是 DryadLINQ 中分布式集合的功能），但是，用户可以通过 2 个操作改变 rdd 的持久性：缓存操作使数据集保持惰性，但提示它应该在第一次计算后保留在 mem 中，因为它将被重用，保存操作评估数据集并将其写入分布式文件系统（如 hdfs），保存的版本将在未来的操作中使用；我们注意到，我们的缓存操作只是一个提示：如果集群中没有足够的内存来缓存数据集的所有分区，Spark 会在使用它们时重新计算它们，我们选择这种设计是为了让 Spark 程序在节点发生故障或数据集过大时继续工作（以降低的性能），这个想法与虚拟机大致类似；我们还计划扩展 Spark 以支持其他级别的持久性（例如跨多个节点的内存复制），我们的目标是让用户在存储 RDD 的成本、访问速度、丢失部分数据的可能性以及重新计算的成本之间进行权衡；
**并行操作：**可以在 rdds 上执行多个并行操作，包括：（i）reduce - 使用关联函数组合数据集元素以在驱动程序中产生结果，（ii）collect - 将数据集的所有元素发送到驱动程序，例如并行更新数组的简单方法是并行化、映射和收集数组，（iii）foreach - 通过用户提供的函数传递每个元素，这只是为了函数的副作用而做的（可能是将数据复制到另一个系统或更新共享变量，如下所述）；我们注意到，Spark 目前不支持像 MapReduce 那样的分组 Reduce 操作，结果仅在一个进程中收集（然而，驱动程序会首先在每个节点上执行本地函数），我们计划在未来使用对分布式数据集进行 shuffle 转换来支持分组 Reduce。然而，即使使用单个 Reducer 也足以表达各种有用的算法，例如最近一篇关于在多核系统上进行机器学习的 MapReduce 的论文，它实现了 10 种不支持并行 Reduce 的学习算法；**共享变量**：程序员通过将闭包（函数）传递给 Spark 来调用 map、filter 和 reduce 等操作，这在函数式编程中很常见，这些集群可以在创建它们的范围内引用变量；通常，当 Spark 在工作节点上运行闭包时，这些变量会被复制到工作节点，但是，Spark 还允许程序员创建 2 种受限类型的共享变量，以支持 2 种简单但常见的使用模式，包括：（i）广播变量 - 如果在多个并行操作中使用大量只读数据（例如查找表），则最好将其仅分发给工作节点一次，而不是将其与每个闭包一起打包，Spark 允许程序员创建一个广播变量对象来包装该值并确保它仅复制到每个工作节点一次，（ii）累加器 - 这些变量只能由工作线程使用关联操作进行加法运算，并且只有驱动程序可以读取。它们可以像 MapReduce 中一样用于实现计数器，并为并行求和提供更命令式的语法。累加器可以定义为任何具有加法运算和 0 值的类型。由于其仅加法语义，它们易于容错。
### 示例
我们现在展示一些 Spark 示例程序，请注意，我们省略了变量类型，因为 Scala 支持类型推断；
**文本搜索：**假设我们希望统计存储在 HDFS 中的大型日志文件中包含错误的行数，可以通过以下文件数据集对象来实现：
```
val file = spark.textFile("hdfs://...")
val errs = file.filter(_.contains("ERROR"))
val ones = errs.map(_ => 1)
val count = ones.reduce(_+_)
```
我们首先创建一个名为 file 的分布式数据集，它将 HDFS 文件表示为行的集合，我们将该数据集转换为包含 ERROR(errs) 的行集，然后对每行进行 amp，并使用 reduce 将这些 1 加总。filter、map 和 reduce 的参数是 Scala 函数字面量语法；请注意，errs 和 ones 是惰性 rdd，永远不会被物化。相反，当调用 reduce 时，每个工作节点会以流式方式扫描输入块以评估 ones，然后添加这些块以执行本地 reduce，并将其本地计数发送给驱动程序。当以这种方式与惰性数据集一起使用时，Spark 会紧密模拟 MapReduce；Spark 与其他框架的不同之处在于，它可以使一些中间数据集在操作之间持久化，例如，如果想要重用 errs 数据集，我们可以从中创建一个缓存的 rdd，如下所示：
```
val cachedErrs = errs.cache()
```
我们现在可以像往常一样对 cachedErrs 或从其派生的数据集调用并行操作，但节点会在第一次计算后将 cachedErrs 的分区缓存在内存中，从而大大加快后续操作的速度；
**逻辑回归**：以下程序实现了逻辑回归，这是一种迭代分类算法，它试图找到能够最佳地区分两组点的超平面 *w*。该算法执行梯度下降：它以随机值开始 *w*，并在每次迭代中，对数据求和 *w* 函数，以使 *w* 朝着改进的方向移动，因此在迭代过程中将数据缓存在内存中是有益的：
```
// 从文本文件中读取点并缓存
val points = spark.textFile(...).map(parsePoint).cache()
// 将 w 初始化为随机 d 维向量
var w = Vector.random(D)
// 运行多次迭代以更新 w
for (i <- 1 to ITERATIONS) {
val grad = spark.accumulator(new Vector(D))
for (p <- points) { // 并行运行
val s = (1/(1+exp(-p.y*(w dot p.x)))-1*p.y)
grad += s*p.x
}
w -= grad.value
}
```
首先，虽然我们创建了一个名为 points 的 rdd，但我们通过对它运行 for 循环来处理它。Scala 中的 for 关键字是语法糖，用于调用集合的 foreach 方法，并将循环体作为闭包，即代码 *for(p <- points){body}* 等同于 *points.foreach(p => {body})*，因此我们调用了 Spark 的并行 foreach 操作。其次，为了计算梯度，我们使用了一个名为 Gradient 的累加器变量（其值类型为 Vector）。请注意，循环使用重载的 += 运算符将 Gradient 相加。累加器和 for 语法的组合使 Spark 程序看起来像命令式串行程序。事实上，这个例子与串行版本的逻辑回归只有 3 个不同之处。行；
**ALS - 交替最小二乘法**：用于协同过滤问题，例如根据用户的电影评分历史预测他们未看过的电影的评分（如在 Netflix 挑战赛中），与我们之前的示例不同，ALS 是 CPU 密集型的，而不是数据密集型的；我们简要概述一下 als - 假设我们想要预测 *u* 个用户对 *m* 部电影的评分，并且我们有一个部分填充的矩阵 *R*，其中包含某些用户-电影对的已知评分，als 模型 *R* 是两个矩阵 *M、U* 的乘积，其维度分别为 *mtimesk、ktimesu*，即每个用户和每部电影都有一个 *k* 维特征向量描述其特征，用户对电影的评分是其特征向量与电影特征的点积，als 使用已知评分求解 *M、U*，然后计算 *MtimesU* 来预测未知评分，这是使用以下迭代过程完成的：1.将 *M* 初始化为随机值，2.给定 *M* 优化 *U* 以最小化 *R* 上的误差，3.给定 *U* 优化 *M* 以最小化 *R* 上的误差，4.重复步骤 2 和 3 直到收敛； als 可以通过在步骤 2 和 3 中更新每个节点上的不同用户/电影来实现并行化，但是，由于所有步骤都使用 *R*，因此将 *R* 设为广播变量会很有帮助，这样它就不会在每个步骤中重新发送到每个节点。请注意，我们并行化了集合 0 直到 u（一个 Scala 范围对象），并将其收集起来以更新每个数组：
```
val Rb = spark.broadcast(R)
for (i <- 1 to ITERATIONS) {
U = spark.parallelize(0 until u).map(j => updateUser(j, Rb, M)).collect()
M = spark.parallelize(0 until m).map(j => updateUser(j, Rb, U)).collect()
}
```
### 实现
Spark 构建于 Mesos 之上。Mesos 是一个集群操作系统，允许多个并行应用程序以细粒度的方式共享集群，并为应用程序提供在集群上启动任务的 API。这使得 Spark 可以与现有的集群计算框架（例如 Mesos 版本的 Hadoop 和 MPI）一起运行，并与它们共享数据。此外，基于 Mesos 构建大大减少了 Spark 的编程工作量；
Spark 的核心是 RDD 的实现，例如，假设我们定义了一个名为 cachedErrs 的缓存数据集，用于记录日志文件中的错误信息，并使用 map 和 reduce 函数对其元素进行计数，就像在文本搜索部分中提到的那样。[这些数据集将存储为一个对象链，用于捕获每个 RDD 的沿袭关系，点击查看](./img/resilient-distributed-datasets-linear-chain.png)，每个数据集对象都包含一个指向其父级的指针以及父级如何转换的信息；
在内部，每个 RDD 对象都实现了相同的简单接口，该接口包含 3 个操作：getPartitions - 返回分区 ID 列表，getIterator(partition) - 迭代分区，getPreferredLocations(partition) - 用于任务调度以实现数据本地化；当对数据集调用并行操作时，Spark 会创建一个任务来处理数据集的每个分区，并将这些任务发送到工作节点。我们会尝试使用一种称为延迟调度的技术将每个任务发送到其首选位置之一。一旦在工作节点上启动，每个任务都会调用 getIterator 来开始读取其分区；
不同类型的 rdd 仅在于它们实现 rdd 接口的方式不同。例如，对于 HDFS-Textfile，分区是 hdfs 中的块 ID，它们的首选位置是块位置，getIterator 打开一个流来读取块；而在 MappedDataset 中，分区和首选位置与父级相同，但迭代器将 map 函数应用于父级的元素；而在 CachedDataset 中，getIterator 方法查找转换后分区的本地缓存副本，并且每个分区的首选位置最初等于父级的首选位置，但在分区缓存到某个节点后会更新为优先使用该节点，这种设计使故障易于处理：如果某个节点发生故障，则其分区将从其父数据集中重新读取，并最终缓存在其他节点上；
将任务发送给 worker 需要将闭包发送给他们——用于定义分布式数据集的闭包，以及传递给诸如 reduce 之类的操作的闭包。为了实现这一点，我们依赖于 Scala 闭包是 Java 对象，并且可以使用 Java 序列化进行序列化，这是 Scala 的一个特性，使得将计算发送到另一台机器变得相对简单；然而，Scala 的内置闭包实现并不理想，因为我们发现闭包对象在其外部作用域中引用了在其主体中实际上并未使用的变量的情况，我们已经提交了有关此问题的错误报告，但与此同时，我们通过对闭包类的字节码进行静态分析来检测这些未使用的变量，并将闭包对象中相应的字段设置为 null，从而解决了该问题；
**共享变量**：Spark 中的两种共享变量，广播变量和累加器，是使用具有自定义序列化格式的类实现的，当创建一个值为 v 的广播变量 b 时，v 将保存到共享文件系统中的文件中，b 的序列化形式是该文件的路径，当工作节点需要 b 的值时，Spark 首先检查 v 是否在本地缓存中，如果不在，则从文件系统中读取，我们最初使用 hdfs 来广播变量，但我们正在开发更高效的流广播系统；累加器使用不同的序列化技巧实现，每个累加器在创建时都会被赋予一个唯一的 ID；累加器保存时，其序列化形式包含其 ID 和对应的类型的 0 值；在工作线程上，会为每个使用线程本地变量运行任务的线程创建一份累加器副本，并在任务开始时将其设置为 0；每个任务运行后，工作线程会向驱动程序发送一条消息，其中包含对各个累加器所做的更新；驱动程序只会将每个操作的每个分区的更新应用一次，以防止在任务因故障重新执行时出现重复计算；**解释器集成**：这里我们仅概述了如何将 Spark 集成到 Scala 解释器中。Scala 解释器通常为用户输入的每一行代码编译一个类，该类包含一个包含该行代码的变量或函数的单例对象，并在其构造函数中运行该行代码，例如，如果用户输入 *var x=5* 后跟 *println(x)*，解释器会定义一个包含 *x* 的类（比如 *Linel*），并导致第二行编译为 *print(Linel.getInstance().x)*，这些类会加载到 jvm 中运行每一行，为了使解释器能够与 Spark 协同工作，我们做了 2 处更改，包括：（i）我们让解释器将其定义的类输出到共享文件系统，工作进程可以使用自定义 Java 类加载器从中加载这些类，（ii）我们更改了生成的代码，使每行的单例对象直接引用前几行的单例对象，而不是通过静态 *getInstance* 方法，这允许闭包在序列化发送给工作进程时捕获它们引用的单例的当前状态，如果我们没有这样做，那么对单例对象的更新（例如上面的行设置 *x=7*）将不会传播给工作进程。
### Scaling Distributed Machine Learning with the Parameter Server
### [Li14](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf)
we propose a param server framework for distributed ml problems, both data and wkloads are distributed over worker nodes, while the server nodes maintain globally shared params, represented as dense or sparse vectors and matrices, the framework manages asynchronous data comm bt nodes, and supports flexible consistency models, elastic scalability, and continuous fault tolerance; to demonstrate scalability of the proposed framework we show experimental results on petabytes of real data with billions of examples and params on problems ranging from Sparse Logistic Regression to Latent Dirichlet Allocation and Distributed Sketching. 
distributed optimization and inference is becoming a prerequisite for solving large scale ml problems, at scale, no single machine can solve these problems sufficiently rapidly, due to growth of data and the resulting model complexity, often manifesting itself in an increased number of params, however, implementing an efficient distributed algorithms is not easy, both intensive computational wkloads and the volumne of data comm demand careful system design; 
realistic quantities of training data can range bt 1TB-1PB, this allows one to create powerful and complex models with 10^9-10^12 params, these models are often shared globally by all worker nodes, which must frequently accesses shared params as they perform computation to refine it, sharing imposes 3 challenges including: accessing the params requires an enormous amount of network bdwidth; many ml algorithms are sequential, the resulting barriers hurt perf when cost of synchronization and machine latency is high; at scale, fault tolerance is critical, learning tasks are often performed in a cloud environment where machines can be unreliable and jobs can be preempted; to illustrate the last point, we collected all job logs for a 3-month period from one cluster at a large internet company, [click for stats of batch ml tasks serving a production environment](./img/statistics-of-machine-learning-jobs.png), here task failure is mostly due to being preempted or losing machines without necessary fault tolerance mechanisms; unlike in many research settings where jobs run exclusively on a cluster without contention, fault tolerance is a necessity in real world deployments; 
**contributions:** since its introduction, the param server framework has proliferated in academia and industry, here we describe a 3rd-gen open source implementation of a param server that focuses on the systems aspects of distributed inference, it confers 2 advantages to developers :first by factoring out commonly required components of ml systems, it enables application-specific code to remain concise, at the same time, as a shared platform to target for sysytems-level optimizations, it provides a robust versatile and high-perf implementation capable of handling a diverse array of algorithms from sparse logistic regression to topic models and distributed sketching; our design decisions were guided by wkloads found in real systems, our param server provides 5 key features including: (i)efficient comm -the asynchronous comm models does not block computation(unless requested), it is optimized for ml tasks to reduce network traffic and overhead, (ii)flexible consistency models -relaxed consistency further hides synchronization cost and latency, we allow the algorithm designer to balance algorithmic convergence rate and system efficiency, the best trade-off depends on data, algorithm, and hw, (iii)elastic scalability -new nodes can be added without restarting the running framework, (iv)fault tolerance and durability -recovery from and repair of non-catastrophic machine failures within 1sec, without interrupting computation, vector clocks ensure well-defined behavior after network partition and failure, (v)ease of use -the globally shared params are represented as (potentially sparse) vectors and matrices to facilitate dev of ml applications, the linear algebra data types come with high-perf multithreaded libs; the novelty of proposed system lies in the synergy achieved by picking the right systems techinuqes, adapting them to ml algorithms, and modifying the ml algorithms to be more systems-friendly, in particular, we can relax a number of otherwise hard systems constraints since the associated ml algorithms are quite tolerant to perturbations, the consequence is the first general purpose ml system capable of scaling to industrial scale sizes; 
**engineering challenges:** when solving distributed data analysis problems, the issue of reading and updating params shared bt different worker nodes is ubiquitous, the param server framework provides an efficient mechanism for aggregating and synchronizing model params and stats bt workers, each param server node maintains only a part of the params, and each worker node typically requires only a subset of these params when operating, 2 key challenges arise in constructing a high perf param server system: (i) comm: while params could be updated as key-value pairs in a conventional datastore, using this abstraction natively is inefficient :values are typically small(floats or ints), and the overhead of sending each update as a key value op is high; our insight to improve this situation comes from the observation that many learning algorithms represent params as structured mathematical objects such as vectors, matrices, or tensors, at each logical time(or an iteration) typically a part of the object is updated, i.e. workers usually send a segment of a vector, or an entire row of the matrix, this provides an opportunity to automatically batch both comm of updates and their processing on the param server, and allows the consistency tracking to be implemented efficiently; 
(ii) fault tolerance: as noted earlier, is critical at scale, and for efficient op it must not require a full restart of a long-running computation, live replication of params bt servers supports hot failover, failover and self-repair in turn support dynamic scaling by treating machine removal or addition as failure or repair respectively; [click for an overview of the scale of the largest supervised and unsupervised ml experiments performed on a number of systems](./img/comparison-of-the-public-largest-machine-learning-experiments-each-system-performed.png), when possible, we confirmed the scaling limits with authors of each of these systems(data current as of 4/2014), as is evident, we are able to cover orders of magnitude more data on orders of magnitude more processors than any other published system, furthermore, [click for an overview of main characteristics of several ml systems](./img/attributes-of-distributed-data-analysis-systems.png), our param server offers the greatest degree of flexibility in terms of consistency, it is the only system offering continuous fault tolerance, its native data types make it particularly friendly for data analysis.
### Machine Learning
ml systems are widely used in web search, spam detection, recommendation systems, computational advertising, and document analysis, these systems automatically learn models from examples, termed training data, and typically consist of 3 components including: feature extraction, objective function, and learning; here, feature extraction processes raw data such as documents, images, and user query logs, to obtain feature vectors, where each feature captures an attribute of the training data, preprocessing can be executed efficiently by existing frameworks such as MapReduce; 
**goals:** goal of many ml algorithms can be expressed via a object function, this function captures properties of the learned model, such as low error in the case of classifying emails into ham and spam, how well the data is explained in the context of estimating topics in documents, or a concise summary of counts in the context of sketching data; the learning algorithm typically minimizes this objective function to obtain the model, in general, there is no closed-form solution, instead, learning starts from an initial model, it iteratively refines this model by processing training data, possibly multiple times, to approach the solution, it stops when a (near) optimal solution is found or the model is considered to be converged; the training data may be extremely large, for example, a larget internet company using 1yr of an ad impression log to train an ad click predictor would have trillions of trainig examples, each training example is typically represented as a possibly very high-dim feature vector, hence training data may consist of trillions of trillion-length feature vectors, iteratively processing such large scale data requires enormous computing and bdwidth resources, moreover, billions of new ad impressions may arrive daily, adding this data into the system often improves both prediction accuracy and coverage, but it also requires the learning algorithm to run daily, possibly in real time. efficient exec of these algorithms is the main focus; to motivate design decisions in our system, next we briefly outline the 2 widely used ml technologies that we will use to demonstrate the efficacy of our param server; 
**risk minimization:** the most intuitive variant of ml problems is that of risk minimization, the risk is roughly a measure of prediction error such as if we were to predict tomorrow's stock price the risk might be the deviation bt prediction and the actual value of stock; training data consists of *n* examples, *xi* is the *i*-th such example and is often a vector of length *d*, as noted earlier both *n, d* may be on the order of billions to trillions of examples and dims respectively, in many cases, each training example *xi* is associated with a label *yi*, for example, in ad click prediction, *yi* might be 1 for clicked or -1 for not clicked; risk minimization learns a model that can predict the value *y* of a feature example *x*, the model consists of params *w*, in the simplest example, the model params might be the clickiness of each feature in an ad impression, to predict whether a new impression would be clicked, the system might simply suns its clickiness based upon features present in the impression, namely *xTw := _sum{j=1}{d}xjwj*, and then decide based on the sign; in any learning algorithm, there is an important relationship bt the amount of training data and model size, a more detailed model typically improves accuracy, but only up to a point :if there is too little training data, a highly-detailed model will overfit and become merely a system that uniquely memorizes every item in the training set, on the other hand, a too-small model will fail to capture interesting and relevant attributes of the data that are important to making a correct decision; regularized risk minimization is a method to find a model that balances model complexity and training error, it does so by minimizing sum of 2 terms :a loss *l(x,y,w)* representing prediction error on training data, and a regularizater $\Omega$\[*w*\] penalizing model complexity, a good model is one with low error and low complexity, consequently we strive to minimize: *F(w) = sigma_{i=1}{n}l(xi,yi,w)+*$\Omega$(*w*); the specific loss and regularizer functions used are important to the prediction perf of the ml algorithm, but relatively unimportant for the purpose of this paper :algorithms we present can be used with all of the most popular loss functions and regularizers; in future section Sparse Logistic Regression, we use a high-perf distributed learning algorithm to evaluate the param server, for the sake of simplicity we describe a much simpler model called [distributed subgradient descent](./img/distributed-subgradient-descent.png)(*the unfamiliar reader could read this as gradient descent, the subgradient aspect is simply a generalization to loss functions and regularizers that need not be continuously differentiate, such as |w| at w=0), here, training data is partitioned among all of the workers, which jointly learn the param vecor *w*, the algorithm operates iteratively, in each iteration, every worker independently uses its own training data to determine what changes should be made to *w* in order to get closer to an optimal value, because each worker's updates reflect only its own training data, the system needs a mechanism to allow these updates to mix, it does so by expressing updates as a subgradient -a direction in which the param vector *w* should be shifted -and aggregates all subgradients before applying them to *w*, these gradients are typically scaled down, with considerable attention paid in algorithm design to the right lr $\eta$ that should be applied in order to ensure that the algorithm converges quickly; here the most expensive step is computing the subgradient to update *w*, this task is divided among all of the workers, each of which execute WORKERITERATE, as part of this, workers compute *wTx_{i_k}* which should be infeasible for very high-dim *w*, fortunately, a worker needs to know a coordinate of *w* iff some of its training data references that entry; for example, in ad click prediction one of the key features are the words in ad, if only very few advertisements contain the phrase OSDI 2014, then most workers will not generate any updates to the corresponding entry in *w*, and hence do not require this entry, while the total size of *w* may exceed capacity of a single machine, the working set of entries needed by a particular worker can be trivially cached locally, to illustrate this, we randomly assigned data to workers and then counted the average working set size per worker on the dataset that is used in future section Sparse Logistic Regression, [click for for 100 workers each worker only needs 7.8% of the total params, with 10000 workers this reduces to 0.15%](./img/each-workers-set-of-parameters-shrinks-as-more-workers-are-used.png); 
**generative models:** in a second major of class of ml algorithms, the label is to be applied to training examples is unknown, such settings call for unsupervised algorithms(for labeled training data one can use supervised or semi-supervised algorithms), they attempt to capture underlying structure of the data, for example, a common problem in this area is topic modeling :given a collection of documents, infer the topics contained in each document; when run on for example the SOSP'13 proceedings, an algorithm might generate topics such as "distributed systems", "machine learnig", and "performance", the algorithms infer these topics from the content of the documents themselves, not an external topic list, in practical settings such as content personalization for recommendation systems, the scale of these problems is huge -hundreds of millions of users and billions of documents, making it cirtical to parallelize the algorithms across large clusters; because of their scale and data volumes, these algorithms only became commercially applicable following the introduction of 1st-gen param servers, a key challenge in topic models is that params describing current estimate of how documents are supposed to be generated must be shared; a popular topic modeling approach is LDA -Latent Dirichlet Allocation, while the statistical model is quite different, the resulting algorithm for learning it is very similar to Distributed Subgradient Descent(*the specific algorithm we use in the evaluation is a parallelized variant of a stochastic variational sampler with an update strategy similar to that used in YahooLDA), however, the key difference is that the update step is not a gradient computation, but an estimate of how well the document can be explained by current model, this computation requires access to auxiliary metadata for each document that is updated each time a document is accessed, because of #documents, metadata is typically read from and written back to disk whenever document is processed; here, this auxiliary data is the set of topics assigned to each word of a document, and the param *w* being learned consists of the relative freq of occurrence of a word; as before, each worker needs to store only the params for words occuring in the documents it processes, distributing documents across workers has the same effect as in the previous section -we can process much bigger models than a single worker may load.
### Architecture
an instance of the param server can run more than one algorithm simultaneously, param server nodes are grouped into a server group and several worker groups as shown in [fig4](./img/architecture-of-a-parameter-server-communication-with-several-groups-of-servers.png), a server node in the server group maintains a partition of the globally shared params, server nodes communicate with each other to replicate and/or to migrate params for reliability and scaling, a server manager node maintains a consistent view of the metadata of servers such as node liveness and the assignment of param partitions; 
each worker group runs an application, a worker typically stores locally a portion of the training data to compute local stats such as gradients, workers communicate only with the server nodes(not among themselves), updating and retrieving shared params, there is a scheduler node for each worker group, it assigns tasks to workers and monitors their progress, if workers are added or removed, it reschedules unfinished tasks; 
the param server supports independent param namespaces, this allows a worker group to isolate its set of shared params from others, several worker groups may also share the same namespace :we may use more than one worker group to solve the same dl application to increase parallelization; another example is that of a model being actively queried by some nodes such as online services consuming this model, simultaneously the model is updated by a different group of worker nodes as new training data arrives; 
the param server is designed to simplify developing distributed ml applications such as those discussed in section Machine Learning, the shared params are represented as (key,value) vectors to facilitate linear algebra ops(see future section (key,value) vectors), they are distributed across a grooup of server nodes(see future section consistent hashing), any node can both push out its local params and pull params from remote nodes(see future section range push and pull), by default, wkloads or tasks are executed by worker nodes, however, they can also be assigned to server nodes via user defined functions(see future section user-defined functions on the server), tasks are asynchronous and run in parallel(see future section asynchronous tasks and dependency), the param server provides the algorithm designer with flexibility in choosing a consistency model via task dependency graph(see future section flexible consistency) and predicates to communicate a subset of params(see future section user-defined filters); 
**(key,value) vectors:** the model shared among nodes can be represented as a set of (key,value) pairs, for example, in a loss minimization problem, the pair is a feature ID and its weight, for LDA, the pair is a combination of word ID and topic ID, and a count, each entry of the model can be read and written locally or remotely by its key, this (key,value) abstraction is widely adopted by existing approaches; our param server improves upon this basic approach by acknowledging underlying meaning of these key value items -ml algorithms typically treat the model as a linear algebra object, for example, *w* is used as a vector for both objective function and optimization in Distributed Subgradient Descent by risk minimization, by treating these objects as sparse linear algebra objects, the param server can provide the same functionality as the (key,value) abstraction, but admits important optimized ops such as vector addition *w+u*, multiplication *Xw*, finding the 2-norm ||w||2, and other more sophisticated ops; to support these optimizations, we assume that the keys are ordered, this lets us treat params as (key,value) pairs while endowing them with vector and matrix semantics, where non-existing keys are associated with 0s, this helps with linear algebra in ml, it reduces the programming effort to implement optimization algorithms, beyond convenience, this interface design leads to efficient code by leveraging cpu-efficient multithreaded self-tuning linear algebra libs such as BLAS, LAPACK, ATLAS; 
**range push and pull:** data is sent bt nodes using push and pull ops, in Distributed Subgradient Descent each worker pushes its entries local gradient into servers and then pulls updated weight back, the more advanced algorithm described in [Delayed Block Proximal Gradient](./img/delayed-block-proximal-gradient.png) uses the same pattern except that only a range of keys is communicated each time; the param server optimizes these updates for programmer convenience as well as computational and network bdwidth efficiency by supporting range-based push and pull -if *R* is a key range, then *w.push(R,dest)* sends all existing entries of *w* in key range *R* to destination, which can be either a particular node or a node group such as the server group, similarly, *w.pull(R,dest)* reads all existing entries of *w* in key range *R* from destination, if we set *R* to be the whole key range then the whole vector *w* will be communicated, if we set *R* to include a single key then only an individual entry will be sent; this interface can be extended to communicate any local data structures that share the same keys as *w*, for example, in Distributed Subgradient Descent a worker pushes its temporary local gradient *g* globally shared, however, note that *g* shares the keys of the worker's working set *w*, hence the programmer can use *w.push(R,g,dest)* for local gradients to save mem and also enjoy the optimization discussed in the following sections; 
**user-defined functions on the server:** beyond aggregating data from workers, server nodes can execute user-defined functions, it is beneficial because server nodes often have more complete or up-to-date info about the shared params, in Distributed Subgradient Descent server nodes evaluate subgradients of the regularizer $\Omega$ in order to update *w*, at the same time in Delayed Block Proximal Gradient a more complicated proximal operator is solved by servers to update the model; 
**asynchronous tasks and dependency:** a tasks is issued by a rpc, it can be a push or a pull that a worker issues to servers, it can also be a user-defined function that the scheduler issues to any node, tasks may include any number of subtasks such as the task WorkerIterate in Distributed Subgradient Descent contains 1 push and 1 pull; tasks are executed asynchronously :the caller can perform further computation immediately after issuing a task, the caller marks a task as finished only once it receives the callee's reply, a reply could be the function return of a user-defined function, the (key,value) pairs requested by the pull or an empty acknowledgement, the callee marks a task as finished only if the call of the task is returned and all subtasks issued by this call are finished; by default, callees execute tasks in parallel for best perf, a caller that wishes to serialize task exec can place an exec-after-finished dependency bt tasks, [click for 3 example iterations of WorkerIterate](./img/three-example-iterations-of-workerIterate.png), here Iterations10&11 are independent, but 12 depends on 11, the callee therefore begins iteration 11 immediately after local gradients are computed in 10, however, 12 is postponed until the pull of 11 finishes; task dependencies help implement algorithm logic, for example, the aggregation logic in ServerIterate of Distributed Subgradient Descent updates the weight *w* only after all worker gradients have been aggregated, this can be implemented by having the updating task depend on the push tasks of all workers, the second important use of dependencies is to support the flexible consistency models described next; 
**flexible consistency:** independent tasks improve system efficiency via parallelizing use of cpu, disk, and network bdwidth, however, this may lead to data inconsistency bt nodes, in [fig5](./img/three-example-iterations-of-workerIterate.png) worker *r* starts Iteration11 before *w^(11)* has been pulled back so it uses the old *w_r^(10)* in this iteration and so obtains the same gradient as in 10, namely *g_r^(11)=g_r^(10)*, this inconsistency potentially slows down the convergence progress of Distributed Subgradient Descent, however, some algorithms may be less sensitive to this type of inconsistency, for example, only a segment of *w* is updated each time in Delayed Block Proximal Gradient, hence starting 11 without waiting for 10 causes only a part of *w* to be inconsistent; the best trade-off bt system efficiency and algorithm convergence rate usually depends on a variety of factors including the algorithm's sensitivity to data inconsistency, feature correlation in training data, and capacity difference of hw components, instead of forcing user to adopt 1 particular dependency that may be illsuited to the problem, the param server gives the algorithm designer flexibility in defining consistency models, this is a substantial difference to other ml systems; [we show 3 different models that can be implemented by task dependency, click for their associated directed acyclic graphs](./img/directed-acyclic-graphs-for-different-consistency-models.png), it is: (i)sequential consistency, all tasks are executed one by one, the next task can be started only if the previous one has finished, it produces results identical to the single-thread implementation, and also named Bulk Synchronous Processing, (ii)eventual consistency, is the opposite -all tasks may be started simultaneously, however, this is only recommendable if underlying algorithms are robust with regard to delays, (iii)bounded delay, when a maximal delay time $\tau$ is set, a new task will blocked until all previous tasks $\tau$ times ago have been finished, Delayed Block Proximal Gradient uses such a model, this model provides more flexible controls than the previous two :$\tau$*=0* is the sequential consistency model, and an infinite delay $\tau$*=*$\infty$ becomes the eventual consistency model; note that the dependency graphs may be dynamic such as the scheduler may increase or decrease the maximal delay according to runtime progress to balance system efficiency and convergence of underlying optimization algorithm, in this case caller traverses directed acyclic graph, if the graph is static, caller can send all tasks with directed acyclic graph to callee to reduce synchronization cost; 
**user-friendly filters:** complementary to a scheduler-based flow control, the parameter server supports user-defined filters to selectively synchronize individual (key,value) pairs, allowing fine-grained control of data consistency within a task, the insight is that the optimization algorithm itself usually possesses info on which params are most useful for synchronization, one example is the significantly modified filter which only pushes entries that have changed by more than a threshold since their last synchronization.
### Implementation
servers store params(key value pairs) using consistent hashing(see future section consistent hashing), for fault tolerance entries are replicated using chain replication(see future section replication and consistency), different from prior (key,value) systems the param server is optimized for range-based comm with compression on both data(see future section msgs) and range-based vector clocks(see future section vector clock); 
**vector clock:** given the potentially complex task dependency graph and the need for fast recovery, each (key,value) pair is associated with a vector clock, which records time of each individual node on this (key,value) pair, vector clocks are convenient such as for tracking aggregation status or rejecting doubly sent data, however, a naive implementation of the vector clock requires O(nm) space to handle n nodes and m params, with thousands of nodes and billions of params, this is infeasible in terms of mem and bdwidth; fortunately, many params hare the same timestamp as a result of the range-based comm pattern of the param server :if a node pushes params in a range then the timestamps of the params associated with the node are likely the same, hence they can be compressed into a single range vector clock, more specifically, assume that *vc_i (k)* is the time of key *k* for node *i*, given a key range *R*, the ranged vector clock *vc_i (R)=t* means for any key *k belongstoR, vc_i (k)=t*; initially, there is only 1 range vector clock for each node *i*, it covers the entire param key space as its range with 0 as its initial timestamp, each range set may split the range and create at most 3 new vector clocks, click to view(./img/set-vector-clock-to-t-for-range-R-and-node-i.png), let *k* be the total #unique ranges communicated by the algorithm, *m* is #nodes, then there are at most O(mk) vector clocks, here, *k* is typically much smaller than the total #params, this significantly reduces the space required for range vector clocks(*ranges can be also merged to reduce #fragments, however, in practice both *m, k* are small enough to be easily handled); 
**msgs:** nodes may send msgs to individual nodes or node groups, a msg consists of a list of (key,value) pairs in the key range *R* and the associated range vector clock: *\[vc(R),(k1,v1),...,(k_p,v_p)\] k belongstoR and j belongsto{1,...,p}*, this is the basic comm format of the param server not only for shared params but also for tasks, for the latter, a (key,value) pair might assume the form (task ID, arguments or return results); msgs may carry a subset of all available keys within range *R*, the missing keys are assigned the same timestamp without changing their values, a msg can be split by the key range, this happens when a worker sends a msg to the whole server group, or when the key assignment of the receiver node has changed, by doing so, we partition the (key,value) lists and split the range vector clock similar to Set Vector Clock to *t* for Range *R* and Node *i*; because ml problems typically require high bdwidth, msg compression is desirable, training data often remains unchanged bt iterations, a worker might send the same key lists again, hence it is desirable for the receiving node to cache the key lists, later the sender only needs to send a hash of the list rather than the list itself, values, in turn, may contain many zero entries, likewise, [a user-defined filter may also zero out a large fraction of the values, click to view](./img/unique-features-ie-keys-filtered-by-the-Karush-Kuhn-Tucker-filter-as-optimization-proceeds.png), hence we need only send nonzero (key,value) pairs, we use the fast Snappy compression lib to compress msgs, effectively removing the 0s, note that key caching and value-compression can be used jointly; 
**consistent hashing:** the param server partitions keys much as a conventional distributed hash table does :keys and server node IDs are both inserted into the hash ring, each server node manages the key range starting with its insertion point to the next point by other nodes in the counter-clockwise direction, this node is called the master of this key range, a physical server is often represented in the ring via multiple virtual servers to improve load balancing and recovery; we simplify the mgmt by using a direct-mapped DHT design, the server manager handles the ring mgmt, all other nodes cache the key partition locally, this way they can determine directly which server is responsible for a key range and are notified of any changes; 
**replication and consistency:** each server node stores a replica of the *k* counterclockwise neighbor key ranges relative to the one it owns, we refer to nodes holding copies as slaves of the appropriate key range, [click for an example with *k=2*, where server1 replicates the key ranges owned by 2 and 3](./img/server-node-layout.png); worker nodes communicate with the master of a key range for both push and pull, any modification on the master is copied with its timestamp to the slaves, modifications to data are pushed synchronously to the slaves, [click for a case where worker1 pushes *x* into server1, which invokes a user defined function *f* to modify the shared data](./img/replica-generation.png), the push task is completed only once the data modification *f(x)* is copied to the slave; naive replication potentially increases the network traffic by *k* times, this is undesirable for many ml applications that depend on high network bdwidth, the param server framework permits an important optimization for many algorithms :replication after aggregation, server nodes often aggregate data from worker nodes such as summing local gradients, servers may therefore postpone replication until aggregation is complete, here in the righthand side of the diagram, 2 workers push *x, y* to the server respectively, the server first aggregates the push by *x+y*, then applies the modification *f(x+y)* and finally performs the replication, with *n* workers, replication uses only *k/n* bdwidth, here often *k* is a small constant while *n* is hundreds to thousands, while aggregation increases delay of the task reply, it can be hidden by relaxed consistency conditions; 
**server mgmt:** to achieve fault tolerance and dynamic scaling we must support addition and removal of nodes, for convenience we refer to virtual servers below, the following steps happen when a server joins: 1.server manager assigns the new code a key range to server as master, this may cause another key range to split or be removed from a terminated node, 2.the node fetches the range of data to maintain as master and *k* additional ranges to keep as slave, 3.server manager broadcasts the node changes, recipients of the msg may shrink their own data based on key ranges they no longer hold and to resubmit unfinished tasks to the new node; fetching the data in the range *R* from some node *S* proceeds in 2 stages, similar to the Ouroboros protocol, first *S* pre-copies all (key,value) pairs in the range together with associated vector clocks, this may cause a range vector clock to split similar to Set Vector Clock to *t* for Range *R* and Node *i*, if the new node fails at this stage, *S* remains unchanged, at the second stage *S* no longer accepts msgs affecting the key range *R* by dropping msgs without executing and replying, at the same time, *S* sends the new node all changes that occured in *R* during the pre-copy stage; on receiving the node change msg a node *N* first checks if it also maintains the key range *R*, if true and if this key range is no longer to be maintained by *N*, it deletes all associated (key,value) pairs and vector clocks in *R*, next, *N* scans all outgoing msgs that have not received replies yet, if a key range intersects with *R* then the msg will be split and resent; due to delays, failures, and lost acknowledgements *N* may send msgs twice, due to use of vector clocks both the original recipient and the new node are able to reject this msg and it does not affect correctness; the departure of a server node(voluntary or due to failure) is similar to a join, server manager takes a new node with taking the key range of the leaving node, server manager detects node failure by a heartbeat signal, integration with a cluster resource manager such as Yarn, Mesos is left for future work; 
**worker environment:** adding a new worker node *W* is similar but simpler than adding a new server node: 1.task scheduler assigns *W* a range of data, 2.this node loads the range of training data from a network file system or existing workers, training data is often read-only so there is no 2-phase fetch, next, *W* pulls shared params from servers, 3.task scheduler broadcasts the change, possibly causing other workers to free some training dataw; when a worker departs, task scheduler may start a replacement, we give the algorithm designer the option to control recovery for 2 reasons including: if the training data is huge, recovering a worker node be may more expensive than recovering a server node, and losing a small amount of training data during optimization typically affects the model only a little, hence the algorithm designer may prefer to continue without replacing a failed worker, it may even be desirable to terminate the slower workers.<br \>
我们提出了一个用于分布式机器学习问题的参数服务器框架，数据和工作负载分布在工作节点上，而服务器节点维护全局共享的参数，以密集或稀疏向量和矩阵表示。该框架管理异步数据通信节点，并支持灵活的一致性模型、弹性可扩展性和持续容错能力；为了展示该框架的可扩展性，我们在PB级真实数据上展示了实验结果，其中包含数十亿个示例和参数，涵盖了从稀疏逻辑回归到潜在狄利克雷分配和分布式草图绘制等问题。
分布式优化和推理正在成为解决大规模机器学习问题的先决条件。在大规模情况下，由于数据的增长以及由此产生的模型复杂性（通常表现为参数数量的增加），任何一台机器都无法足够快速地解决这些问题。然而，实现高效的分布式算法并不容易，密集的计算负载和数据通信量都需要精心的系统设计；实际的训练数据量范围为 1TB-1PB，这使得人们可以创建具有 10^9-10^12 个参数的强大而复杂的模型，这些模型通常由所有工作节点全局共享，这些工作节点必须在执行计算以改进模型时频繁访问共享参数。共享带来了 3 个挑战，包括：访问参数需要大量的网络带宽；许多机器学习算法是顺序的，当同步成本和机器延迟很高时，由此产生的障碍会损害性能；在大规模情况下，容错至关重要，学习任务通常在云环境中执行，在云环境中，机器可能不可靠，作业可能会被抢占；为了说明最后一点，我们收集了一家大型互联网公司一个集群 3 个月内的所有作业日志，[点击查看服务于生产环境的批量机器学习任务的统计数据](./img/statistics-of-machine-learning-jobs.png)，其中任务失败主要是由于在没有必要的容错机制的情况下被抢占或丢失机器；与许多研究环境中的作业在集群上完全运行且不存在争用不同，在实际部署中，容错是必需的；
**贡献：**自从它被推出以来，param 服务器框架已经在学术界和工业界中激增，这里我们描述了一个第三代开源的 param 服务器实现，它专注于分布式推理的系统方面，它给开发人员带来了两个好处：首先，通过分解出 ml 系统通常需要的组件，它使特定于应用程序的代码保持简洁，同时，作为针对系统级优化的共享平台，它提供了一个强大、多功能和高性能的实现，能够处理从稀疏逻辑回归到主题模型和分布式草图的各种算法；我们的设计决策以实际系统中的工作负载为指导，我们的参数服务器提供 5 个关键功能，包括：（i）高效通信 - 异步通信模型不会阻止计算（除非请求），它针对机器学习任务进行了优化，以减少网络流量和开销，（ii）灵活的一致性模型 - 宽松的一致性进一步隐藏了同步成本和延迟，我们允许算法设计者平衡算法收敛速度和系统效率，最佳权衡取决于数据、算法和硬件，（iii）弹性可扩展性 - 无需重新启动正在运行的框架即可添加新节点，（iv）容错和耐久性 - 在 1 秒内恢复和修复非灾难性的机器故障，而不会中断计算，矢量时钟确保网络分区和故障后明确定义的行为，（v）易于使用 - 全局共享的参数表示为（可能稀疏的）向量和矩阵以促进机器学习应用程序的开发，线性代数数据类型附带高性能多线程库；所提系统的创新之处在于通过选择正确的系统技术、使其适应机器学习算法以及修改机器学习算法使其更加系统友好而实现的协同效应，特别是，由于相关的机器学习算法对扰动具有相当强的容忍度，我们可以放宽许多原本困难的系统约束，其结果是第一个能够扩展到工业规模的通用机器学习系统；
**工程挑战：**在解决分布式数据分析问题时，读取和更新不同工作节点共享的参数的问题无处不在，参数服务器框架提供了一种有效的机制来聚合和同步工作节点的模型参数和统计数据，每个参数服务器节点只维护部分参数，并且每个工作节点在运行时通常只需要这些参数的子集，构建高性能参数服务器系统面临两个关键挑战：（i）通信：虽然参数可以在传统数据存储中作为键值对进行更新，但本机使用这种抽象效率低下：值通常很小（浮点数或整数），并且将每个更新作为键值操作发送的开销很高；我们改进这种情况的思路来自于这样的观察：许多学习算法将参数表示为结构化的数学对象，例如向量、矩阵或张量，在每个逻辑时间（或迭代）通常更新对象的一部分，即，工作器通常发送向量的一段或矩阵的整行，这提供了在参数服务器上自动批量发送更新及其处理的机会，并允许高效地实现一致性跟踪；（二）容错：如前所述，容错在规模化方面至关重要，为了实现高效操作，它不能要求完全重启长时间运行的计算，参数服务器的实时复制支持热故障转移，故障转移和自我修复反过来又通过将机器的移除或添加分别视为故障或修复来支持动态扩展； [点击查看在多个系统上进行的最大规模的有监督和无监督机器学习实验的概览](./img/comparison-of-the-public-largest-machine-learning-experiments-each-system-performed.png)，在可能的情况下，我们与每个系统的作者确认了扩展限制（数据截至 2014 年 4 月），显然，我们能够在比任何其他已发布的系统多几个数量级的处理器上覆盖更多数量级的数据，此外，[点击查看多个机器学习系统的主要特征的概览](./img/attributes-of-distributed-data-analysis-systems.png)，我们的参数服务器在一致性方面提供了最大程度的灵活性，它是唯一提供持续容错的系统，其原生数据类型使其特别适合数据分析。
### 机器学习
机器学习系统广泛应用于网络搜索、垃圾邮件检测、推荐系统、计算广告和文档分析。这些系统会自动从样本（称为训练数据）中学习模型。机器学习系统通常包含三个部分：特征提取、目标函数和学习；其中，特征提取处理原始数据（例如文档、图像和用户查询日志）以获取特征向量，其中每个特征都代表训练数据的一个属性。预处理可以通过现有框架（例如 MapReduce）高效地执行；
**目标**：许多机器学习算法的目标可以通过一个目标函数来表达，该函数捕获已学习模型的属性，例如在将电子邮件分类为正常邮件和垃圾邮件时，其错误率要低；在估计文档主题时，数据的解释能力要好；或者在绘制数据时，其计数要简洁。学习算法通常最小化该目标函数以获得模型，一般来说，没有封闭形式的解决方案，而是从初始模型开始学习，通过处理训练数据迭代地改进该模型（可能多次），以接近解决方案，当找到（接近）最优解或模型被认为收敛时停止;训练数据可能非常大，例如，一家大型互联网公司使用1年的广告展示日志来训练广告点击预测器将拥有数万亿个训练示例，每个训练示例通常表示为可能非常高维的特征向量，因此训练数据可能由数万亿个万亿长度的特征向量组成，迭代处理如此大规模的数据需要巨大的计算和带宽资源，此外，每天可能会有数十亿个新的广告展示，将这些数据添加到系统中通常可以提高预测准确性和覆盖率，但它也要求学习算法每天运行，可能实时运行。高效执行这些算法是主要关注点；为了激励我们系统中的设计决策，接下来我们简要概述两种广泛使用的机器学习技术，我们将使用它们来展示我们的参数服务器的有效性；
**风险最小化**：机器学习问题最直观的变体是风险最小化，风险大致是预测误差的度量，例如，如果我们要预测明天的股价，风险可能是预测值与股票实际值之间的偏差；训练数据包含*n*个示例，*xi*是第*i*个这样的示例，通常是一个长度为*d*的向量，如前所述，*n和d*可能分别在数十亿到数万亿个示例和维度的数量级上。在许多情况下，每个训练示例*xi*都与一个标签*yi*相关联，例如，在广告点击预测中，*yi*可能为1表示点击，或-1表示未点击；风险最小化学习一个模型，该模型可以预测特征示例 *x* 的值 *y*，该模型由参数 *w* 组成，在最简单的例子中，模型参数可能是广告印象中每个特征的点击率，为了预测是否会点击新的印象，系统可能只是根据印象中存在的特征来预测其点击率，即 *xTw := _sum{j=1}{d}xjwj*，然后根据符号决定；在任何学习算法中，训练数据量和模型大小之间存在重要的关系，更详细的模型通常可以提高准确性，但只能达到一定程度：如果训练数据太少，高度详细的模型将过度拟合并成为仅仅唯一地记住训练集中每个项目的系统，另一方面，太小的模型将无法捕获对做出正确决策很重要的数据的有趣和相关属性；正则化风险最小化是一种寻找平衡模型复杂性和训练误差的模型的方法，它通过最小化两个项的总和来实现：损失 *l(x,y,w)* 表示训练数据的预测误差，正则化器 $\Omega$\[*w*\] 惩罚模型复杂性，好的模型是低误差和低复杂度的模型，因此我们努力最小化：*F(w) = sigma_{i=1}{n}l(xi,yi,w)+*$\Omega$(*w*)；所使用的特定损失和正则化函数对于 ml 算法的预测性能很重要，但对于本文的目的来说相对不重要：我们提出的算法可以与所有最流行的损失函数和正则化器一起使用；在后面的稀疏逻辑回归部分中，我们使用高性能分布式学习算法来评估参数服务器。为了简单起见，我们描述了一个更简单的模型，称为[分布式次梯度下降](./img/distributed-subgradient-descent.png)(*不熟悉的读者可能会将其理解为梯度下降，次梯度方面只是对损失函数和正则项的推广，这些函数和正则项不需要连续微分，例如 w=0 时的 |w|)，在这里，训练数据被分配给所有工作器，它们共同学习参数向量 *w*。该算法迭代运行，在每次迭代中，每个工作器独立使用自己的训练数据来确定应该对 *w* 进行哪些更改以更接近最优值。因为每个工作器的更新仅反映其自己的训练数据，所以系统需要一种机制来允许这些更新混合，它通过将更新表示为次梯度来实现 - 参数向量 *w* 应该沿此方向移动 - 并且在将所有子梯度应用到 *w* 之前对其进行聚合，这些梯度通常会缩小，在算法设计中要特别注意应用正确的 lr $\eta$，以确保算法快速收敛；这里最昂贵的步骤是计算子梯度来更新 *w*，这个任务被分配给所有的工作者，每个工作者执行 WORKERITERATE，作为其中的一部分，工作者计算 *wTx_{i_k}*，这对于非常高维的 *w* 来说是不可行的，幸运的是，工作者需要知道 *w* 的坐标当且仅当它的一些训练数据引用该条目；例如，在广告点击预测中，关键特征之一是广告中的单词。如果只有极少数广告包含短语“OSDI 2014”，那么大多数工作器将不会对 *w* 中的相应条目生成任何更新，因此不需要此条目。虽然 *w* 的总大小可能超过单台机器的容量，但特定工作器所需的条目工作集可以轻松地在本地缓存。为了说明这一点，我们随机将数据分配给工作器，然后计算每个工作器在数据集上的平均工作集大小，该数据集将在后面的“稀疏逻辑回归”部分中使用。[点击查看，对于 100 个工作器，每个工作器仅需要总参数的 7.8%，而对于 10000 个工作器，这个比例会降低到 0.15%](./img/each-workers-set-of-parameters-shrinks-as-more-workers-are-used.png);
**生成模型**：在第二种主要的机器学习算法中，应用于训练示例的标签是未知的，这种设置需要无监督算法（对于标记的训练数据，可以使用监督或半监督算法），它们试图捕捉数据的底层结构，例如，该领域的一个常见问题是主题建模：给定一个文档集合，推断每个文档中包含的主题；例如，在 SOSP'13 会议记录中运行时，算法可能会生成诸如“分布式系统”、“机器学习”和“性能”等主题，算法从文档本身的内容推断出这些主题，而不是外部主题列表，在实际设置中，例如推荐系统的内容个性化，这些问题的规模是巨大的——数亿用户和数十亿文档，因此在大型集群上并行化算法至关重要；由于其规模和数据量，这些算法只有在引入第一代参数服务器后才具有商业应用价值，主题模型中的一个关键挑战是必须共享描述文档生成方式当前估计的参数；一种流行的主题建模方法是 LDA - 潜在狄利克雷分配，虽然统计模型有很大不同，但学习它的算法与分布式次梯度下降非常相似（*我们在评估中使用的具体算法是随机变分采样器的并行化变体，其更新策略与 YahooLDA 中使用的类似），然而，关键区别在于更新步骤不是梯度计算，而是对文档可以用当前模型解释得有多好的估计，这种计算需要访问每次访问文档时更新的每个文档的辅助元数据，因为#documents，所以每当处理文档时，通常会读取元数据并将其写回磁盘；这里，这些辅助数据是分配给文档中每个单词的主题集合，而正在学习的参数 *w* 由单词出现的相对频率组成；与之前一样，每个工作器只需存储其处理的文档中出现的单词的参数，将文档分布到各个工作器上的效果与上一节相同——我们可以处理比单个工作器加载的更大的模型。
### 架构
一个参数服务器实例可以同时运行多个算法，参数服务器节点被分组为一个服务器组和多个工作器组，如[图4](./img/architecture-of-a-parameter-server-communication-with-several-groups-of-servers.png) 所示，服务器组中的服务器节点维护全局共享参数的分区，服务器节点相互通信以复制和/或迁移参数，从而实现可靠性和可扩展性，服务器管理器节点维护服务器元数据的一致视图，例如节点活跃度和参数分区的分配；
每个工作组运行一个应用程序。每个工作组通常在本地存储部分训练数据，用于计算梯度等本地统计数据。工作组仅与服务器节点通信（而非彼此通信），更新和检索共享参数。每个工作组都有一个调度节点，负责将任务分配给各个工作组并监控其进度。如果添加或移除了各个工作组，调度节点会重新安排未完成的任务；
参数服务器支持独立的参数命名空间，这使得一个工作组可以将其共享参数集与其他工作组隔离开来，多个工作组也可以共享相同的命名空间：我们可以使用多个工作组来解决同一个深度学习应用程序，以提高并行化程度；另一个示例是，某些节点（例如使用该模型的在线服务）正在主动查询某个模型，同时，随着新的训练数据的到来，该模型会由不同的工作组更新；
参数服务器旨在简化分布式机器学习应用程序的开发，例如机器学习部分中讨论的那些，共享参数表示为（键，值）向量以促进线性代数运算（参见未来的（键，值）向量部分），它们分布在一组服务器节点上（参见未来的一致性哈希部分），任何节点都可以推出其本地参数并从远程节点拉取参数（参见未来的范围推送和拉取部分），默认情况下，工作负载或任务由工作节点执行，但是，也可以通过用户定义的函数将它们分配给服务器节点（参见未来的服务器上的用户定义函数部分），任务是异步的并并行运行（参见未来的异步任务和依赖关系部分），参数服务器为算法设计者提供了通过任务依赖图选择一致性模型的灵活性（参见未来的灵活一致性部分）和谓词来传达参数子集（参见未来的用户定义的过滤器部分）；
d 过滤器）；
**(key,value) 向量**：节点间共享的模型可以表示为一组 (key,value) 对。例如，在损失最小化问题中，该对是特征 ID 及其权重；对于 LDA，该对是单词 ID、主题 ID 和计数的组合。模型的每个条目都可以通过其键在本地或远程进行读写，这种 (key,value) 抽象被现有方法广泛采用；我们的参数服务器通过承认这些键值项的潜在含义改进了这种基本方法 -ml 算法通常将模型视为线性代数对象，例如，*w* 用作分布式次梯度下降中的目标函数和优化的向量，通过风险最小化，通过将这些对象视为稀疏线性代数对象，参数服务器可以提供与 (key,value) 抽象相同的功能，但承认重要的优化操作，如向量加法 *w+u*、乘法 *Xw*、查找 2 范数 ||w||2 和其他更复杂的操作；为了支持这些优化，我们假设键是有序的，这样我们就可以将参数视为 (键,值) 对，同时赋予它们向量和矩阵语义，其中不存在的键与 0 关联。这有助于机器学习中的线性代数运算，减少了实现优化算法的编程工作量。除了方便之外，这种接口设计还利用了 CPU 高效的多线程自调优线性代数库（如 BLAS、LAPACK、ATLAS），从而实现了高效的代码；
**范围推送和拉取**：数据使用推送和拉取操作在节点之间发送。在分布式次梯度下降中，每个工作器将其条目的局部梯度推送到服务器，然后将更新后的权重拉回。[延迟块近端梯度](./img/delayed-block-proximal-gradient.png) 中描述的更高级算法使用相同的模式，只是每次只传递一定范围的键；参数服务器通过支持基于范围的推送和拉取来优化这些更新，以方便程序员以及提高计算和网络带宽效率 - 如果 *R* 是一个键范围，则 *w.push(R,dest)* 将键范围 *R* 中 *w* 的所有现有条目发送到目的地，目的地可以是特定节点或节点组（例如服务器组），类似地，*w.pull(R,dest)* 从目的地读取键范围 *R* 中 *w* 的所有现有条目，如果我们将 *R* 设置为整个键范围，则将传达整个向量 *w*，如果我们将 *R* 设置为包含单个键，则只会发送单个条目；此接口可以扩展，以便与任何与 *w* 共享相同键的本地数据结构进行通信。例如，在分布式次梯度下降中，一个工作节点将其临时的局部梯度 *g* 推送到全局共享节点。但请注意，*g* 共享该工作节点工作集 *w* 的键，因此程序员可以使用 *w.push(R,g,dest)* 来获取局部梯度，以节省内存，并享受以下章节中讨论的优化功能；
**服务器上的用户定义函数**：除了从工作节点聚合数据之外，服务器节点还可以执行用户定义函数，这是非常有益的，因为服务器节点通常拥有关于共享参数的更完整或最新的信息。在分布式次梯度下降中，服务器节点会评估正则化器 $\Omega$ 的子梯度以更新 *w*；同时，在延迟块近端梯度中，服务器会求解更复杂的近端算子来更新模型；
**异步任务及依赖**：任务由rpc发起，可以是worker向服务器发起的推送或拉取，也可以是调度器向任意节点发起的用户定义函数，任务可以包含任意数量的子任务，例如分布式次梯度下降中的任务WorkerIterate就包含1个推送和1个拉取；任务是异步执行的：调用者在发起任务后可以立即进行后续计算，调用者只有在收到被调用者的回复后才将任务标记为完成，回复可以是用户定义函数的函数返回值、拉取请求的（键，值）对或空的确认，被调用者只有在任务调用返回并且本次调用发起的所有子任务都完成后才将任务标记为完成；默认情况下，被调用者为了获得最佳性能会并行执行任务，希望序列化任务执行的调用者可以在任务中放置一个 exec-after-finished 依赖项，[点击查看 WorkerIterate 的 3 个示例迭代](./img/three-example-iterations-of-workerIterate.png)，这里的迭代 10 和 11 是独立的，但迭代 12 依赖于迭代 11，因此被调用者在迭代 10 计算出局部梯度后立即开始迭代 11，而迭代 12 会推迟到迭代 11 完成；任务依赖关系有助于实现算法逻辑，例如，分布式次梯度下降的 ServerIterate 中的聚合逻辑仅在所有工作者梯度都聚合后才更新权重 *w*，这可以通过使更新任务依赖于所有工作进程的推送任务的方式实现，依赖关系的第二个重要用途是支持下文描述的灵活一致性模型；
**灵活的一致性**：独立任务通过并行使用 CPU、磁盘和网络带宽来提高系统效率，然而这可能导致节点之间的数据不一致。在 [图5](./img/three-example-iterations-of-workerIterate.png) 中，worker *r* 在 *w^(11)* 被拉回之前就开始了迭代 11，因此它在本轮迭代中使用旧的 *w_r^(10)*，从而获得与 10 相同的梯度，即 *g_r^(11)=g_r^(10)*，这种不一致性可能会减慢分布式次梯度下降的收敛进度。然而，有些算法可能对这种不一致性不太敏感，例如，在延迟块近端梯度中，每次只更新 *w* 的一部分，因此不等 10 就开始迭代 11 会导致只有一部分 *w* 不一致； BT 系统效率和算法收敛速度的最佳权衡通常取决于多种因素，包括算法对数据不一致的敏感性、训练数据中的特征相关性以及硬件组件的容量差异，参数服务器不会强迫用户采用 1 种可能不适合问题的特定依赖关系，而是为算法设计者提供了定义一致性模型的灵活性，这是与其他机器学习系统的一个实质性区别； [我们展示了三种可以通过任务依赖关系实现的不同模型，点击查看它们对应的有向无环图](./img/directed-acyclic-graphs-for-different-consistency-models.png)，它们是：（i）顺序一致性，所有任务逐个执行，只有前一个任务完成后才能启动下一个任务，它产生的结果与单线程实现相同，也称为批量同步处理；（ii）最终一致性，与此相反 - 所有任务可以同时启动，但是，这仅在底层算法对延迟具有鲁棒性时才可推荐；（iii）有界延迟，当设置最大延迟时间$\tau$时，新任务将被阻止，直到所有$\tau$次之前的任务都完成为止；延迟块近端梯度使用这种模型，该模型比前两个模型提供更灵活的控制：$\tau$*=0*是顺序一致性模型，无限延迟$\tau$*=*$\infty$变为最终一致性模型；需要注意的是，依赖关系图可能是动态的，例如，调度程序可能会根据运行时进度增加或减少最大延迟，以平衡系统效率和底层优化算法的收敛性。在这种情况下，调用者会遍历有向无环图。如果依赖关系图是静态的，调用者可以将所有包含有向无环图的任务发送给被调用者，以降低同步成本；
**用户友好的过滤器**：作为基于调度程序的流控制的补充，参数服务器支持用户定义的过滤器，可以选择性地同步单个（键，值）对，从而允许对任务内的数据一致性进行细粒度控制。其原理是，优化算法本身通常包含哪些参数对同步最有用的信息。例如，经过显著修改的过滤器仅推送自上次同步以来变化超过阈值的条目。
### 实现
服务器使用一致性哈希存储参数（键值对）（参见后续章节“一致性哈希”）。为了实现容错，条目使用链式复制进行复制（参见后续章节“复制与一致性”）。与之前的（键，值）系统不同，参数服务器针对基于范围的通信进行了优化，并对数据（参见后续章节“消息”）和基于范围的向量时钟（参见后续章节“向量时钟”）进行了压缩；
**向量时钟**：考虑到潜在的复杂任务依赖图和快速恢复的需求，每个（键，值）对都与一个向量时钟相关联，该向量时钟记录该（键，值）对上每个节点的时间。向量时钟非常方便，例如用于跟踪聚合状态或拒绝重复发送的数据。然而，一个原始的向量时钟实现需要 O(nm) 空间来处理 n 个节点和 m 个参数，对于数千个节点和数十亿个参数，这在内存和带宽方面是不可行的；幸运的是，由于参数服务器的基于范围的通信模式，许多参数具有相同的时间戳：如果一个节点在某个范围内推送参数，那么与该节点关联的参数的时间戳很可能相同，因此它们可以压缩为单个范围向量时钟，更具体地说，假设 *vc_i (k)* 是节点 *i* 的键 *k* 的时间，给定一个键范围 *R*，范围向量时钟 *vc_i (R)=t* 意味着对于任何键 *k 属于 R，vc_i (k)=t*；最初，每个节点 *i* 只有 1 个范围向量时钟，它以 0 作为初始时间戳，覆盖整个参数键空间作为其范围，每个范围集可以拆分范围并创建最多 3 个新的矢量时钟，点击查看(./img/set-vector-clock-to-t-for-range-R-and-node-i.png)，令 *k* 为算法传递的唯一范围总数，*m* 为节点数，则最多有 O(mk) 个矢量时钟，其中 *k* 通常远小于参数总数，这显著减少了范围矢量时钟所需的空间（*范围也可以合并以减少碎片，但实际上 *m 和 k* 都足够小，可以轻松处理）；
**msgs：**节点可以向单个节点或节点组发送消息，一个消息由键范围 *R* 中的 (键，值) 对列表和相关范围向量时钟组成：*\[vc(R),(k1,v1),...,(k_p,v_p)\] k 属于 R 且 j 属于 {1,...,p}*，这是参数服务器的基本通信格式，不仅适用于共享参数，也适用于任务，对于后者，(键，值) 对可能采用以下形式 (任务 ID、参数或返回结果)；消息可能携带范围 *R* 内所有可用键的子集，缺失的键被分配相同的时间戳而不改变它们的值，消息可以根据键范围进行拆分，当工作人员向整个服务器组发送消息时会发生这种情况，或者当接收方节点的键分配发生变化时，通过这样做，我们对（键，值）列表进行分区，并拆分范围矢量时钟，类似于将范围 *R* 和节点 *i* 的矢量时钟设置为 *t*；由于机器学习问题通常需要高带宽，因此需要进行消息压缩。训练数据通常在迭代过程中保持不变。工作节点可能会重复发送相同的键列表，因此接收节点最好缓存这些键列表。之后，发送者只需发送列表的哈希值，而不是列表本身。值也可能包含许多零值。同样，[用户定义的过滤器也可能将大部分值清零，点击查看](./img/unique-features-ie-keys-filtered-by-the-Karush-Kuhn-Tucker-filter-as-optimization-proceeds.png)，因此我们只需要发送非零的 (键，值) 对。我们使用快速的 Snappy 压缩库来压缩消息，有效地删除其中的 0。请注意，键缓存和值压缩可以联合使用；
**一致性哈希：** 参数服务器对键的分区与传统的分布式哈希表类似：键和服务器节点 ID 均插入到哈希环中，每个服务器节点按逆时针方向管理从其插入点到其他节点的下一个点的键范围，该节点称为该键范围的主节点。物理服务器通常通过多个虚拟服务器在环中表示，以提高负载平衡和恢复能力；我们通过使用直接映射 DHT 设计来简化管理，服务器管理器处理环管理，所有其他节点在本地缓存键分区，这样它们就可以直接确定哪个服务器负责某个键范围，并在任何更改时收到通知；
**复制和一致性**：每个服务器节点都存储相对于其拥有的键范围的 *k* 个逆时针邻居键范围的副本，我们将拥有副本的节点称为相应键范围的从属节点，[单击查看 *k=2* 的示例，其中 server1 复制了 2 和 3 拥有的键范围](./img/server-node-layout.png)；工作节点与键范围的主节点进行通信以进行推送和拉取，主节点上的任何修改都会连同其时间戳一起复制到从属节点，对数据的修改会同步推送到从属节点，[单击查看 worker1 将 *x* 推送到 server1 的情况，server1 调用用户定义函数 *f* 来修改共享数据](./img/replica-generation.png)，只有在数据修改 *f(x)* 复制到从属节点后，推送任务才算完成；简单的复制可能会使网络流量增加 *k* 倍，这对于许多依赖高网络带宽的机器学习应用来说是不可取的。参数服务器框架允许对许多算法进行重要的优化：先聚合后复制。服务器节点通常会聚合来自工作节点的数据，例如对局部梯度求和。因此，服务器可能会推迟复制，直到聚合完成。在下图的右侧，两个工作节点分别将 *x, y* 推送到服务器。服务器首先通过 *x+y* 聚合推送，然后应用修改 *f(x+y)*，最后执行复制。在 *n* 个工作节点的情况下，复制仅使用 *k/n* 带宽。这里 *k* 通常是一个小常数，而 *n* 则是数百到数千。聚合会增加任务回复的延迟，但可以通过宽松的一致性条件来隐藏这一延迟。
**服务器管理：**为了实现容错和动态扩展，我们必须支持节点的添加和删除，为了方便起见，我们在下面引用虚拟服务器，服务器加入时会发生以下步骤：1.服务器管理器将新代码的密钥范围分配给服务器作为主服务器，这可能会导致另一个密钥范围分裂或从终止的节点中删除，2.节点获取要作为主节点维护的数据范围以及 *k* 个附加范围作为从节点保留，3.服务器管理器广播节点更改，消息的接收者可以根据不再持有的关键范围缩小自己的数据，并将未完成的任务重新提交给新节点；从某个节点 *S* 获取范围 *R* 中的数据分两个阶段进行，类似于 Ouroboros 协议，首先 *S* 预先复制范围内的所有（键，值）对以及相关的矢量时钟，这可能导致范围矢量时钟分裂，类似于将范围 *R* 和节点 *i* 的矢量时钟设置为 *t*，如果新节点在此阶段失败，*S* 保持不变，在第二阶段 *S* 不再接受影响关键范围 *R* 的消息，通过丢弃未执行和回复的消息，同时，*S* 向新节点发送在预复制阶段在 *R* 中发生的所有更改；节点 *N* 收到节点变更消息后，首先检查自身是否也维护着键值范围 *R*，如果是，且该键值范围不再由 *N* 维护，则删除 *R* 中所有关联的 (键,值) 对和向量时钟；接下来，*N* 扫描所有尚未收到回复的外发消息，如果某个键值范围与 *R* 相交，则该消息将被拆分并重新发送；由于延迟、故障和丢失确认，*N* 可能会发送两次消息，由于使用了向量时钟，原始接收者和新节点都可以拒绝该消息，并且不会影响正确性；服务器节点的离开（自愿或由于故障）类似于加入，服务器管理器会接收新节点并接收离开节点的键值范围，服务器管理器通过心跳信号检测节点故障，与 Yarn、Mesos 等集群资源管理器的集成留待以后完成；
**工作节点环境：**添加新的工作节点 *W* 与添加新的服务器节点类似但更简单：1.任务调度程序为 *W* 分配一系列数据，2.此节点从网络文件系统或现有工作节点加载一系列训练数据，训练数据通常是只读的，因此没有两阶段获取，接下来，*W* 从服务器拉取共享参数，3.任务调度程序广播更改，可能导致其他工作节点释放一些训练数据；当一个工作节点离开时，任务调度程序可能会开始替换，我们为算法设计者提供了控制恢复的选项，原因有二：如果训练数据庞大，恢复工作节点可能比恢复服务器节点更昂贵，并且在优化过程中丢失少量训练数据通常只会对模型产生一点影响，因此算法设计者可能更愿意继续而不替换失败的工作节点，甚至可能希望终止速度较慢的工作节点。
****
### Tail Latency & Interference
### The Tail at Scale
### [Dean13](https://dl.acm.org/doi/pdf/10.1145/2408776.2408794)
### Even rare perf hiccups affect a significant fraction of all reqs in large-scale distribted systems; elliminating all resources of latency variability in large-scale systems is impractical, especially in shared environments; using an approach analogous to fault-tolerant computing, tail-tolerant sw techniques form a predictable whole out of less-predictable parts.
### A simple way to curb latency variability is to issue the same req to multiple replicas and use the results from whichever replica responds first.
systems that respond to user actions quickly(within 100ms) feel more fluid and natural to users than those that take longer, improvements in internet connectivity and rise of warehouse-scale computing systems have enabled web services that provide fluid responsiveness while consulting multiterabyte datasets spanning thousands of servers, for example, the Google search system updates query results interactively as user types, predicting the most likely query based on prefix typed so far, performing the search and showing the results within a few tens of ms; emerging ar devices(such as the Google Glass prototype) will need associated web services with even greater responsiveness in order to guarantee seamless interactivity;
it is challenging for service providers to keep tail of latency distribution short for interactive services as the size and complexity of the system scales up or as overall use increases, temporary high-latency episodes(unimportant in moderate-size systems) may come to dominate overall service perf at large scale, just as fault-tolerant computing aims to create a reliable whole out of less-reliable parts, large online services need to create a predictably responsive whole out of less-predictable parts, we refer to such systems as latency tail-tolerant, or simply tail-tolerant; here, we outline some common causes for high-latency episodes in large online services and describe techniques that reduce their severity or mitigate their effect on whole-system perf, in many cases, tail-tolerant techniques can take advantage of resources already deployed to achieve fault-tolerance, resulting in low additional overhead, we explore how these techniques allow system utilization to be driven higher without lengthening latency tail, hence avoiding wasteful overprovisioning.
### Why Variablity Exists?
variability of response time that leads to high tail latency in individual components of a service can arise for many reasons including: (i)shared resources -machines might be shared by different applications contending for shared resources(such as cpu cores, processor caches, mem bdwidth, and network bdwidth), and within the same application different reqs might contend for resources; (ii)daemons -bg darmons may use only limited resources on average but when scheduled can generate multims hiccups; (iii)global resource sharing -applications running on different machines might contend for global resources(such as network switches and shared file systems); (iv)maintainance activities -bg activities(such as data reconstruction in distributed file systems, periodic log compactions in storage systems like BigTable, and periodic garbage collection in garbage-collected langs) can cause periodic spikes in latency; (v)queueing -multiple layers of queueing in intermediate servers and network switches amplify this variability; (vi)increased variability is also due to several hw trends including: power limits -modern cpus are designed to temporarily run above their average power envelope, mitigating thermal effects by throttling if this activity is sustained for a long period, garbage collection -solid-state storage devices provide very fast random read access, but the need to periodically garbage collect a large #data blocks can increase read latency by a factor of 100 with even a modest level of write activity, energy mgmt -power-saving modes in many types of devices save considerable energy but add additional latency when moving from inactive to active modes.
### Component-Level Variability Amplified by Scale
a common technique for reducing latency in large-scale online services is to parallelize sub-ops across many different machines, where each sub-op is co-located with its portion of a large dataset; parallelization happens by fanning out a req from a root to a large #leaf servers and merging responses via a req-distribution tree, these sub-ops must all complete within a strict deadline for the service to feel responsive; 
variability in the latency distribution of individual components is magnified at the service level, for example, consider a system where each server typically responds in 10ms but with a 99th-percentile latency of 1sec, if a user req is handled on jsut one such server, an user req in 100 will be slow(1sec), [click for an outline of how service-level latency in this hypothetical scenario is affected by very modest fractions of latency outliers](./img/component-level-variability-amplified-by-scale-example.png), if a user req must collect responses from 100 such servers in parallel, then 63% of user reqs will take more than 1sec(marked "x" in the fig), even for services with only one in 10000 reqs experiencing more than 1sec latencies at the single-server level, a service with 2000 such servers will see almost one in five user reqs taking more than 1sec(marked "o" in the fig); table1 lists measurements from a real Google service that is logically similar to this idealized scenario, root servers distribute a req through intermediate servers to a very large #leaf servers, table1 shows the effect of large fan-out on latency distributions, the 99th-percentile latency for a single random req to finish, measured at the root, is 10ms, however, the 99th-percentile latency for all reqs to finish is 140ms, and the 99th-percentile latency for 95% of reqs finishing is 70ms, meaning that waiting for the slowest 5% of reqs to complete is responsible for half of the total 99th-percentile latency, techniques that concentrate on these slow outliers can yield dramatic reductions in overall service perf; 
overprovisioning of resources, careful realtime engineering of sw, and improved reliability can all be used at all levels and in all components to reduce base causes of variability, we next describe general approaches useful for reducing variability in service responsiveness.
### Reducing Component Variability
interactive response-time variability can be reduced by ensuring interactive reqs are serviced in a timely manner through many small engineering decisions including: (i)differentiating service classes and higher-level queueing -differentiated service classes can be used to prefer scheduling reqs for which a user is waiting over non-interactive reqs; keep low-level queues short so higher-level policies take effect more quickly, for example, storage services in Google's cluster-level filesystem sw keep few ops outstandig in os's disk queue, instead maintaining their own priority queues of pending disk reqs, this shallow queue allows servers to issue incoming high-priority interactive reqs before older reqs for latency-intensive batch ops are served; (ii)reducing head-of-line blocking -high-level services can handle reqs with widely varying intrinsic costs, it is sometimes useful for the system to break long-running reqs into a sequence of smaller reqs to allow interleaving of the exec of other short-running reqs, for example, Google's web search system uses such time-slicing to prevent a small #very computationally expensive queries from adding substantial latency to a large #concurrent cheaper queries; (iii)managing bg activities and synchronized disruption -bg tasks can create significant cpu, disk, or network load, examples are log compaction in log-oriented storage systems and garbage-collector activity in garbage-collected langs; a combination of throttling, breaking down heavyweight ops into smaller ops, and triggering such ops at times of lower overall load is often able to reduce effect of bg activities on interactive req latency; for large fan-out services, it is sometimes useful for the system to synchronize the bg activity across many different machines, this synchronization enforces a brief burst of activity on each machine simultaneously, slowing only those interactive reqs being handled during the brief period of bg activity, in contrast, without synchronization a few machines are always doing some bg activity, pushing out the latency tail on all reqs; 
missing in this discussion so far is any reference to caching, while effective caching layers can be useful, even a necessity in some systems, they do not directly address tail latency, aside from configs where it is guaranteed that the entire working set of an application can reside in a cache.
### Living with Latency Variability
the careful engineering techniques in the preceding section are essential for building high-perf interactive services, but the scale and complexity of modern web services make it infeasible to eliminate all latency variability, even if such perfect behavior could be achieved in isolated environments, systems with shared computational resources exhibit perf fluctuations beyond control of application developers, Google has therefore found it advantageous to develop tail-tolerant techniques that mask or work around temporary latency pathologies, instead of trying to eliminate them altogether, we separate these techinques into 2 main classes -the first corresponds to within-req immediate-response techinques that operate at a time scale of tens of ms, before longer-term techniques have a chance to react, the second consists of cross-req long-term adaptations that perform on a time scale of tens of secs to mins and are meant to mask effect of longer-term phenomena.
### Within Req Shrot-Term Adaptations
a broad class of web services deploy multiple replicas of data items to provide additional throughput capacity and maintain availability in the presence of failures, this approach is particularly effective when most reqs operate on largely read-only, loosely consistent datasets, an example is a spelling-correction service that has its model updated once a day while handling thousands of correction reqs per sec, similarly, distributed file systems may have multiple replicas of a given data chunk that can all be used to service read reqs, the techniques here show how replication can also be used to reduce latency variability within a single higher-level req including: 
**hedged reqs:** a simple way to curb latency variability is to issue the same req to multiple replicas and use the results from whichever replica responds first, we term such reqs "hedged reqs" because a client first sends one req to the replica believed to be the most appropriate, but then falls back on sending a secondary req after some brief delay, the client cancels remaining outstanding reqs once the first result is received, although naive implementations of this technique typically add unacceptable additional load, many variations exist that give most of the latency-reduction effects while increasing load only modestly; one such approach is to defer sending a secondary req until the first req has been outstanding for more than the 95th-percentile expected latency for this class of reqs, this approach limites the additional load to approximately 5% while substantially shortening the latency tail, the technique worker because the source of latency is often not inherent in the particular req but rather due to other forms of interference, for example, in a Google benchmark that reads values for 1000 keys stored in a BigTable table distributed across 100 different servers, sending a hedging req after a 10ms delay reduces the 99.9th-percentile latency for retrieving all 1000 values from 1800ms to 74ms while sending just 2% more reqs, the overhead of hedged reqs can be further reduced by tagging them as lower priority than the primary reqs; 
**tied reqs:** the hedged-reqs technique also has a window of vulnerability in which multiple servers can execute the same req unnecessarily, that extra work can be capped by waiting for the 95th-percentile expected latency before issuing the hedged reqs, but this approach limites the benefits to only a small fraction of reqs, permitting more aggressive use of hedged reqs with moderate resource consumption requires faster cancellation of reqs; a common source of variability is queueing delays on the server before a req begins exec, for many services, once a req is actually scheduled and begins exec, the variability of its completion time goes down substantially, it said that allowing a client to choose bt 2 servers based on queue lengths at enqueue time exponentially improves load-balancing perf over a uniform random scheme, we advocate not choosing but rather enqueueing copies of a req in multiple servers simultaneously and allowing servers to communicate upates on the status of these copies to each other, we call reqs where servers perform cross-server status updates "tied reqs", the simplest form of a tied req has the client send the req to 2 different servers, each tagged with the identity of the other server("tied"), when a req begins exec, it sends a cancellation msg to its counterpart, the corresponding req, if still enqueued in the other server, can be aborted immediately or deprioritized substantially; there is a brief window of one average network msg delay where both servers may start executing the req while the cancellation msgs are both in flight to the other server, a common case where this situation can occur is if both server queues are completely empty, it is useful therefore for client to introduce a small delay of 2 times the average network msg delay(1ms or less in modern datacenter networks) bt sending the first req and sending the second req; Google's implementation of this technique is the context of its cluster-level distributed file system is effective at reducing both median and tail latencies, [click for the times for servicing a small read req from a BigTable where the data is not cached in mem but must be read from the underlying file system, each file chunk has 3 replicas on distinct machines](./img/read-latencies-observed-in-a-bigtable-service-benchmark.png), table2 includes read latencies observed with and without tied reqs for 2 scenarios -the first is a cluster in which the benchmark is running in isolation, in which case latency variability is mostly from self-interference and regular cluster-mgmt activities, in it, sending a tied req that does cross-server cancellation to another file system replica following 1ms reduces median latency by 16% and is increasingly effective along the tail of the latency distribution, achieving nearly 40% reduction at the 99.9th-percentile latency, the second scenario is like the first except there is also a large concurrent sorting job running on the same cluster contending for the same disk resources in the shared file system, although overall latencies are somewhat higher due to higher utilization, similar reductions in the latency profile are achieved with the tied-req technique discussed earlier, the latency profile with tied reqs while running a concurrent large sorting job is nearly identical to the latency profile of a mostly idle cluster without tied reqs, tied reqs allow wkloads to be consolidated into a single cluster, resulting in dramatic computing cost reductions, in both table2 scenarios, the overhead of tied reqs in disk utilization is less than 1%, indicating the cancellation strategy is effective at eliminating redundant reads; an alternative to the tied-req and hedged-req schemes is to probe remote queues first, then submit the req to the least-loaded server, it can be beneficial but is less effective than submitting work to 2 queues simultaneously for 3 main reasons including: (i)load levels can change bt probe and req time, (ii)req service times can be difficult to estimate due to underlying system and hw variability, (iii)clients can create temporary hot spots by all clients picking the same(least-loaded) server at the same time; the Distributed Shortest-Positioning Time First system uses another variation in which the req is sent to one server and forwarded to replicas only if the initial server does not have it in its cache and uses cross-server cancellations; worth noting is this technique is not restricted to simple replication but is also applicable in more-complex coding schemes(such as Reed-Solomon) where a primary req is sent to the machine with the desired data block, and, if no response is received following a brief delay, a collection of reqs is issued to a subset of the remaining replication group sufficient to reconstruct the desired data, with the whole ensemble forming a set of tied reqs; note: the class of techniques described here is effective only when the phenomena that causes variability does not tend to simultaneously affect multiple req replicas, we expect such uncorrelated phenomena are rather common in large-scale systems.
### Cross-Req Long-Term Adaptations
here we turn to techniques that are applicable for reducing latency variability caused by coarser-grain phenomena(such as service-time variations and load imbalance), although many systems try to partition data in such a way that the partitions have equal cost, a static assignment of a single partition to each machine is rarely sufficient in practice for 2 reasons -first the perf of the underlying machines is neither uniform nor constant overtime, for reasons(such as thermal throttling and shared wkload interference) mentioned earlier, and, outliers in the assignment of items to partitions can cause data-induced load imbalance(such as when a particular item becomes popular and the load for its partition increases); 
**micro-partitions:** to combat imbalance, many of Google's systems generate many more partitions in the service, then do dynamic assignment and load balancing of these partitions to particular machines, load balancing is then a matter of moving responsibility for one of these small partitions from one machine to another, say, with an average of 20 partitions per machine, the system can shed load in roughly 5% increments and in 1/20th the time it would take if the system simply had a one2one mapping of partitions to machines, the BigTable distributed-storage system stores data in tablets, with each machine managing bt 20-1000 tablets at a time, failure-recovery speed is also improved through micro-partitioning, since many machines pick up one unit of work when a machine failure occurs, this method of using micro-partitions is similar to the virtual servers notion and the virtual-processor-partitioning technique; 
**selective replication:** an enhancement of the micro-partitioning scheme is to detect or even predict certain items that are likely to cause load imbalance and create additional replicas of these items, load-balancing systems can then use additional replicas to spread the load of these hot micro-partitions across multiple machines without having to actually move micro-partitions; Google's web search system uses this approach, making additional components in multiple micro-partitions, at various times in Google's web search system's evolution, it has also created micro-partitions biased toward particular document langs and adjusted replication of these micro-partitions as the mix of query langs changes through the course of a typical day, query mixes can also change abruptly, say, as when an Asian datacenter outage causes a large fraction of Asian-lang queries to be directed to a North America facility, materially changing its working behavior; 
**latency-induced probation:** by observing the latency distribution of responses from the various machines in the system, intermediate servers sometimes detect situations where the system performs better by excluding a particularly slow machine, or putting it on probation, the source of the slowness is frequently temporary phenomena like interference from unrelated networking traffic or a spike in cpu activity for another job on the machine, and the slowness tends to be noticed when the system is under greater load, however, the system continues to issue shadow reqs to these excluded servers, collecting stats on their latency so they can be reincorporated into the service when the problem abates, this situation is somewhat peculiar, as removal of serving capacity from a live system during periods of high load actually improves latency.
### Large IR Systems
in ir -information retrieval systems, speed is more than a perf metric, it is a key quality metric, as returning good results quickly is better than returning the best results slowly, 2 techniques apply to such systems, as well as other to systems that inherently deal with imprecise results: (i)good enough -in large ir systems, once a sufficient fraction of all the leaf servers has responded, user may be best served by being given slightly incomplete("good-enough") results in exchange for better end2end latency, the chance thta a particular leaf server has the best result for the query is less than 1 in 1000 queries, odds further reduced by replicating the most important documents in the corpus into multiple leaf servers, since waiting for exceedingly slow servers might stretch service latency to unacceptable levels, Google's ir systems are tuned to occasionally respond with good-enough results when an acceptable fraction of overall corpus has been searched, while being careful to ensure good-enough results remain rare, in general, good-enough schemes are also used to skip nonessential subsystems to improve responsiveness, for example, results from ads or spelling-correction systems are easily skipped for web searches if they do not respond in time; (ii)canary reqs -another problem that can occur in systems with very high fan-out is that a particular req exercises an untested code path, causing crashes or extremely long delays on thousands of servers simultaneously, to prevent such correlated crash scenarios, some of Google's ir systems employ a technique called canary reqs -rather than initially send a req to thousands of leaf servers, a root server sends it first to 1 or 2 leaf servers, the remaining servers are only queried if the root gets a successful response from the canary in a reasonable period of time, if the server crashes or hangs while the canary req is outstanding, the system flags the req as potentially dangerous and prevents further exec by not sending it to the remaining leaf servers, canary reqs provide a measure of robustness to backends in the face of difficult-to-predict programming errors, as well as malicious denial-of-service attacks; the canary req phase adds only a small amount of overall latency because the system must wait for only a single server to respond, producing much less variability than if it had to wait for all servers to respond for large fan-out reqs, compare the first and last rows in [table1](./img/component-level-variability-amplified-by-scale-example.png), despite the slight increase in latency caused by canary reqs, such reqs tend to be used for every req in all of Google's large fan-out search systems due to the additional safety they provide.
### Mutations
the techniques we have discussed so far are most applicable for ops that do not perform critical mutations of the system's state, which covers a broad range of data-intensive services, tolerating latency variability for ops that mutate state is somewhat easier for a number of reasons including: (i)scale of latency-critical modifications in these services is generally small, (ii)updates can often be performed off the critical path, after responding to user, (iii)many services can be structured to tolerate inconsistent update models for(inherently more latency-tolerant) mutations, (iv)for those services that require consistent updates, the most commonly used techniques are quorum-based algorithms(such as Lamport's Paxos), since these algorithms must commit to only 3-5 replicas, they are inherently tail-tolerant.
### 即使是罕见的性能故障也会影响大规模分布式系统中很大一部分请求；在大规模系统中消除所有延迟变化的资源是不切实际的，尤其是在共享环境中；使用类似于容错计算的方法，尾部容错软件技术将难以预测的部分组成一个可预测的整体。
### 抑制延迟变化的一个简单方法是向多个副本发出相同的请求，并使用最先响应的副本的结果。
快速响应用户操作（100 毫秒内）的系统比那些需要更长时间的系统让用户感觉更流畅、更自然。互联网连接的改进和仓库规模计算系统的兴起使得 Web 服务能够提供流畅的响应能力，同时查询跨越数千台服务器的多 TB 数据集，例如，Google 搜索系统会根据用户输入以交互方式更新查询结果，根据迄今为止输入的前缀预测最可能的查询，执行搜索并在几十毫秒内显示结果；新兴的增强现实 (AR) 设备（例如谷歌眼镜原型）将需要具有更高响应速度的相关 Web 服务，以保证无缝的交互体验；
随着系统规模和复杂度的扩大，或总体使用量的增加，服务提供商很难保持交互式服务的延迟分布尾部较短，临时的高延迟事件（在中等规模的系统中并不重要）可能会在大规模系统中主导整体服务性能。正如容错计算旨在用可靠性较低的部分创建一个可靠的整体一样，大型在线服务需要用可预测性较低的部分创建一个可预测的响应整体，我们将这样的系统称为延迟尾部容忍系统，或简称为尾部容忍系统；在这里，我们概述了大型在线服务中高延迟事件的一些常见原因，并描述了一些降低其严重性或减轻其对整个系统性能影响的技术。在许多情况下，尾部容错技术可以利用已部署的资源来实现容错，从而降低额外开销。我们将探讨这些技术如何在不延长延迟尾部的情况下提高系统利用率，从而避免浪费的过度配置。
### 为什么存在可变性？
导致服务各个组件尾部延迟过高的原因有很多，包括：(i) 共享资源 - 不同的应用程序可能共享机器，从而争用共享资源（例如 CPU 核心、处理器缓存、内存带宽和网络带宽），并且在同一个应用程序中，不同的请求可能会争用资源；(ii) 守护进程 - 守护进程平均使用资源有限，但在调度时可能会产生多毫秒的故障； （iii）全局资源共享 - 在不同机器上运行的应用程序可能会争夺全局资源（例如网络交换机和共享文件系统）；（iv）维护活动 - bg 活动（例如分布式文件系统中的数据重建、BigTable 等存储系统中的定期日志压缩以及垃圾收集语言中的定期垃圾收集）可能会导致延迟的周期性峰值；（v）排队 - 中间服务器和网络交换机中的多层排队会放大这种变化； (vi) 可变性增加也是由于多种硬件趋势造成的，包括：功率限制 - 现代 CPU 设计为暂时以高于其平均功率范围的功率运行，如果这种活动持续很长时间，则通过节流来减轻热效应；垃圾收集 - 固态存储设备提供非常快的随机读取访问，但需要定期对大型数据块进行垃圾收集，即使是适度的写入活动，读取延迟也会增加 100 倍；能源管理 - 许多类型设备中的节能模式可以节省大量能源，但在从非活动模式切换到活动模式时会增加额外的延迟。
### 组件级可变性因规模而放大
减少大规模在线服务延迟的一种常用技术是在许多不同的机器上并行化子操作，其中每个子操作与其在大型数据集中的部分位于同一位置；并行化通过将请求从根服务器分散到大型叶服务器并通过请求分布树合并响应来实现，这些子操作必须在严格的截止期限内完成，服务才能响应；各个组件的延迟分布差异在服务级别被放大，例如，考虑一个系统，其中每个服务器通常在 10 毫秒内响应，但 99% 的延迟为 1 秒，如果一个用户请求仅在一个这样的服务器上处理，那么 100% 的用户请求就会很慢（1 秒）。[点击查看服务级延迟如何受到此假设场景中极少量延迟异常值的影响](./img/component-level-variability-amplified-by-scale-example.png)，如果一个用户请求需要从 100 台这样的服务器并行收集响应，那么 63% 的用户请求将花费超过 1 秒（图中标记为“x”）；即使对于单台服务器级别上只有万分之一请求延迟超过 1 秒的服务，对于拥有 2000 台此类服务器的服务，几乎五分之一的用户请求将花费超过 1 秒（图中标记为“o”）；表 1 列出了与此理想化场景逻辑上类似的 Google 真实服务的测量结果。根服务器通过中间服务器将请求分发到数量众多的叶子服务器。表 1 显示了大扇出对延迟分布的影响。在根节点测量的单个随机请求完成的第 99 百分位延迟为 10 毫秒，但所有请求完成的第 99 百分位延迟为 140 毫秒，95% 请求完成的第 99 百分位延迟为 70 毫秒，这意味着等待最慢的 5% 请求完成占总第 99 百分位延迟的一半。专注于这些缓慢异常值的技术可以显著降低整体服务性能；资源的过度配置、精心的软件实时工程以及更高的可靠性都可以在所有级别和所有组件中使用，以减少造成差异的根本原因。接下来，我们将介绍一些有助于减少服务响应差异的通用方法。
### 降低组件差异性
通过确保交互式请求得到及时服务，可以降低交互式响应时间的差异性，这需要通过许多小型工程决策来实现，包括：（i）区分服务类别和更高级别的队列 - 区分服务类别可用于优先调度用户正在等待的请求，而非非交互式请求；保持低级队列较短，以便更高级别的策略更快地生效，例如，Google 集群级文件系统中的存储服务将少量操作保留在操作系统的磁盘队列中，而不是维护自己的待处理磁盘请求优先级队列。这种浅队列允许服务器在处理延迟密集型批处理操作的旧请求之前发出传入的高优先级交互式请求； (ii)减少队头阻塞-高级服务可以处理具有广泛内在成本的请求，有时系统将长时间运行的请求分解成一系列较小的请求以允许交错执行其他短期运行的请求是有用的，例如，谷歌的网络搜索系统使用这种时间分片来防止小型#very 计算昂贵的查询向大型#concurrent 更便宜的查询添加大量的延迟; (iii)管理 bg 活动和同步中断-bg 任务可以创建显着的 CPU、磁盘或网络负载，例如面向日志的存储系统中的日志压缩和垃圾收集语言中的垃圾收集器活动; 节流、将重量级操作分解成更小的操作以及在整体负载较低时触发此类操作的组合通常能够减少 bg 活动对交互式请求延迟的影响;对于大型扇出服务，系统有时需要在许多不同的机器上同步 bg 活动。这种同步会在每台机器上同时强制执行短暂的活动爆发，只会减慢在 bg 活动短暂期间处理的交互请求。相反，如果没有同步，一些机器会始终执行一些 bg 活动，从而延长所有请求的延迟尾部；
到目前为止，本讨论中尚未提及缓存。虽然有效的缓存层可能很有用，甚至在某些系统中是必需的，但它们并不能直接解决尾部延迟问题，除非配置可以保证应用程序的整个工作集可以驻留在缓存中。
### 应对延迟变化
上一节中提到的严谨的工程技术对于构建高性能交互式服务至关重要，但现代 Web 服务的规模和复杂性使得消除所有延迟变化变得不可行。即使在隔离环境中可以实现这种完美的行为，共享计算资源的系统的性能波动也超出了应用程序开发人员的控制范围。因此，Google 发现开发能够掩盖或解决暂时性延迟问题的尾部容忍技术是有利的。与其试图完全消除它们，不如将这些技术分为两大类：第一类对应于在请求内立即响应的技术，其运行时间在数十毫秒内，在长期延迟技术有机会做出反应之前；第二类是跨请求的长期自适应技术，其运行时间在数十秒到几分钟内，旨在掩盖长期延迟的影响。
### 在请求内短期自适应
许多 Web 服务会部署数据项的多个副本，在出现故障的情况下保持额外的吞吐量和维护性，这种方法在大多数请求操作主要是只读、松散一致的数据集时特别有效，一个例子是拼写校正服务，它的模型每天更新一次，同时每秒处理数千个校正请求，类似地，分布式文件系统可能具有给定数据块的多个副本，这些副本都可以用于服务读取请求，这里的技术展示了如何使用复制来减少单个更高级别请求中的延迟变化，包括：
**对冲请求**：抑制延迟变化的一种简单方法是向多个副本发出相同的请求，并使用最先响应的副本的结果，我们将此类请求称为“对冲请求”，因为客户端首先向被认为最合适的副本发送一个请求，但在短暂的延迟后又返回发送第二个请求，一旦收到第一个结果，客户端就会取消剩余的未完成请求，尽管这种技术的简单实现通常会增加不可接受的额外负载，但存在许多变体在仅适度增加负载的情况下，实现大部分延迟减少效果；一种方法是推迟发送次要请求，直到第一个请求的未完成时间超过此类请求的第 95 个百分位预期延迟，这种方法将额外负载限制在 5% 左右，同时显著缩短延迟尾部，该技术之所以有效，是因为延迟的来源通常不是特定请求所固有的，而是由于其他形式的干扰，例如，在 Google 基准测试中，该基准读取分布在 100 台不同服务器上的 BigTable 表中存储的 1000 个键的值，在延迟 10 毫秒后发送对冲请求可将检索所有 1000 个值的 99.9 个百分位延迟从 1800 毫秒减少到 74 毫秒，同时仅多发送 2% 的请求，可以通过将对冲请求标记为比主要请求优先级更低来进一步降低其开销；
**绑定请求**：对冲请求技术也有一个漏洞窗口，其中多个服务器可以不必要地执行相同的请求，可以通过在发出对冲请求之前等待第 95 个百分位的预期延迟来限制额外的工作，但这种方法将好处限制在只有一小部分请求上，允许更积极地使用对冲请求，适度的资源消耗需要更快地取消请求；一个常见的变化来源是在请求开始执行之前服务器上的排队延迟，对于许多服务而言，一旦请求实际被安排并开始执行，其完成时间的变化就会大大降低，据说允许客户端在排队时根据队列长度选择 2 个服务器，可以成倍地提高均匀随机方案的负载平衡性能，我们主张不要选择，而是同时在多个服务器中排队请求的副本，并允许服务器相互传达这些副本的状态更新，我们将服务器执行跨服务器状态更新的请求称为“绑定请求”，绑定请求的最简单形式是客户端将请求发送到 2 个不同的服务器，每个服务器都标有另一个服务器的身份（“绑定”），当请求开始执行时，它会向其对应方发送取消消息，如果相应的请求仍在另一个服务器中排队，则可以立即中止或大幅降低优先级；有一个短暂的平均网络消息延迟窗口，在此期间两个服务器可能开始执行请求，而取消消息都在向另一个服务器发送，这种情况可能发生的常见情况是两个服务器队列都完全为空，因此对于客户端来说，在发送第一个请求和发送第二个请求之间引入 2 倍平均网络消息延迟（在现代数据中心网络中为 1 毫秒或更短）的小延迟是有用的； Google 在其集群级分布式文件系统的背景下实施了这项技术，可以有效减少中值延迟和尾部延迟，[单击查看从 BigTable 处理小规模读取请求的时间，其中数据未缓存在内存中，但必须从底层文件系统读取，每个文件块在不同的机器上都有 3 个副本](./img/read-latencies-observed-in-a-bigtable-service-benchmark.png)，表 2 包括在两种情况下观察到的有和无绑定请求的读取延迟 - 第一种是基准测试独立运行的集群，在这种情况下，延迟变化主要来自自身干扰和常规集群管理活动，其中，在 1ms 之后向另一个文件系统副本发送跨服务器取消的绑定请求可将中值延迟降低 16%，并且在延迟分布的尾部越来越有效，在 99.9 百分位延迟处实现了近 40% 的减少，第二种情况与第一种类似，只是还有一个在同一台集群上运行大型并发排序作业，在共享文件系统中争用相同的磁盘资源，虽然由于利用率较高，整体延迟会有所增加，但使用前面讨论过的 Tied-req 技术也可以实现类似的延迟降低，在运行并发大型排序作业时，使用 Tied reqs 的延迟配置文件几乎与没有 Tied reqs 的大部分空闲集群的延迟配置文件相同，Tied reqs 允许将工作负载合并到单个集群中，从而显着降低计算成本，在表 2 的两个场景中，Tied reqs 在磁盘利用率方面的开销均小于 1%，表明取消策略对于消除冗余读取是有效的； Tied-req 和 Heged-req 方案的替代方法是首先探测远程队列，然后将 req 提交给负载最小的服务器，这种方法虽然有益，但不如同时向 2 个队列提交工作有效，主要有 3 个原因：（i）负载水平可以通过探测和 req 时间改变，（ii）由于底层系统和硬件变化，req 服务时间可能难以估计，（iii）客户端可以通过所有客户端同时选择相同（负载最小）的服务器来创建临时热点；分布式最短定位时间优先系统使用另一种变体，其中 req 被发送到一台服务器，并且只有当初始服务器的缓存中没有该 req 时才转发到副本，并且使用跨服务器取消；值得注意的是，这种技术不仅限于简单的复制，还适用于更复杂的编码方案（例如 Reed-Solomon），其中将主要请求发送到包含所需数据块的机器，如果在短暂延迟后未收到响应，则将一组请求发送到剩余复制组的子集，足以重建所需数据，整个集合形成一组绑定的请求；注意：此处描述的技术类别仅在导致变异的现象不会同时影响多个请求副本时才有效，我们预计这种不相关的现象在大型系统中相当常见。
### 跨请求长期自适应
这里我们讨论一些适用于减少由粗粒度现象（例如服务时间变化和负载不平衡）引起的延迟变化的技术。尽管许多系统尝试以分区成本相等的方式对数据进行分区，但在实践中，将单个分区静态分配给每台机器很少能满足需求，原因有二：首先，由于前面提到的一些原因（例如热节流和共享负载干扰），底层机器的性能随时间变化既不均匀也不恒定；其次，在将项目分配到分区时出现的异常值可能会导致数据引起的负载不平衡（例如，当某个项目变得流行并且其分区的负载增加时）；
**微分区**：为了应对不平衡，许多 Google 系统会在服务中生成更多分区，然后对这些分区进行动态分配和负载均衡，使其分配到特定的机器上。负载均衡就是将其中一个小分区的职责从一台机器转移到另一台机器上。假设每台机器平均有 20 个分区，系统可以以大约 5% 的增量减少负载，所需时间仅为系统简单地将分区与机器进行一对一映射所需时间的二十分之一。BigTable 分布式存储系统将数据存储在 Tablet 中，每台机器一次管理 20-1000 个 Tablet。微分区还可以提高故障恢复速度，因为当一台机器发生故障时，许多机器会接手一个工作单元。这种使用微分区的方法类似于虚拟服务器的概念和虚拟处理器分区技术；
**选择性复制**：微分区方案的一项增强功能是检测甚至预测可能导致负载不平衡的某些项目，并创建这些项目的额外副本。负载平衡系统可以使用额外的副本将这些热微分区的负载分散到多台机器上，而无需实际移动微分区；谷歌的网络搜索系统就采用了这种方法，在多个微分区中创建了额外的组件。在谷歌网络搜索系统发展的不同时期，它还创建了偏向特定文档语言的微分区，并根据查询语言组合在一天中的变化调整这些微分区的复制。查询组合也可能突然发生变化，例如，当亚洲数据中心发生故障导致大量亚洲语言查询被定向到北美数据中心时，会显著改变其工作行为；
**延迟诱导试用**：通过观察系统中各个机器的响应延迟分布，中间服务器有时可以通过排除一台特别慢的机器，或者让它处于试用状态的方式，检测到系统性能提升，速度缓慢的根源通常是暂时现象，例如来自不相关的网络流量的干扰或机器上另一个作业的 CPU 活动激增，并且当系统负载较大时，速度缓慢往往会被注意到，但是，系统会继续向这些被排除的服务器发出影子请求，收集有关其延迟的统计信息，以便在问题减轻时将它们重新合并到服务中，这种情况有点奇怪，因为在高负载期间从实时系统中移除服务容量实际上会改善延迟。
### 大型信息检索系统
在信息检索系统中，速度不仅仅是一个性能指标，更是一个关键的质量指标，因为快速返回好的结果比缓慢返回最佳结果更好。有两种技术适用于此类系统，以及其他一些处理不精确结果的系统：（i）足够好 - 在大型信息检索系统中，一旦所有叶子服务器中有足够多的部分响应，用户可能会得到略微不完整（“足够好”）的结果，以换取更好的端到端延迟，特定叶子服务器获得查询最佳结果的概率小于千分之一，通过将语料库中最重要的文档复制到多个叶子服务器，概率会进一步降低，因为等待速度极慢的服务器可能会将服务延迟延长到不可接受的水平。谷歌的信息检索系统经过调整，当搜索了可接受比例的整体语料库时，偶尔会返回足够好的结果，同时会小心确保“足够好”的结果通常很少出现。足够好的方案也用于跳过不必要的子系统以提高响应能力，例如，如果广告或拼写纠正系统没有及时响应，则很容易跳过网络搜索的结果； (ii) canary reqs - 在扇出率非常高的系统中可能出现的另一个问题是，特定的 req 执行未经测试的代码路径，导致数千台服务器同时崩溃或极长的延迟，为了防止这种相关的崩溃情况，Google 的一些 ir 系统采用了一种称为 canary reqs 的技术 - 而不是最初将 req 发送到数千个叶服务器，根服务器首先将其发送到 1 或 2 个叶服务器，只有当根服务器在合理的时间内从 canary 获得成功响应时，才会查询其余服务器，如果服务器在 canary req 未完成时崩溃或挂起，系统会将该 req 标记为潜在危险并通过不将其发送到剩余的叶服务器来防止进一步执行，canary reqs 在面对难以预测的编程错误以及恶意拒绝服务攻击时为后端提供了一定程度的稳健性；金丝雀发布请求阶段仅增加少量总体延迟，因为系统只需等待单个服务器响应，产生的变化比等待所有服务器响应大型扇出请求时要小得多，比较[table1](./img/component-level-variability-amplified-by-scale-example.png)中的第一行和最后一行，尽管金丝雀发布请求导致延迟略有增加，但由于它们提供的额外安全性，此类请求往往用于所有 Google 大型扇出搜索系统中的每个请求。
### 变更
到目前为止，我们讨论的技术最适用于不会对系统状态执行关键变更的操作，这涵盖了广泛的数据密集型服务，对于会变更状态的操作，容忍延迟变化要容易一些，原因如下：（i）这些服务中对延迟至关重要的修改规模通常较小；（ii）更新通常可以在关键路径之外执行，在响应用户之后；（iii）许多服务可以构建为容忍不一致的更新模型（本质上对延迟更具容忍度）；（iv）对于那些需要一致更新的服务，最常用的技术是基于仲裁的算法（例如 Lamport 的 Paxos），因为这些算法只需提交 3-5 个副本，因此它们本质上具有尾部容忍性。
****
### Data Lakes & Warehouses
### Lakehouse:A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics
### [Armbrust21](https://www.cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf)
we argue that the data warehouse architecture as we know it today will wither in the coming years and be replaced by a new architectural pattern, the Lakehouse, which will (i)be based on open direct-access data formats, such as Apache Parquet, (ii)have first-class support for mlds, (iii)offer state-of-the-art perf; Lakehouses can help address several major challenges with data warehouses, including data staleness, reliability, total cost of ownership, data lock-in, and limited use case support; we discuss hwo the industry is already moving toward Lakehouses and how this shift may affect work in data mgmt, we also report results from a Lakehouse system using Parquet that is competitive with popular cloud data warehouses on TPC-DS. 
the history of data warehousing started with helping business leaders get analytical insights by collecting data from operational dbs into centralized warehouses, which then could be used for decision support and business intelligence, data in these warehouses would be written with schema-on-write, which ensured that the data model was optimized for downstream BI consumption, we refer to this as the first-gen da platforms; a decade ago, the first-gen systems started to face several challenges including: (i)they typically coupled compute and storage into an on-premises appliance, this forced enterprises to provision and pay for peak of user load and data under mgmt, which became very costly as datasets grew, (ii)not only were datasets growing rapidly, but more and more datasets were completely unstructured, such as video, audio, and text documents, which data warehouses could not store and query at all; to solve these problems, the second-gen da platforms started offloading all raw data into data lakes :low-cost storage systems with a file api that hold data in generic and usually open file formats such as Apache Parquet, this approach started with the Apache Hadoop movement, using the hdfs for cheap storage, the data lake was a schema-on-read architecture that enabled agility of storing any data at low cost, but on the other hand, punted the problem of data quality and governance downstream, in this architecture, a small subset of data in the lake would later be ETLed to a downstream data warehouse such as Teradata for the most important decision support and BI applications, the use of open formats also made data lake data directly accessible to a wide range of other analytics engines such as ml systems; from 2015 onwards, cloud data lakes, such as S3, ADLS, GCS, started replacing hdfs, they have superior durability(often >10 nines), geo-replication, and extremely low cost with the possibility of automatic, even cheaper, archival storage, such as AWS Glacier, the rest of the architecture is largely the same in the cloud as in the second-gen systems, with a downstream data warehouse such as Redshift, Snowflake, this 2-tier data lake + warehouse architecture is now dominant in the industry in our experience(used at virtually all Fortune500 enterprises); this brings us to challenges with current data architectures, while cloud data lake and warehouse architecture is ostensibly cheap due to separate storage(such as S3) and compute(such as Redshift), a 2-tier architecture is highly complex for users :in the first-gen platforms all data was ETLed from operational data systems directly into a warehouse, vs., in today's architectures data is first ETLed into lakes and then again ETLed into warehouses, creating complexity, delays, and new failure modes, moreover, enterprise use cases now include advanced analytics such as ml, for which neither data lakes nor warehouses are ideal, specifically, today's data architectures commonly suffer from 4 problems including: (i) reliability: keeping data lake and warehouse consistent is difficult and costly, continuous engineering is required to ETL data bt the 2 systems and make it available to high-perf decision support and BI, each ETL step also risks incurring failures or introducing bugs that reduce data quality, such as due to subtle difference bt data lake and warehouse engines; 
(ii) data staleness: data in the warehouse is stale compared to that of data lake, with new data frequently taking days to load, this is a step back compared to the first-gen da systems, where new operational data was immediately available for queries, according to a survey by Dimensional Research and Fivetran, 86% of analysts use out-of-date data and 62% report waiting on engineering resources numerous times per month; 
(iii) limited support for advanced analytics: businesses want to ask predictive questions using their warehouse data, such as "which customers should I offer discounts to?" despite much research on the confluence of ml and data mgmt, none of the leading ml systems, such as Tensorflow, Pytorch, and XGBoost, work well on top of warehouses, unlike BI queries, which extract a small amount of data, these systems need to process large datasets using complex nonsql code, reading this data via ODBC/JDBC is inefficient, and there is no way to directly access the internal warehouse proprietary formats, for these use cases, warehouse vendors recommend exporting data to files, which further increases complexity and staleness(adding a third ETL step), alternatively, users can run these systems against data lake data in open formats, however, they then lose rich mgmt features from data warehouses, such as ACID transactions, data versioning and indexing; 
(iv) total cost of ownership: apart from paying for continuous ETL, users pay double the storage cost for data copied to a warehouse, and commercial warehouses lock data into proprietary formats that increase cost of migrating data or wkloads to other systems; 
a straw-man solution that has had limited adoption is to eliminate data lake altogether and store all data in a warehouse that has built-in separation of compute and storage, we will argue that this has limited variability as evidenced by lack of adoption, because it still does not support managing video/audio/text data easily or fast direct access from mlds wkloads; 
here we discuss the following technical question :is it possible to turn data lakes based on standard open data formats such as Parquet into high-perf systems that can provide both perf and mgmt features of data warehouses and fast direct io from advanced analytics wkloads? we argue that this type of system design, which we refer to as a [Lakehouse](./img/lakehouse.png), is both feasible and is already showing evidence of success, in various forms, in the industry, as more business applications start relying on operational data and on advanced analytics, we believe Lakehouse is a compelling design point that can eliminate some of the top challenges with data warehousing; in particular, we believe that time for Lakehouse has come due to recent solutions that address the following key problems including: (i) reliable data mgmt on data lakes: a Lakehouse needs to be able to store raw data, similar to today's data lakes, while simultaneously supporting ETL/ELT processes that curate this data to improve its quality for analysis, traditionally, data lakes have managed data as "just a bunch of files" in semi-structured formats, making it hard to offer some of the key mgmt features that simplify ETL/ELT in data warehouses, such as transactions, rollbacks to old table versions, and zero-copy cloning, however, a recent family of systems such as DeltaLake, Apache Iceberg, provide transactional views of a data lake, and enable these mgmt features, of course, orgs still have to do the hard work of writing ETL/ELT logic to create curated datasets with a Lakehouse, but there are fewer ETL steps overall, and analysts can also easily and performantly query raw data tables if they wish to, much like in first-gen analytics platforms; 
(ii) support for mlds: ml systems' support for direct access from data lake formats already places them in a good position to efficiently access a Lakehouse, in addition, many ml systems have adopted DataFrames as the abstraction for manipulating data, and recent systems have designed declarative DataFrame apis that enable performing query optimizations for data accesses in ml wkloads, these apis enable ml wkloads to directly benefit from many optimizations in Lakehouses; 
(iii) sql perf: Lakehouses will need to provide state-of-the-art sql perf on top of the massive Parquet datasets that have been amassed over the last decade(or in the long term, some other standard format that is exposed for direct access to applications), in contrast, classic data warehouses accept sql and are free to optimize everything under the hood, including proprietary storage formats, nonetheless, we show that a variety of techniques can be used to maintain auxiliary data about Parquet datasets and to optimize data layout within these existing formats to achieve competitive perf, we present results from a sql engine over Parquet(the Databricks Delta Engine) that outperforms leading cloud data warehouses on TPC-DS.
### Motivation :Data Warehousing Challenges
data warehouses are critical for many business processes, but they still regularly furstrate users with incorrect data, staleness, and high costs, we argue that at least part of each these challenges is "accidental complexity" from the way enterprise data platforms are designed, which could be eliminated with a Lakehouse; 
first, the top problem reported by enterprise data users today is usually data quality and reliability, implementing correct data pipelines is intrinsically difficult, but today's 2-tier data architectures with a separate lake and warehouse add extra complexity that exacerbates this problem, for example, data lake and warehouse systems might have different semantics in their supported data types, sql dialects, etc., data may be stored with different schemas in the lake and the warehouse(such as denormalized in one), and the increased #ETL/ELT jobs, spanning multiple systems, increases the prob of failures and bugs; 
second, more and more business applications require up2date data, but today's architectures increase data staleness by having a separate sharing area for incoming data before the warehouse and using periodic ETL/ELT jobs to load it, theoretically, orgs could implement more streaming pipelines to updata data warehouse faster, in the first-gen platforms warehouse users had immediate access to raw data loaded from operational systems in the same environment as derived datasets, business applications such as customer support systems and recommendation engines are simply ineffective with stale data, and even human analysts querying warehouses and their api do not easily support it; 
finally, most orgs are now deploying mlds applications, but these are not well served by data warehouses and lakes, as discussed before, these applications need to process large amounts of data with nonsql code, so they cannot run effectively over ODBC/JDBC, as advanced analytics systems continue to develop, we believe that giving them direct access to data in an open format will be the most effective way to support them, in addition, mlds applications suffer from the same data mgmt problems that classical applications do, such as data quality, consistency, and isolation, so there is immense value in bringing dbms features to their data; 
several currect industry trends give further evidence that customers are unsatisfied with the 2-tier lake + warehouse model including: first, in recent years virtually all major data warehouses have added support for external tables in Parquet format, this allows warehouse users to also query the data lake tables easier to manage and it does not remove ETL complexity, staleness, and advanced analytics challenges for data in the warehouse, in practice, these connectors also often perform poorly because sql engine is mostly optimized for its internal data format, second, there is also broad investment in sql engines that run directly against data lake storage, such as SparkSQL, Presto, Hive, and AWS Athena, however, these engines alone cannot solve all problems with data lakes and replace warehouses :data lakes still lack basic mgmt features such as ACID transactions and efficient access methods such as indexes to match data warehouse perf.
### Lakehouse Architecture
we define a Lakehouse as a data mgmt system based on low-cost and directly-accessible storage that also provides traditional analytical dbms mgmt and perf features such as ACID transactions, data versioning, auditing, indexing, caching, and query optimization, Lakehouses thus combine key benefits of data lakes and data warehouses :low-cost storage in an open format accessible by a variety of systems from the former, and powerful mgmt and optimization features from the latter, they key question is whether one can combine these benefits in an effective way :in particular, Lakehouses' support for direct access means that they give up some aspects of data independence, which has been a cornerstone of relational dbms design; 
we note that Lakehouses are an especially good fit for cloud environments with separate compute and storage :different computing applications can run on-demand on completely separate computing nodes(such as a gpu cluster for ml) while directly accessing the same storage data, however, one coudl also implement a Lakehouse over an on-premise storage system such as hdfs; 
in this section, we sketch one possible design for Lakehouse systems, based on 3 recent technical ideas that have appeared in various forms throughout the industry, we have been building towards a Lakehouse platform based on this design at Databricks through the Delta Lake, Delta Engine, and Databricks ML Runtime projects, other designs may also be visible, however, as are other concrete technical choices in our high-level design(such as our stack at Databricks currently builds on the Parquet storage format, but it is possible to design a better format), we discuss several alternatives and future directions for research; 
**implementing a Lakehouse system:** the first key idea we propose for implementing a Lakehouse is to have the system store data in a low-cost object store(such as Amazon S3) using a standard file format such as Apache Parquet, but implement a transactional metadata layer on top of the object store that defines which objects are part of a table version, this allows the system to implement mgmt features such as ACID transactions or versioning within the metadata layer, while keeping the bulk of data in low-cost object store and allowing clients to directly read objects from this store using a standard file format in most cases, several recent systems including DeltaLake, Apache Iceberg, have successfully added mgmt features to data lakes in this fashion, for example, DeltaLake is now used in about half of Databrick's wkload, by thousands of customers; although a metadata layer adds mgmt capabilities, it is not sufficient to achieve good sql perf, data warehouses use several techniques to get state-of-the-art perf such as storing hot data on fast devices such as SSDs, maintaining stats, building efficient access methods such as indexes, and co-optimizing data format and compute engine; in a Lakehouse based on existing storage formats, it is not possible to change the format, but we show that it is possible to implement other optimizations that leave the data files unchanged, including caching, auxiliary data structures such as indexes and stats, and data layout optimizations; finally, Lakehouses can both speed up advanced analytics wkloads and give them better data mgmt features thanks to the dev of declarative DataFrame APIs, many ml libs such as Tensorflow, Spark MLlib, can already read data lake file formats such as Parquet, hence the simplest way to integrate them with a Lakehouse would be to query the metadata layer to figure out which Parquet files are currently part of a table, and simply pass those to the ml lib, however, most of these systems support a DataFrame API for data preparation that creates more optimization opportunities; DataFrames were popularized by R and Pandas and simply give users a table abstraction with various transformation operators, most of which map to relational algebra, systems such as SparkSQL have made this api declarative by lazily evaluating transformations and passing the resulting operator plan to an optimizer, these apis can thus leverage the new optimization features in a Lakehouse, such as caches and auxiliary data, to further accelerate ml; [click for how these ideas fit together into a Lakehouse system design](./img/lakehouse-system-design.png); 
**metadata layers for data mgmt:** the first component that we believe will enable Lakehouse is metadata layers over data lake storage that can raise its abstraction level to implement ACID transactions and other mgmt features, data lake storage systems such as S3 or hdfs only provide a low-level object store or filesystem interface where even simple ops such as updating a table that spans multiple files, are not atomic; orgs soon began designing richer data mgmt layers over these systems, starting with Apache Hive ACID, which tracks which data files are part of a hive table at a given table version using an OLTP DBMS and allows ops to update this set transactionally; in recent years, new systems have provided even more capabilities and improved scalability, in 2016 Databricks began developing DeltaLake, which stores info about which objects are part of a tabel in the data lake itself as a transaction log in Parquet format, enabling it to scale to billions of objects per table, Apache Iceberg which started at Netflix, uses a similar design and supports Parquet storage, Apache Hudi which started at Uber, is another system in this area focused on simplifying streaming ingest into data lakes, although it does not support concurrent writers; experience with these systems has shown that they generally provide similar or better perf to raw Parquet data lakes, while adding highly useful mgmt features such as transactions, zero-copy coning, and time travel to past versions of a table, in addition, they are easy to adopt for orgs that already have a data lake, for example, DeltaLake can convert an existing dir of Parquet files into a DeltaLake table with zero copies just by adding a transaction log that starts with an entry that references all existing files, as a result, orgs are rapidly adopting these metadata layers, for example, DeltaLake grew to cover half the compute-hrs on Databricks in 3 yrs; in addition, metadata layers are a natural place to implement data quality enforcement features, for example, DeltaLake implements schema enforcement to ensure that data uploaded to a table matches its schema, and constraints api that allows table owners to set constraints on the ingested data(such as country can only be one of a list of values), Delta's client libs will automatically reject records that violate these expectations or quarantine them in a special location, customers have found these simple features very useful to improve quality of data lake based pipelines; finally, metadata layers are a natural place to implement governance features such as access control and audit logging, for example, a metadata layer can check whether a client is allowed to access a table before granting it credentials to read raw data in the table from a cloud object store, and can reliably log all accesses; 
**future directions and alternative design:** because metadata layers for data lakes are a fairly new dev, there are many open questions and alternative designs, for example, we designed DeltaLake to store its transaction log in the same object store that it runs over(such as S3) in order to simplify mgmt(removing the need to run a separate storage system) and offer high availability and high read bdwidth to the log(the same as the object store), however, this limits the rate of transactions per sec it can support due to object stores' high latency, a design using a faster storage system for the metadata may be preferable in some cases, likewise, DeltaLake, Iceberg, and Hudi only support transactions on 1 table at a time, but it should be possible to extend them to support cross-table transactions, optimizing the format of transaction logs and size of objects managed are also open questions; 
**sql perf in a Lakehouse:** perhaps the largest technical question with the Lakehouse approach is how to provide state-of-the-art sql perf while giving up a significant portion of the data independence in a traditional dbms design, the answer clearly depends on a number of factors including: what hw resources we have available(such as can we implement a caching layer on top of the object store), and whether we can change the data object storage format instead of using existing standards such as Parquet(new designs that improve over these formats continue to emerge), however, regardless of the exact design the core challenge is that the data storage format becomes part of the system's public api to allow fast direct access, unlike in a traditional dbms; we propose several techniques to implement sql perf optimizations in a Lakehouse independent of the chosen data format, which can therefore be applied either with existing or future formats, we have also implemented these techniques within the Databricks Delta Engine, and show that they yield competitive perf with popular cloud data warehouses, though there is plenty of room for further perf optimizations, these format-independent optimization are: (i) caching: when using a traditional metadata layer such as DeltaLake, it is safe for a Lakehouse system to cache files from cloud object store on faster storage devices suhc as SSDs and RAM on the processing nodes, running transactions can easily determine when cached files are still valid to read, moreover, cache can be in a transcoded format that is more efficient for the query engine to run on, matching any optimization that would be used in a traditional "closed-world" data warehouse engine, for example, our cache at Databricks partially decompresses the Parquet data it loads; 
(ii) auxiliary data: even though a Lakehouse needs to expose the base table storage format for direct io, it can maintain other data that helps optimize queries in auxiliary files that it has full control over, in DeltaLake and Delta Engine, we maintain column min-max stats for each data file in the table within the same Parquet file used to store the transaction log, which enables data skipping optimizations when the base data is clustered by particular columns, we are also implementing a Bloom filter based index, one can imagine implementing a wide range of auxiliary data structures here, similar to proposals for indexing "raw" data; 
(iii) data layout: data layout plays a large role in access perf, even when we fix a storage formate such as Parquet, there are multiple layout decisions that can be optimized by Lakehouse system, the most obvious is recorder ordering -which records are clustered together and hence easiest to read together, in DeltaLake we support ordering records using individual dims or space-filling curves such as Z-order, Hilbert curves, to provide locality across multiple dims, one can also imagine new formats that support placing columns in different orders within each data file, choosing compression strategies differently for various groups of records, or other strategies; these 3 optimizations work especially well together for the typical access patterns in analytical systems, in typical networks most queries tend to be concentrated against a hot subset of data, which Lakehouse can cache using the same optimized data structures as a closed-world data warehouse to provide competitive perf; for cold data the a cloud object store, the main determinant of perf is likely to be the amount of data read per query, in that case, the combination of data layout optimizations(which cluster co-accessed data) and auxiliary data structures such as zone maps(which let the engine rapidly figure out what ranges of data files to read) can allow a Lakehouse system to minimize io the same way a closed-world proprietary data warehouse would, despite running against a standard open file format; 
(iv) perf results: at Databricks we combined these 3 Lakehouse optimizations with a new C++ exec engine for Apache Spark called Delta Engine, to evaluate the feasibility of Lakehouse architecture, [click for comparing Delta Engine on TPC-DS at scale factor 30000 with 4 widely used cloud data warehouses(from cloud providers as well as third-party companies that run over public clouds)](./img/delta-engine.png), using comparable clusters on AWS, Azure, GCP with 960 vGPUs each and local SSD storage(*we started all systems with data cached on SSDs when applicable, because some of the warehouses we compared with only supported node-attached storage, however, Delta Engine was only 18% slower when starting with a cold cache), we report the time to run all 99 queries as well as total cost for customers in each service's pricing model(Databricks lets users choose spot and on-demand instances, so we show both), Delta Engine provides comparable or better perf than these systems at a lower price point; 
(v) future directions and alternative designs: designing performant yet directly-accessible Lakehouse systems is a rich area for future work, one clear direction that we have not explored yet is designing new data lake storage formats that will work better in this use case, such as formats that provide more flexibility for Lakehouse system to implement data layout optimizations or indexes over or are simply better suited to modern hw, of course, such new formats may take a while for processing engines to adopt, limiting #clients that can read from them, but desiging a high quality directly-accessible open fortmat for next gen wkloads is an important research problem; even without changing data format, there are many types of caching strategies, auxiliary data structures, and data layout strategies to explore for Lakehouses, determining which ones are likely to be most effective for massive datasets in cloud object stores is an open question; finally, another exicting research direction is determining when and how to use serverless computing systems to answer queries and optimizing the storage, metadata layer, and query engine designs to minimize latency in this case; 
**efficient access for advanced analytics:** (i) as we discussed earlier, advanced analytics libs are usually written using imperative code that cannot run as sql, yet the need to access large amount of data, there is an interesting research question in how to design data access layers in these libs to maximize flexibility for the node running on top but still benefit from optimization opportunities in a Lakehouse; one approach that we have had success with is offering a declarative version of DataFrame APIs used in such libs, which maps data preparation computations into SparkSQL query plans and can benefit from optimizations in DeltaLake and Delta Engine, we used this approach in both Spark DataFrames and in Koalas, a new DataFrame API for Spark that offers improved compatibility with Pandas, DataFrames are the main data type used to pass input into the ecosystem of advanced analytics libs for Apache Spark, including MLlib, GraphFrames, SparkR, and many community libs, so all of these wkloads can enjoy accelerated io if we can optimize the DataFrame computation; Spark's query planner pushes selections and projections in the user's DataFrame computation directly into the "data source" plugin class for each data source read, [hence in our implementation of DeltaLake data source, we leverage caching, data skipping, and data layout optimizations described in section sql perf in a Lakehouse to accelerate these reads from DeltaLake and thus accelerate mlds wkloads, click to view](./img/dataframe-api.png); however, ml apis are quickly evolving, and there are also other data access apis, such as Tensorflow's tf.data, that do not attempt to push query semantics into underlying storage system, many of these apis also focus on overlapping data loading on the cpu with cpu2gpu transfers and gpu computation, which has not received much attention in data warehouses, recent systems work has shown that keeping modern accelerators well-utilized, especially for ml inference, can be a difficult problem, so Lakehouse access libs will need to tackle this challenge; 
(ii) future directions and alternative designs: apart from the questions about existing apis and efficiency that we have just discussed, we can explore radically different designs for data access interfaces from ml, for example, recent work has proposed factorized ml frameworks that push ml logic into sql joins, and other query optimizations that can be applied for ml algorithms implemented in sql; finally, we still need standard interfaces to let data scientists take full advantage of powerful data mgmt capabilities in Lakehouse(or even data warehouses), for example at Databricks we have integrated DeltaLake with ml experiment tracking service in MLflow to let data scientists easily track the table versions used in an experiment and reproduce that version of data later; there is also an emerging abstraction of feature stores in the industry as a data mgmt layer to store and update features used in ml application, which would benefit from using the standard dbms functions in a Lakehouse design, such as transactions and data versioning.
### Research Questions and Implications
beyond research challenges that we raised as future directions, Lakehouse raise several other research questions, in addition, the industry trend towards increasingly feature-rich data lakes has implications for other areas of data systems research; 
**are there other ways to achieve Lakehouse goals?** one can imagine other means to achieve primary goals of Lakehouse such as building a massively parallel serving layer for a data warehouse that can support parallel reads from advanced analytics wkloads, however, we believe that such infra will be significantly more expensive to run, harder to manage, and likely less performant than giving wkloads direct access to the object store; we have not seen broad deployment of systems that add this type of serving layer, such as Hive LLAP, moreover, this approach punts the problem of selecting an efficient data format for reads to the serving layer, and this format still needs to be easy to transcode from warehouse's internal format, the main draws of cloud object stores are their low cost, high bdwidth access from elastic wkloads, and extremely high availability, all these get worse with a separate serving layer in front of the object store; beyond the perf, availability, cost, and lock-in challenges with these alternative approaches, there are also important governance reasons why enterprises may prefer to keep their data in an open format, with increasing regulatory requirements about data mgmt, orgs may need to search through old datasets, delete various data, or change their data processing infra on short notice, and standardizing on an open format means that they will always have direct access to data without blocking on a vendor, the long-term trend in sw industry has been towards open data formats, and we believe that this trend will continue for enterprise data; 
**what are the right storage formats and access apis?** the access interface to a Lakehouse includes raw storage format, client libs to directly read this format(such as when reading into Tensorflow), and a high-level sql interface, there are many different ways to place rich functionality across these layers, such as storage schemes that provide more flexibility to the system by asking readers to perform more sophisticated, programmable decoding logic; it remains to be seen which combination of storage formats, metadata layer designs, and access apis works best; 
**how does Lakehouse affect other data mgmt research and trends?** the prevalence of data lakes and the increasing use of rich mgmt interfaces over them, whether they be metadata layers or the full Lakehouse design, has implications for several other areas of data mgmt research including: (i)polystores were designed to solve the difficult problem of querying data across disparate storage engines, this problem will persist in enterprises, but the increasing fraction of data that is available in an open format in a cloud data lake means that many polystore queries could be answered by running directly against the cloud object store, even if the underlying data files are part of logically separate Lakehouse deployments; (ii)data integration and cleaning tools can also be designed to run in place over a Lakehouse with fast parallel access to all the data, which may enable new algorithms such as running large joins and clustering algorithms over many of the datasets in an org; (iii)HTAP systems could perhaps be built as bolt-on layers in front of a Lakehouse by archieving data directly into a Lakehouse system using it transaction mgmt apis, the Lakehouse would be able to query consistent snapshots of the data; (iv)data mgmt for ml may also become simpler and more powerful if implemented over a Lakehouse, today, orgs are building a wide range of ml-specific data versioning and feature store systems that implement standard dbms functionality, it might be simpler to just use a lake house abstraction with dbms mgmt functions built-in to implement feature store functionality, at the same time, declarative ml systems such as factorized ml could likely run well against a Lakehouse; (v)cloud-native dbms designs such as serverless engines will need to integrate with richer metadata mgmt layers such as DeltaLake instead of just scanning over raw files in a data lake, but may be able to achieve increased perf; (vi)finally, there is ongoing discussion in the industry about how to organize data engineering processes and teams, with concepts such as the data mesh, where separate teams own different data products end2end, gaining popularity over traditional central data team approach, Lakehouse designs lend themselves easily to distributed collaboration structures because all datasets are directly accessible from an object store without having to onboard users on the same compute resources, making it straightforward to share data regardless of which teams produce and consume it.<br \>
我们认为，我们今天所知的数据仓库架构将在未来几年逐渐衰落，并被一种新的架构模式 Lakehouse 所取代，该模式将 (i) 基于开放的直接访问数据格式，例如 Apache Parquet，(ii) 对 mlds 提供一流的支持，(iii) 提供最先进的性能；Lakehouse 可以帮助解决数据仓库面临的几个主要挑战，包括数据陈旧性、可靠性、总体拥有成本、数据锁定和有限的用例支持；我们讨论了业界如何转向 Lakehouse，以及这种转变将如何影响数据管理工作，我们还报告了使用 Parquet 的 Lakehouse 系统的结果，该系统在 TPC-DS 上可与流行的云数据仓库相媲美。
数据仓库的历史始于帮助企业领导者通过将数据从运营数据库收集到集中仓库中获得分析见解，然后可用于决策支持和商业智能，这些仓库中的数据将使用写入时模式写入，这确保了数据模型针对下游 BI 消费进行了优化，我们将其称为第一代 da 平台；十年前，第一代系统开始面临几个挑战，包括：（i）它们通常将计算和存储耦合到内部设备中，这迫使企业在管理下配置和支付用户负载和数据的峰值，随着数据集的增长，这变得非常昂贵，（ii）不仅数据集增长迅速，而且越来越多的数据集完全是非结构化的，例如视频、音频和文本文档，数据仓库根本无法存储和查询；为了解决这些问题，第二代 DA 平台开始将所有原始数据卸载到数据湖中：具有文件 API 的低成本存储系统，以通用且通常开放的文件格式（例如 Apache Parquet）保存数据，这种方法始于 Apache Hadoop 运动，使用 HDFS 进行廉价存储，数据湖是一种读取时模式架构，可以以低成本灵活地存储任何数据，但另一方面，将数据质量和治理问题推给了下游，在这种架构中，湖中的一小部分数据稍后将被 ETL 到下游数据仓库（例如 Teradata），用于最重要的决策支持和 BI 应用程序，使用开放格式也使得数据湖数据可以直接被各种其他分析引擎（例如 ml 系统）访问；从 2015 年起，云数据湖（例如 S3、ADLS、GCS）开始取代 HDFS，它们具有卓越的耐用性（通常 >10 个 9）、地理复制和极低的成本，并且可以实现自动、甚至更便宜的存档存储，例如 AWS Glacier，其余架构在云中与第二代系统大致相同，具有下游数据仓库（例如 Redshift、Snowflake），根据我们的经验，这种 2 层数据湖 + 仓库架构现在在行业中占据主导地位（几乎所有财富 500 强企业都在使用）；这给我们带来了当前数据架构的挑战。虽然云数据湖和数据仓库架构由于存储（如 S3）和计算（如 Redshift）分离而表面上看起来很便宜，但两层架构对用户来说非常复杂：在第一代平台中，所有数据都是从运营数据系统直接进行 ETL 处理并存入数据仓库的，而在当今的架构中，数据首先被 ETL 到湖中，然后再被 ETL 到数据仓库，这带来了复杂性、延迟和新的故障模式。此外，企业用例现在包括机器学习等高级分析，数据湖和数据仓库都不是理想的选择。具体来说，当今的数据架构通常存在 4 个问题，包括：（i）可靠性：保持数据湖和数据仓库的一致性既困难又昂贵，需要持续的工程来将数据 ETL 到这两个系统并使其可用于高性能决策支持和 BI，每个 ETL 步骤也都有可能导致故障或引入错误，从而降低数据质量，例如由于数据湖和数据仓库引擎之间的细微差异；
(ii) 数据陈旧：与数据湖相比，数据仓库中的数据陈旧，新数据通常需要数天才能加载，这与第一代数据系统相比倒退了一步。第一代数据系统可以立即获取新的运营数据以供查询。根据 Dimensional Research 和 Fivetran 的一项调查，86% 的分析师使用过时的数据，62% 的分析师报告每月多次等待工程资源；(iii) 对高级分析的支持有限：企业希望使用其数据仓库数据提出预测性问题，例如“我应该为哪些客户提供折扣？”尽管对机器学习和数据管理的融合进行了大量研究，但领先的机器学习系统（例如 TensorFlow、Pytorch 和 XGBoost）都无法在数据仓库上运行良好。与提取少量数据的 BI 查询不同，这些系统需要使用复杂的 nonsql 代码来处理大型数据集，通过 ODBC/JDBC 读取这些数据效率低下，并且无法直接访问内部仓库的专有格式。对于这些用例，仓库供应商建议将数据导出到文件，但这进一步增加了复杂性和陈旧性（增加了第三个 ETL 步骤）。或者，用户可以针对开放格式的数据湖数据运行这些系统，但是，他们会失去数据仓库丰富的管理功能，例如 ACID 事务、数据版本控制和索引；
（iv）总体拥有成本：除了支付持续 ETL 费用外，用户还需要为复制到仓库的数据支付双倍的存储成本，而商业仓库将数据锁定为专有格式，这增加了将数据或工作负载迁移到其他系统的成本；
一种采用率有限的“稻草人”解决方案是完全消除数据湖，将所有数据存储在一个内置计算和存储分离的仓库中。我们认为，这种方案的可变性有限，这一点从其采用率低就可以看出，因为它仍然不支持轻松管理视频/音频/文本数据，也无法支持从MLDS负载中快速直接访问；
我们在此讨论以下技术问题：是否有可能将基于标准开放数据格式（例如Parquet）的数据湖转变为高性能系统，该系统既能提供数据仓库的性能和管理功能，又能支持从高级分析负载中快速直接输入/输出？我们认为，这种系统设计（我们称之为[Lakehouse](./img/lakehouse.png)）是可行的，并且已经以各种形式在业界显示出成功的证据。随着越来越多的业务应用程序开始依赖运营数据和高级分析，我们相信Lakehouse是一个引人注目的设计点，可以消除数据仓库面临的一些主要挑战；具体来说，我们认为 Lakehouse 的时代已经到来，因为最近的解决方案解决了以下关键问题：（i）数据湖上的可靠数据管理：Lakehouse 需要能够存储原始数据，类似于今天的数据湖，同时支持整理这些数据以提高其分析质量的 ETL/ELT 流程。传统上，数据湖将数据管理为半结构化格式的“一堆文件”，这使得很难提供一些简化数据仓库中 ETL/ELT 的关键管理功能，例如事务、回滚到旧表版本和零拷贝克隆。然而，最近的 DeltaLake、Apache Iceberg 等系统系列提供了数据湖的事务视图，并启用了这些管理功能。当然，组织仍然需要努力编写 ETL/ELT 逻辑来使用 Lakehouse 创建整理的数据集，但总体而言，ETL 步骤更少，分析师也可以轻松高效地查询原始数据表，就像在第一代分析平台中；
(ii) 对机器学习负载 (MLD) 的支持：机器学习系统对数据湖格式直接访问的支持，已经使其能够高效地访问 Lakehouse。此外，许多机器学习系统已采用 DataFrames 作为数据操作的抽象，并且最近的系统设计了声明式 DataFrame API，可以对机器学习负载中的数据访问进行查询优化，这些 API 使机器学习负载能够直接受益于 Lakehouse 中的众多优化；
(iii) SQL 性能：Lakehouse 需要在过去十年积累的海量 Parquet 数据集（或从长远来看，是其他一些可供应用程序直接访问的标准格式）之上提供最先进的 SQL 性能。相比之下，经典数据仓库接受 SQL，并且可以自由优化底层的一切，包括专有存储格式。尽管如此，我们展示了可以使用各种技术来维护 Parquet 数据集的辅助数据，并优化这些现有格式中的数据布局，以实现具有竞争力的性能。我们展示了基于 Parquet 的 SQL 引擎（Databricks Delta Engine）的结果，其在 TPC-DS 上的表现优于领先的云数据仓库。
### 动机：数据仓库挑战
数据仓库对于许多业务流程至关重要，但它们仍然经常因数据不正确、数据陈旧和高成本而困扰用户。我们认为，这些挑战至少有一部分是企业数据平台设计方式带来的“意外复杂性”，而 Lakehouse 可以消除这些复杂性；
首先，当今企业数据用户报告的首要问题通常是数据质量和可靠性，实施正确的数据管道本质上很困难，但当今具有独立数据湖和数据仓库的两层数据架构增加了额外的复杂性，加剧了这个问题。例如，数据湖和数据仓库系统在其支持的数据类型、SQL 方言等方面可能具有不同的语义，数据可能以不同的模式存储在数据湖和数据仓库中（例如，在一个数据湖中使用非规范化模式），并且增加 ETL/ELT 作业数量，其跨越多个系统，增加了故障和错误的可能性；
其次，越来越多的业务应用程序需要更新数据，但当今的架构通过在数据仓库之前为传入数据设置单独的共享区域并使用定期的 ETL/ELT 作业来加载数据，增加了数据陈旧性。理论上，组织可以实现更多流式传输管道以更快地更新数据仓库。在第一代平台中，数据仓库用户可以立即访问从与派生数据集相同的环境中的操作系统加载的原始数据，而客户支持系统和推荐引擎等业务应用程序对于陈旧数据根本无效，甚至查询数据仓库及其 API 的人工分析师也难以轻松支持它；
最后，大多数组织现在都在部署 mlds 应用程序，但数据仓库和数据湖并不能很好地支持这些应用程序。如前所述，这些应用程序需要使用非 SQL 代码处理大量数据，因此它们无法通过 ODBC/JDBC 有效运行。随着高级分析系统的不断发展，我们相信让它们以开放格式直接访问数据将是支持它们的最有效方式。此外，mlds 应用程序与传统应用程序一样，也面临数据管理问题，例如数据质量、一致性和隔离性，因此将 DBMS 功能引入其数据具有巨大的价值；
当前的几个行业趋势进一步证明客户对 2 层湖 + 仓库模型并不满意，包括：首先，近年来几乎所有主要数据仓库都增加了对 Parquet 格式的外部表的支持，这使仓库用户也可以更轻松地查询数据湖表，但这并没有消除 ETL 复杂性、陈旧性和仓库中数据的高级分析挑战；实际上，这些连接器通常性能不佳，因为 SQL 引擎主要针对其内部数据格式进行了优化；其次，直接针对数据湖存储运行的 SQL 引擎也有广泛的投资，例如 SparkSQL、Presto、Hive 和 AWS Athena，但是，仅凭这些引擎无法解决数据湖的所有问题并取代仓库：数据湖仍然缺乏基本的管理功能，例如 ACID 事务和高效的访问方法，以匹配数据仓库的性能。
### Lakehouse 架构
我们将 Lakehouse 定义为基于低成本、可直接访问存储的数据管理系统，该系统同时提供传统的分析型数据库管理系统的管理和性能功能，例如 ACID 事务、数据版本控制、审计、索引、缓存和查询优化。因此，Lakehouse 结合了数据湖和数据仓库的关键优势：前者提供易于各种系统访问的开放格式低成本存储，后者提供强大的管理和优化功能。关键问题在于能否有效地结合这些优势：特别是，Lakehouse 对直接访问的支持意味着它们放弃了某些数据独立性，而数据独立性一直是关系型数据库管理系统设计的基石；
我们注意到，Lakehouse 特别适合计算和存储分离的云环境：不同的计算应用程序可以在完全独立的计算节点（例如用于机器学习的 GPU 集群）上按需运行，同时直接访问相同的存储数据。此外，也可以在本地存储系统（例如 HDFS）上实现 Lakehouse；
在本节中，我们基于业界以各种形式出现的 3 个最新技术理念，勾勒出 Lakehouse 系统的一种可能设计。Databricks 一直在通过 Delta Lake、Delta Engine 和 Databricks ML Runtime 项目构建基于此设计的 Lakehouse 平台。当然，其他设计也可能出现，但是，正如我们高层设计中的其他具体技术选择一样（例如，Databricks 的堆栈目前基于 Parquet 存储格式构建，但可以设计出更好的格式），我们将讨论几种替代方案和未来的研究方向；
**实施 Lakehouse 系统：**我们提出的实施 Lakehouse 的第一个关键思想是让系统使用标准文件格式（例如 Apache Parquet）将数据存储在低成本对象存储（例如 Amazon S3）中，但在对象存储之上实现事务元数据层，该层定义哪些对象是表版本的一部分，这允许系统在元数据层内实现管理功能，例如 ACID 事务或版本控制，同时将大量数据保存在低成本对象存储中，并允许客户端在大多数情况下使用标准文件格式直接从该存储中读取对象，包括 DeltaLake、Apache Iceberg 在内的几个最近的系统已经成功地以这种方式向数据湖添加了管理功能，例如，DeltaLake 现在已被数千名客户用于 Databrick 大约一半的工作负荷中；尽管一个元数据层增加了管理功能，但仅实现良好的 SQL 性能是不够的，数据仓库使用多种技术来获得最先进的性能，例如将热数据存储在 SSD 等快速设备上、维护统计数据、构建高效的访问方法（如索引）以及共同优化数据格式和计算引擎；在基于现有存储格式的 Lakehouse 中，无法更改格式，但我们表明可以实现保持数据文件不变的其他优化，包括缓存、辅助数据结构（如索引和统计数据）以及数据布局优化；最后，由于声明性 DataFrame API 的开发，Lakehouse 既可以加速高级分析负载，又可以为它们提供更好的数据管理功能，许多机器学习库（如 Tensorflow、Spark MLlib）已经可以读取数据湖文件格式（如 Parquet），因此将它们与 Lakehouse 集成的最简单方法是查询元数据层，以确定哪些 Parquet 文件当前是表的一部分，然后将它们传递给机器学习库，但是，这些系统中的大多数都支持用于数据准备的 DataFrame API，从而创造了更多的优化机会；DataFrames 由 R 和 Pandas 推广，它们只是为用户提供了一个带有各种转换运算符的表抽象，其中大多数映射到关系代数，诸如 SparkSQL 之类的系统通过延迟评估转换并将生成的运算符计划传递给优化器，使该 API 具有声明性，因此，这些 API 可以利用 Lakehouse 中的新优化功能（如缓存和辅助数据）来进一步加速机器学习； [点击查看这些想法如何融入 Lakehouse 系统设计](./img/lakehouse-system-design.png); **数据管理的元数据层**：我们认为能够赋能 Lakehouse 的第一个组件是数据湖存储上的元数据层，它可以提升其抽象级别以实现 ACID 事务和其他管理功能。S3 或 HDFS 等数据湖存储系统仅提供低级对象存储或文件系统接口，即使是像更新跨多个文件的表这样的简单操作也不是原子的；各组织很快开始在这些系统上设计更丰富的数据管理层，首先是 Apache Hive ACID，它使用 OLTP DBMS 跟踪哪些数据文件是给定表版本的 Hive 表的一部分，并允许操作以事务方式更新此集合；近年来，新系统提供了更多功能并提高了可扩展性，2016 年 Databricks 开始开发 DeltaLake，它将哪些对象属于数据湖本身的表的信息作为 Parquet 格式的事务日志存储，使其能够扩展到每个表数十亿个对象，始于 Netflix 的 Apache Iceberg 使用类似的设计并支持 Parquet 存储，始于 Uber 的 Apache Hudi 是该领域的另一个系统，专注于简化流式传输到数据湖，尽管它不支持并发写入；这些系统的经验表明，它们通常提供与原始 Parquet 数据湖相似或更好的性能，同时添加了非常有用的管理功能，例如事务、零拷贝锥体和时间旅行到表的过去版本，此外，它们很容易被已经拥有数据湖的组织采用，例如，DeltaLake 只需添加以引用所有现有文件的条目开头的事务日志，就可以将现有的 Parquet 文件目录转换为具有零拷贝的 DeltaLake 表，因此，组织正在迅速采用这些元数据层，例如，DeltaLake 在 3 年内发展到覆盖 Databricks 上一半的计算小时数；此外，元数据层是实现数据质量强制功能的自然场所，例如，DeltaLake 实现了模式强制，以确保上传到表的数据与其模式匹配，并实现了约束 API，允许表所有者对提取的数据设置约束（例如，国家/地区只能是值列表之一）。Delta 的客户端库会自动拒绝违反这些期望的记录或将其隔离在特殊位置。客户发现这些简单的功能对于提高基于数据湖的管道质量非常有用；最后，元数据层是实现访问控制和审计日志等治理功能的自然场所，例如，元数据层可以在授予客户端从云对象存储读取表中原始数据的凭据之前检查是否允许客户端访问表，并且可以可靠地记录所有访问；
**未来方向和替代设计**：由于数据湖的元数据层是一个相当新的开发，因此存在许多悬而未决的问题和替代设计，例如，我们设计 DeltaLake 将其事务日志存储在其运行的同一对象存储中（例如 S3），以简化管理（无需运行单独的存储系统）并提供高日志的可用性和高读取带宽（与对象存储相同），然而，由于对象存储的高延迟，这限制了它每秒可以支持的事务速率，在某些情况下，使用更快的元数据存储系统的设计可能是更好的选择，同样，DeltaLake、Iceberg 和 Hudi 一次仅支持在一张表上进行事务，但应该可以扩展它们以支持跨表事务，优化事务日志的格式和管理对象的大小也是悬而未决的问题；
**Lakehouse 中的 sql perf：** Lakehouse 方法中最大的技术问题可能是如何在放弃传统 dbms 设计中相当一部分数据独立性的同时提供最先进的 sql perf，答案显然取决于许多因素，包括：我们有哪些可用的硬件资源（比如，我们可以在对象存储之上实现一个缓存层吗），以及我们是否可以更改数据对象存储格式而不是使用现有标准，如 Parquet（改进这些格式的新设计不断涌现），然而，无论确切的设计如何，核心挑战是数据存储格式成为系统公共 api 的一部分，以允许快速直接访问，这与传统 dbms 不同；我们提出了几种在 Lakehouse 中实现 SQL 性能优化的技术，这些优化与所选数据格式无关，因此可以应用于现有或未来的格式。我们也在 Databricks Delta Engine 中实现了这些技术，并表明它们的性能与流行的云数据仓库相当，尽管还有很大的性能优化空间，但这些与格式无关的优化包括：（i）缓存：当使用传统的元数据层（例如 DeltaLake）时，Lakehouse 系统可以安全地将云对象存储中的文件缓存在处理节点上更快的存储设备（例如 SSD 和 RAM）上，运行事务可以轻松确定缓存文件何时仍然有效可读。此外，缓存可以采用转码格式，这对于查询引擎的运行效率更高，可以匹配传统“封闭世界”数据仓库引擎中使用的任何优化，例如，我们在 Databricks 的缓存会部分解压缩它加载的 Parquet 数据；
(ii) 辅助数据：即使 Lakehouse 需要公开用于直接 I/O 的基表存储格式，它也可以维护其他有助于优化辅助文件中查询的数据，这些辅助文件由 Lakehouse 完全控制。在 DeltaLake 和 Delta Engine 中，我们在用于存储事务日志的同一 Parquet 文件中维护表中每个数据文件的列最小-最大统计信息，这使得当基表数据按特定列聚类时可以进行数据跳过优化。我们还实现了基于布隆过滤器的索引，可以想象在这里实现各种各样的辅助数据结构，类似于索引“原始”数据的提案；
(iii) 数据布局：数据布局在访问性能中起着重要作用，即使我们修复了 Parquet 等存储格式，Lakehouse 系统也可以优化多个布局决策，最明显的是记录器排序 - 哪些记录聚集在一起，因此最容易一起读取，在 DeltaLake 中，我们支持使用单独的维度或空间填充曲线（如 Z 顺序、希尔伯特曲线）对记录进行排序，以提供跨多个维度的局部性，还可以想象支持在每个数据文件中以不同顺序放置列的新格式，为不同的记录组选择不同的压缩策略或其他策略；这 3 种优化对于分析系统中的典型访问模式尤其有效，在典型的网络中，大多数查询往往集中在热数据子集上，Lakehouse 可以使用与封闭世界数据仓库相同的优化数据结构进行缓存，以提供有竞争力的性能；对于云对象存储中的冷数据，性能的主要决定因素可能是每个查询读取的数据量。在这种情况下，数据布局优化（将共同访问的数据聚类）和辅助数据结构（如区域图，使引擎能够快速确定要读取的数据文件范围）的组合可以使 Lakehouse 系统以与封闭世界专有数据仓库相同的方式最小化 io，尽管它运行的是标准开放文件格式；
(iv) 性能结果：在 Databricks，我们将这 3 个 Lakehouse 优化与 Apache Spark 的新 C++ 执行引擎 Delta Engine 相结合，以评估 Lakehouse 架构的可行性，[点击比较 TPC-DS 上规模因子 30000 的 Delta Engine 与 4 个广泛使用的云数据仓库（来自云提供商以及在公共云上运行的第三方公司）](./img/delta-engine.png)，使用 AWS、Azure、GCP 上的可比集群，每个集群具有 960 个 vGPU 和本地 SSD 存储（*我们启动所有系统时，数据尽可能缓存在 SSD 上，因为我们比较的一些仓库仅支持节点附加存储，而 Delta Engine 在从冷缓存启动时仅慢了 18%，我们报告了运行所有 99 个查询的时间以及每个服务的定价模型中客户的总成本（Databricks 允许用户选择现货和按需实例，因此我们同时展示了这两种情况），Delta Engine 以更低的价格提供了与这些系统相当或更好的性能；（v）未来方向和替代设计：设计高性能且可直接访问的 Lakehouse 系统是未来工作的丰富领域，我们尚未探索的一个明确方向是设计新的数据湖存储格式，以便在这种用例中更好地工作，例如为 Lakehouse 系统提供更多灵活性以实现数据布局优化或索引的格式，或者更适合现代硬件的格式，当然，处理引擎可能需要一段时间才能采用这种新格式，从而限制了可以从中读取数据的客户端数量，但为下一代工作负载设计高质量的可直接访问的开放格式是一个重要的研究问题；即使不改变数据格式，Lakehouse 也有许多类型的缓存策略、辅助数据结构和数据布局策略可供探索，确定哪些策略对于云对象存储中的海量数据集最有效仍是一个悬而未决的问题；最后，另一个令人兴奋的研究方向是确定何时以及如何使用无服务器计算系统来回答查询，并优化存储、元数据层和查询引擎设计，以最大限度地减少这种情况下延迟；
**高效访问高级分析：** (i) 正如我们之前讨论过的，高级分析库通常使用不能以 SQL 形式运行的命令式代码编写，但由于需要访问大量数据，因此有一个有趣的研究问题，即如何设计这些库中的数据访问层，以最大限度地提高运行在顶层节点的灵活性，同时仍然受益于 Lakehouse 中的优化机会；我们成功采用的一种方法是提供此类库中使用的 DataFrame API 的声明性版本，它将数据准备计算映射到 SparkSQL 查询计划中，并且可以从 DeltaLake 和 Delta Engine 中的优化中受益，我们在 Spark DataFrames 和 Koalas 中都使用了这种方法，Spark 的新 DataFrame API 可以提高与 Pandas 的兼容性，DataFrames 是用于将输入传递到 Apache Spark 高级分析库生态系统的主要数据类型，包括 MLlib、GraphFrames、SparkR 和许多社区库，因此如果我们可以优化 DataFrame 计算，所有这些工作负荷都可以享受加速 io； Spark 的查询规划器会将用户 DataFrame 计算中的选择和投影直接推送到每次数据源读取的“数据源”插件类中，[因此，在我们 DeltaLake 数据源的实现中，我们利用了 Lakehouse 中的 sql perf 部分中描述的缓存、数据跳过和数据布局优化来加速从 DeltaLake 的读取，从而加速 mlds wkloads，点击查看](./img/dataframe-api.png)；然而，机器学习 API 正在快速发展，还有其他数据访问 API，例如 Tensorflow 的 tf.data，它们并不尝试将查询语义推送到底层存储系统，其中许多 API 还专注于将数据加载到 CPU 上，并通过 CPU2GPU 传输和 GPU 计算，这在数据仓库中尚未引起太多关注，最近的系统工作表明，保持现代加速器的充分利用，尤其是对于机器学习推理而言，可能是一个难题，因此 Lakehouse 访问库需要应对这一挑战；
(ii) 未来方向和替代设计：除了我们刚刚讨论过的有关现有 API 和效率的问题之外，我们还可以探索与机器学习截然不同的数据访问接口设计，例如，最近的研究提出了将机器学习逻辑推入 SQL 连接中的分解式机器学习框架，以及其他可应用于 SQL 中实现的机器学习算法的查询优化；最后，我们仍然需要标准接口，以便数据科学家充分利用 Lakehouse（甚至数据仓库）中强大的数据管理功能，例如在 Databricks，我们将 DeltaLake 与 MLflow 中的机器学习实验跟踪服务集成在一起，以便数据科学家轻松跟踪实验中使用的表版本并在以后重现该版本的数据；业界还出现了一种新兴的特征存储抽象，作为数据管理层来存储和更新机器学习应用程序中使用的特征，这将受益于在 Lakehouse 设计中使用标准 DBMS 功能，例如事务和数据版本控制。
### 研究问题与启示
除了我们提出的未来研究方向的挑战之外，Lakehouse 还提出了其他几个研究问题，此外，行业趋势是数据湖的功能越来越丰富对数据系统研究的其他领域具有重要意义；
**还有其他方法可以实现 Lakehouse 的目标吗？** 我们可以设想其他方法来实现 Lakehouse 的主要目标，例如为数据仓库构建一个大规模并行服务层，以支持从高级分析负载并行读取数据。然而，我们认为，与允许负载直接访问对象存储相比，这种基础设施的运行成本将显著增加，管理难度也将加大，性能也可能更低；我们尚未看到添加此类服务层的系统得到广泛部署，例如 Hive LLAP。此外，这种方法将选择高效的数据格式读取问题推给了服务层，而且这种格式仍然需要易于从仓库内部格式转码。云对象存储的主要优势在于其低成本、弹性负载高带宽访问以及极高的可用性，而如果在对象存储前面设置单独的服务层，所有这些优势都会变得更糟；除了这些替代方法在性能、可用​​性、成本和锁定方面的挑战之外，企业可能更倾向于将数据保存在开放格式中，这背后还有重要的治理原因。随着数据管理监管要求的不断增加，企业可能需要搜索旧数据集、删除各种数据或在短时间内更改其数据处理基础设施，而标准化开放格式意味着他们将始终能够直接访问数据，而不会受到供应商的阻碍。软件行业的长期趋势一直是开放数据格式，我们相信这种趋势将持续适用于企业数据；
**哪些是正确的存储格式和访问 API？** Lakehouse 的访问接口包括原始存储格式、用于直接读取此格式的客户端库（例如，读入 TensorFlow 时）以及高级 SQL 接口。在这些层上部署丰富的功能有很多不同的方法，例如通过要求读取器执行更复杂、可编程的解码逻辑，为系统提供更多灵活性的存储方案；哪种存储格式、元数据层设计和访问 API 的组合效果最佳，还有待观察；
**Lakehouse 如何影响其他数据管理研究和趋势？** 数据湖的普及以及在其上日益增长的丰富管理接口的使用（无论是元数据层还是完整的 Lakehouse 设计）对数据管理研究的其他几个领域产生了影响，包括：（i）polystore 旨在解决跨不同存储引擎查询数据的难题，这个问题将在企业中持续存在，但云数据湖中以开放格式提供的数据比例不断增加意味着许多 polystore 查询可以通过直接针对云对象存储运行来回答，即使底层数据文件是逻辑上独立的 Lakehouse 部署的一部分；（ii）数据集成和清理工具也可以设计为在 Lakehouse 上就地运行，并快速并行访问所有数据，这可能支持新算法，例如在组织中的许多数据集上运行大型连接和聚类算法； (iii) HTAP 系统或许可以构建为 Lakehouse 前面的附加层，通过使用其事务管理 API 将数据直接归档到 Lakehouse 系统中，Lakehouse 将能够查询数据的一致快照；(iv) 如果通过 Lakehouse 实施，机器学习的数据管理也可能变得更简单、更强大，如今，各组织正在构建各种实现标准 DBMS 功能的机器学习特定的数据版本控制和功能存储系统，使用内置 DBMS 管理功能的 Lake House 抽象来实现功能存储功能可能会更简单，同时，声明式机器学习系统（如分解机器学习）可能在 Lakehouse 上运行良好；(v) 无服务器引擎等云原生 DBMS 设计需要与更丰富的元数据管理层（如 DeltaLake）集成，而不仅仅是扫描数据湖中的原始文件，但可能能够实现更高的性能； （vi）最后，业界正在持续讨论如何组织数据工程流程和团队，其中数据网格等概念（即不同的团队端到端拥有不同的数据产品）比传统的中央数据团队方法更受欢迎，Lakehouse 设计很容易实现分布式协作结构，因为所有数据集都可以直接从对象存储访问，而无需在相同的计算资源上加入用户，从而可以直接共享数据，无论哪个团队生产和使用数据。
****
### Geo-Replication
### Stronger Consistency and Semantics for Low-Latency Geo-Replication Storage
### [Lloyd13](https://www.cs.princeton.edu/~wlloyd/papers/PhdThesis.pdf)
### Only Chapters 1-3 from Lloyd13.
geo-replicated, distributed data stores that support complex online applications, such as social networks, strive to provide an always-on experience where ops always successfully complete with low latency, today's systems sacrifice strong consistency and expressive semantics to achieve these goals, exposing inconsistencies to their clients and necessitating complex application logic, here we identify and define a consistency model -causal consistency with convergent conflict handling, or causal+ -that is the strongest achieved under these constraints; 
we explore ramifications of and implementation techniques for causal+ consistency through design and implementation of 2 storage systems :COPS, Eiger, here COPS is a key-value store that provides causal+ consistency, a key contribution of COPS is its scalability, which can enforce causal dependencies bt keys stored across an entire cluster, rather than a single server like previous systems, Eiger is a system that provides causal+ consistency for the rich column-family data model, the central approach in COPS, Eiger is tracking and explicitly checking that causal dependencies bt keys are satisfied in the local cluster before exposing writes; 
we also provide stronger semantics, in the form of read-only and write-only transactions, that allow programmers to consistently interact with data spread across many servers, COPS includes the first scalable read-only transactions that can optimistically complete in 1rd of local reads and take at most 2 non-blocking rds of local reads, Eiger includes read-only transactions with the same properties that are more general -they are applicable to non-causal systems -and more fault-tolerant, Eiger also includes scalable write-only transactions that take 3rds of local non-blocking comm and do not block concurrent read-only transactions; 
we implemented COPS, Eiger and desmonstrate their perf experimentally, the later of these systems, Eiger, achieves low latency, has throughput competitive with an eventually-consistent and non-transactional production system, and scales to large clusters almost linearly. 
large-scale data stores are a critical infra component in many of today's largest internet services, these services supoort many millions of concurrent users interacting with many petabytes of data, the extreme scale of the infra needed to support these services provides a new environment to explore distributed storage designs, [in contrast to classical distributed storage systems that focus on local-area op in the small, these services are typically characterized by massive wide-area deployment across a few to tens of datacenters, click to view](./img/the-general-architecture-of-modern-web-services.png); 
in these systems each datacenter stores a full replica of data, i.e. there is a copy of each data item in each datacenter(*extending this work to settings where data may be only replicated over some some subset of datacenters is an interesting avenue of future work), for example, Facebook stores all user profiles, comments, friends lists, and likes at each of its datacenters, replicating data in this way across geographically distinct locations is called geo-replication, geo-replication brings 2 key benefits to web services :fault tolerance and low latency, it provides fault tolerance through redundancy :if one location fails, other locations can continue to provide the service, it provides low latency through proximity :users can de directed to and served by a nearby location to avoid speed-of-light delays associated with cross-country or round-the-globe comm; 
the scale of data stored at each datacenter is far too large to fit on a single physical machine, instead, it must be spread across clusters of 10s-1000s of machines, a typical technique for spreading data is sharding, which puts disjoint subsets of data on different servers, as a primitive example, Server1 might store and serve user profiles for people whose names start with A, Server2 for B, and so on, such sharding allows clusters to scale in capacity and throughput, adding additional machines to a cluster results in a rebalancing of shards across the new larger set of servers, each server in the new cluster is then responsible for a smaller shard of the data, this allows the throughput and capacity of a cluster to increase, or scale, with the addition of more machines, the storage systems that support these large web services are thus doubly distributed systems, replicas are distributed geographically and each is itself distributed across many machines via sharding; 
when users initiate a connection with a service they are directed to a nearby datacenter, within that datacenter users are a connected one to many identical frontend web servers, frontends serve reqs by reading and writing data to and from storage tier nodes(in a 3-tier web service the web server and application logic are considered separate tiers, we describe a 2-tier web service where they are combined in this dissertation, but all discussion applies in the 3-tier model as well with the application logic tier as out client), for the storage system, clients are the frontend web servers that issue read and write ops on behalf of human users, when we say, "a client writes a value", we mean that an application running on a web or application server writes into the storage system; 
in this dissertation, we address the problem of building a geo-replicated data store targeted at applications that demand low latency, such applications are common :Amazon, eBay, Google all claim that a slight increase in user-perceived latency translates into concrete revenue loss, user-perceived latency consists of many components, these include, at a minimum, (i)the network delay to the service instance a user is connected to, (ii)processing time on frontend web server that accepts the user's req, (iii)one or more rds of accesses to the storage system, (iv)additional processig time on frontend web server, (v)the network delay for the response to travel back to the user, (vi)user-side rendering time; perf goals vary across services, but typical targets are in the hundreds or low thousands of ms; 
these low page-load-time targets help demonstrate the latency benefit of geo-replication :when less time is used for user2service&back delay(also called rd trip time or rtt) it easier to meet these targets, [click for some examples rtts from a user in Princeton to servers around the world](./img/rtts-example.png), clearly, the page load time for a Princeton user will be much faster when accessing a service instance in NYC(8ms rtt), than when accessing one in LA(72ms rtt) or Sydney(240ms rtt); 
given the low targets for page load time and the many components that add to it, it is imperative that accesses to the data store have low latency, to this end, many storage systems -including those described in this dissertation -serve all accesses from the local replica of the data store, reads return a current view of data from this replica, they do not examine remote replicas, writes are applied at this replica immediately, they return before being applied at remote replicas; 
the local-replica-only design helps ensure low latency by avoiding slow accesses to remote replicas, which would take at least the rtt bt datacenters, [table1.1](./img/rtts-example.png) is again a good reference point for the magnitude of these rtts that are avoided, however, the local-replica-only design precludes providing the strongest forms of consistency, which restricts exec of ops to make the data store more intuitive for users and programmers, for example, linearizability, a strong consistency model, specifies that a read issued after a write returns must see effects of that write, linearizability is clearly incompatibe with the local-replica-only design because writes return after being written to their local replicas, but a read issued to a different replica instantaneously after that write returns must see its effects even though it is impossible for them to have propagated to the other replicas yet due to speed-of-light delay, theoretical results formalize the trade-off bt low latency and the strongest forms of consistency; 
the strongest forms of consistency are also at odds with other desirable properties of geo-replicated storage systems, the famous CAP theorem shows it is impossible to create a system that is linearizable, always available for reads and writes, and able to continue operating during network partitions, as a result, many modern web services have chosen to embrace availability and partition tolerance at the cost of linearizability, we refer to scalable systems with these 3 properties as ALPS systems, because they provide availability, low latency, partition-tolerance, and high scalability, we define the ALPS properties more rigorously and give an in-depth discussion of trade-offs with consistency in future section ALPS Systems and Tradeoffs; 
given that ALPS systems must sacrifice strong consistency(i.e. linearizability), we seek the strongest consistency model that is achievable under these constraints, such stronger consistency is desirable because it makes systems easier for a programmer to reason about and exposes fewer anomalies to end users of the system, here we consider causal consistency with convergent conflict handling, which we refer to as causal+ consistency, many previous systems believed to impelement the weaker causal consistency, such as Bayou, PRACTI actually implement the more useful causal+ consistency, through none do so in a scalable manner; 
the causal component of causal+ consistency ensures that the data store respects the causal dependencies bt ops, consider a scenario where a user uploads a picture to a website, the picture is saved, and then a reference to it is added to that user's album, the reference depends on the picture being saved, under causal+ consistency, these dependencies are always satisfied, programmers never need to reason about the situation where they can get the reference to the picture but not the picture itself, unlike in systems with weaker guarantees, such as eventual consistency; 
the convergent conflict handling component of causal+ consistency ensures that replicas never permanently diverge and that conflicting updates to the same key are dealt with identically at all sites, when combined with causal consistency, this property ensures that clients see only progressively newer versions of keys, in comparison, eventually consistent systems may expose versions out of order, by combining causal consistency and convergent conflict hanlding, causal+ consistency ensures clients see a causally-correct, conflict-free, and always-progressing data store; 
beyond causal+ consistency, we seek stronger semantics from out data store that give programmers more powerful tools for interacting with data, these stronger semantics include a rich data model, read-only transactions, and write-only transactions; 
key-value storage -perhaps the simplest data model provided by data stores -is used today by many services, however, this data model does not provide the abstractions typically used to build web services such as counters, lists, and graphs, in contrast, the column-family data models offered by systems like BigTable, Cassandra, do provide these abstractions, these rich data models provide hierarchical sorted column-families and numerical counters, column-families are well matched to many services such as Facebook, while counter columns are particularly useful for numerical stats, as used by collaborative filtering(Digg, Reddit), likes(Facebook), or re-tweets(Twitter), here we demonstrate how to provide causal+ consistency for the simple key-value data model and the rich column-family data model; 
in a scalable setting where data is spread across many machines, asynchronous reqs to a consistently-updated data store are insufficient for providing consistency to clients of the system, simultaneously, issued reqs to different servers may arrive at different times, hence returning distinct and potential inconsistent views of data, instead, what is needed are read-only transactions that provide programmers with a consistent and up2date view of data spread across many machines in the system, write-only transaction similarly benefit programmers by enabling them to atomically update data spread across many machines in the system; 
in this dissertation we describe 3 systems that provide causal+ consistency :COPS, COPS-RT, Eiger, each processing system is a step forward for scalable geo-replicated storage, with Eiger supersending our original efforts on COPS, COPS-RT; 
our COPS -cluster of order-preserving servers system provides causal+ consistency and is designed to support complex online applications that are hosted from a small #large-scale datacenters, each of which is composed of frontend servers(clients of COPS) and backend key-value data stores, COPS executes all read and write ops in the local datacenter in a linearizable fashion, and then replicates data across datacenters in a causal+ consistent order in the background; 
COPS-RT(the RT in COPS-RT stands for read-only transactions) builds on COPS by adding read-only transactions that give clients a consistent view of multiple keys spread across many servers, read-only transactions are needed to obtain a consistent view of multiple keys, even in a fully-linearizable system, our read-only transactions require no locks, are non-blocking, and take at most 2 parallel rds of intra-datacenter reqs, these read-only transactions do come at some cost :compared to the regular version of COPS, COPS-RT is less efficient fro cetain wkloads(such as write-heavy) and is less robust to long network partitions and transient datacenter failures; 
our Eiger system continues this line of work by providing stronger semantics for ALPS systems, these stronger semantics include providing causal+ consistency for the rich column-family data model, improved read-only transactions, and write-only transactions, Eiger's read-only and write-only transaction algorithms each represent an advance in the state-of-the-art; COPS introduces a read-only transaction algorithm that normally completes in 1rd of local reads, and 2rds in the worst case, Eiger's read-only transaction algorithm has the same properties, but achieves them using logical time instead of expicit dependencies, not storing explicit dependencies allows Eiger to avoid drawbacks of COPS-RT, Eiger is as efficient as COPS for all wkloads and is robust to long network partitions and transient datacenter failures; 
the scalability requirements for ALPS systems creates the largest distinction bt this work and prior causal+ consistent systems, previous systems required that all data fit on a single machine or that all data that potentially could be accessed together fit on a single machine, in comparision, data stored in COPS, Eiger can be spread across an arbitrarily-sized datacenter, with dependencies, read-only, and write-only transaction that can sketch across many servers in the datacenter, to the best of our knowledge, our work describes the first scalable systems to implement causal+(and thus causal) consistency.
### ALPS Systems and Tradeoffs
distributed storage systems have multiple, sometimes competing goals :avaiability, low latency, and partition tolerance to provide an always on ux, scalability to adpat to increasing load and storage demands, and a sufficiently strong consistency model to simplify programming and provide users with the system behavior that they expect, in slightly more depth, the desirable properties including: (i)availability -all ops issued to the data store completely successfullt, no op can block indefenitely or return an error signifying that data is unavailable, availability is important for data storage because it ensures clients can interact with a functioning service, unresponsive storage results in an unresponsive service that quickly becomes an unused service, our definition of availability also importantly avoids designs that cheat to provide low latency by quickly returning ann error; (ii)low latency -client ops complete quickly, commercial service-level objectives suggest average perf of a few ms and worst-case perf(i.e. 99.9th-percentile) of 10s or 100s of ms, low latency is critical for data storage, it prevents slow page load times that are correlated with lost revenue; (iii)partition tolerance -the data store continues to operate under network partitions, such as 1 separating datacenters in Asia from the US, the failure of a datacenter is equivalent to it being partitioned from rest of the system, and hence a partition tolerant system also continues to operate during failure of a datacenter; partition tolerance and availability are different, but complementary requirements, systems can provide one property but not the other, quorum-based systems are considered partition tolerant because the majority partitions can continue operating, but are not considered available because nodes in the minority partition cannot form a valid quorum and hence are unable to complete ops, a system that assumes away partitions could be described as available, even though that property would not hold in practice if a partition did occur; providing both partition tolerance and availability guarantees that all replicas of a system will continue to operate and be available as long as they have not failed; (iv)high scalability -the data store scales linearly, adding N resources to the system increases aggregate throughput and storage capacity by O(N), scalability is paramount for modern storage systems, the amount of data backing web services continues to grow, systems with all data fitting on a single machine are not longer practical, instead, scalable storage must be used that spreads data across many machines and that can grow along with the data, enabling larger storage systems enables supporting a larger #users, which is an important goal for almost all web services, larger storage also enables an increase in the amount of storage per user that can be used to provide a richer web service, which is also an important goal for many web services; (v)stronger consistency -the data store provides consistency stronger than eventual consistency, which we define in this dissertation as achieving causal+ consistency, we define causal+ consistency in future section Causal+ Consistency; 
an ideal data store would provide linearizability -sometimes called strong consistency -which dictates that ops appear to take effect across the entire system as a single instance in time bt the invocation and completion of the op, in a data store that provides linearizability, as soon as a client completes a write op to an object in 1 datacenter, read ops to the same object in all other datacenters will reflect its newly written state, linearizability simplifies programming -the distributed system provides a single, consistent image -and users experience the storage behavior they expect; weaker, eventual consistency models, common in many large distributed systems, are less intuitive :not only might subsequent reads not reflect the largest value, reads across multiple objects might reflect an incoherent mix of old and new values; 
the CAP theorem proves that a shared-data system that has availability and partition tolerance cannot achieve linearizability, this impossibility result can be explained quite intuitively :if a network partition occurs, availability requires that a write on one side of the partition must be able to complete, while linearizability requires that later reads on the other side of the partition must return the result of that write, which is clearly impossible; 
several forms of strong consistency, such as linearizability, serializability, and sequential consistency have been proven incompatible with low latency -defined as latency for all ops that is less than half of the maximum speed-of-light delay bt replicas, for example, linearizability is easily demonstrated to be incompatible :it requires that a write to a datacenter in Asia be reflected immediately in a read to a US datacenter, to achieve this property, either the write must be propagated to the US site before returning, or the read must query Asia before returning, both approaches are incompatible with the goal of low latency, in general, all consistency models with a total order over ops on multiple data locations have been shown to be incompatible with low latency.
### Causal+ Consistency
a storage system's consistency guarantees can restrict the possible orderings and timing of ops throughout the system, helping to simplify the possible behaviors that a programmer must reason about and the anomalies that clients may see; 
given that the strongest forms of consistency -linearizability, serializability, and sequential consistency -are provably incompatible with our low latency requirement, previous partition-tolerant, low-latency systems settled for the weakest form of consistency, eventual consistency, the commonly agreed upon definition of eventual consistency is that writes to one replica will eventual appear at other replicas and that if all replicas have received the same set of writes, they will have the same values for all data, this weak form of consistency does not restrict ordering of ops on different keys in any way, forcing programmers to reason about all possible orderings and exposing many inconsistencies to users; 
fortunately, causal+ consistency can avoid many such inconvenient orderings while guaranteeing low latency; 
to define causal consistency with convergent conflict handling(causal+ consistency), we first describe the abstract model over which it operates, there are 2 basic ops in our model :read and write, data is stored and retrieved from logical replicas, each of which hosts the entire key space, here a single logical replica corresponds to an entire local cluster of nodes; 
an important concept in our model is the notion of potential causality bt ops, 3 rules define potential causality, denoted -->: (i)thread-of-exec -if a and b are 2 ops in a single thread of exec, then a-->b if op a happens before op b, (ii)reads-from -if a is a write op and b is a read op that returns the value written by a, then a-->b, (iii)transitivity -for ops a,b,c, if a-->b, b-->c, then a-->c, hence the causal relationship bt ops is the transitive closure of the first 2 rules; these rules establish potential causality bt ops within the same exec thread and bt ops whose exec threads interacted through the data store, our model, like many, does not allow threads to communicate directly, requiring instead that all comm occur through the data store; [click for an example exec of the ops in fig3.1(a) is shown in fig3.1(b)](./img/causal-consistency-example.png), here it demonstrates all 3 rules, the thread-of-exec rule gives w1~->w4, the reads-from rule gives w1~->w2, and the transitivity rule gives w1~->w8; 
**definition:** we define causal+ consistency as a combination of 2 properties :causal consistency and convergent conflict handling, we present intuitive definitions here not the formal definitions, previous work describes systems that provide causal+ consistency, our contribution includes identifying difference bt causal and causal+, naming causal+, and explicitly defining causal+; 
causal consistency requires that values returned from read ops at a replica are consistent with the order defined by -->(causality), i.e. it must appeart that the op that writes a value occurs after all ops that causally precede it, for example, in [fig3.1(b)](./img/causal-consistency-example.png) it must appear that w1 happens before w3, which in turn happened before w6, if a client saw w6 but not w1, causal consistency would be violated; 
causal consistency does not order concurrent ops, if a not --> b and b not --> a, then a, b are concurrent, normally, this allows increased  efficiency in an implementation :2 unrelated writes can be replicated in any order, avoiding the need for a serialization point bt them, however, if a, b are both writes to the same key, then they are in conflict; 
conflicts are undesirable for 2 reasons: (i)because they are unordered by causal consistency, conflicts allow replicas to diverge forever, for example, if a writes 1 to location X and b writes 2 to location X, then causal consistency allows one replica to forever return 1 for X and another replica to forever return 2 for X, (ii)conflicts may represent an exceptional condition that requires special handling, for example, in a shopping cart application if 2 people logged in to the same account concurrently add items to their cart, the desired result is to end up with both items in the cart; 
convergent conflict handling requires that all conflicting writes be handles in the same manner at all replicas, using a handler function h, h must be associative and commutative, so that replicas can handle conflicting writes in the same order they receive them and that results of these handlings will converge(such as one replica's h(a,h(b,c)) and another's h(c,h(b,a)) agree); 
one common way to handle conflicting writes in a convergent fashion is the last-writer-wins rule(also called Thomas's write rule), which declares one of the conflicting writes as having occurred later and has it overwrite the earlier write, another common way to handle conflicting writes is to mark them as conflicting and require their resolution by some other means such as through direct user intervention as in Coda, or through a programmed procedure as in Bayou, Dynamo; 
all potential forms of convergent conflict handling avoid the first issue -conflicting updates may continually diverge -by ensuring that replicas reach the same result after exchanging ops, on the other hand, the second issue with conflicts -applications may want special handling of conflicts -is only avoided by use of more explicit conflict resolutio procedures, these explicit procedures provide greater flexibility for applications, but require additional programmer complexity and/or perf overhead, although our system can be configured to detect conflicting updates explicitly and apply some application-defined resolution, the default versions of our systems use the last-writer-wins rule; 
**causal+ vs. other consistency models:** distributed systems literature defines several popular consistency models including: (i)linearizability(or strong consistency) -maintains a global, realtime ordering, (ii)serializability -ensures a global ordering of transactions, (iii)sequential consistency -ensures a global ordering of ops, (iv)causal consistency -ensures partial orderings bt dependent ops, (iv)fifo(pram) consistency -only preserves partial ordering of an exec thread, not bt threads, (v)per-key sequential consistency -ensures for each individual key, all ops have a global order, (vi)eventual consistency, a catch-all term used today to suggest eventual convergence to some type of agreement; [click for the causal+ consistency we introduce falls bt sequential and causal consistency](./img/a-spectrum-of-consistency-models.png), it is weaker than sequential consistency, but sequential consistency is provably not achievable in an ALPS system, it is stronger than causal consistency asnd per-key sequential consistency, and is achievable for ALPS systems; 
to illustrate utility of causal component of the causal+ model, consider an example where a user uploads a photo to an internet service and then adds that photo to an album, under causal and causal+ consistency, the photo will always arrive before the add2album op to appear before the photo, this forces programmers to reason about that scenario :should they drop the add2album op, queue up a callback for when the photo arrives, add a broken reference to the album, or do something else? the generalization of this scenario where op B appears before a proceeding op A maps to many situations where user expectations are not obeyed and/or programmers must reason about out-of-order ops, some of these other situations including: (i)removing a friend's status update (A) then replying to it (B), (ii)adding an item to a shopping cart (A) then checking to see if it is there (B), (iii)a status update (A) followed by trying to hide it (B), (iv)an account op (A) followed by trying to log in (B); 
if the data store was sequentially consistent or linearizable, it would still be possible for there to be 2 simultaneous updates to a key, however, in these stronger models it is possible to implement mutual exclusion algorithms that can be used to avoid creating a conflict altogether; 
**causal+ in practice:** we use 2 abstractions in our systems :versions and dependencies, to help us reason about causal+ consistency, we refer to the different values a data location has as the versions of a location, which we denote loc_version, in our systems, versions are assigned in a manner that ensures that if xi-->yj then i<j, once a replica returns version i of a location, xi, causal+ consistency ensures it will then only return that version or a causally later version(note that handling of a conflict is causally later than conflicting writes it resolves, to see this, condier by contradiction the following scenario :assume a replica first returns xi and then xk, where i not equal to k and xi not --> xk, causal consistency ensures that if xk is returned after xi then xk not --> xi, and so xi, xk conflict, but, if xi, xk conflict, then convergent conflict handling ensures that as soos as both are preset at a replica, their handling h(xi,xk) which is causally after both, will be returned instead of either xi or xk, which contradicts our assumption), hence each replica in our system always returns non-decreasing versions of a key, we refer to this as causal+ consistency's progressing property; 
causal consistency dictates that all ops that causally precede a given ops must appear to take effect before it, i.e. if xi-->yj then xi must be written before yj, we call these preceding values dependencies, more formally, we say yj depends on xi iff write(xi)-->write(yj), these dependencies are the reverse of the causal ordering of writes and are, by definition, the same as the happens-before relationship, [click for a graph of causality bt ops in fig3.3(a)](./img/causal-consistency-example-2.png)(this is the same graph as [fig3.1(b)](./img/causal-consistency-example.png)) and fig3.3(b) shows the corresponding dependencies bt the write ops; 
our systems provide causal+ consistency during replication by writing a version only after all of its dependencies are satisfied, meaning those writes have already been applied in the cluster; 
**scalable causality:** to our knowledge, our work is the first to name and formally define causal+ consistency, interestingly, several previous systems believed to achieve causal consistency in fact achieved the stronger guarantees of causal+ consistency, these systems were not designed to and do not provide scalable causal(or causal+) consistency, however, as they all use a form of log serialization and exchange, all ops at a logical replica are written to a single log in serialized order, commonly marked with a version vector, different replicas then exchange these logs, using version vectors to establish potential causality and detect concurrency bt ops at different replicas; 
log-exchange-based serialization impairs replica scalability, as it relies on a single serialization point in each replica to establish ordering, hence either causal dependencies bt keys are limited to set of keys that can be stored on one node \[12,25,46,59\], or a single node(or replicated state machine) must provide a commit ordering and log for all ops across a cluster; 
our systems achieve scalability by taking a different approach, nodes in each datacenter are responsible for different partitions of the keyspace, but the system can track and enforce dependencies bt keys stored on different nodes, our systems explicitly encode dependencies in metadata associated with each key's version, when keys are replicated remotely, the receiving datacenter performs dependency checks before committing the incoming version, they perform these checks by communicating directly with nodes responsible for storing the other keys, in this way, they provide the first mechanisms fir achieving scalable causal+ replication.<br \>
支持复杂在线应用（例如社交网络）的地理复制分布式数据存储致力于提供始终在线的体验，使操作始终能够以低延迟成功完成。当今的系统为了实现这些目标，牺牲了强一致性和表达性语义，从而向客户端暴露了不一致性，并需要复杂的应用程序逻辑。在此，我们识别并定义了一个一致性模型——具有收敛冲突处理的因果一致性，或因果+——这是在这些约束条件下实现的最强一致性；
我们通过设计和实现两个存储系统来探索因果一致性的影响和实现技术：COPS、Eiger。COPS 是一个提供因果一致性的键值存储系统。COPS 的一个关键贡献是它的可扩展性，它可以强制将键存储在整个集群中，而不是像以前的系统那样存储在单个服务器上。Eiger 是一个为富列族数据模型提供因果一致性的系统。COPS 和 Eiger 的核心方法是在公开写入之前跟踪并明确检查本地集群中键的因果依赖关系是否得到满足；
我们还以只读和只写事务的形式提供了更强大的语义，使程序员能够与分布在多台服务器上的数据进行一致交互。COPS 包含首个可扩展的只读事务，乐观情况下，它可以在 1/3 的本地读取中完成，并且最多需要 2 次非阻塞的本地读取。Eiger 包含具有相同属性的只读事务，它们更加通用——它们适用于非因果系统——并且具有更高的容错能力。Eiger 还包含可扩展的只写事务，它们需要 1/3 的本地非阻塞通信，并且不会阻塞并发的只读事务；
我们实现了 COPS 和 Eiger，并通过实验证明了它们的性能。后者，Eiger，实现了低延迟，其吞吐量可与最终一致性和非事务性生产系统相媲美，并且几乎可以线性扩展到大型集群。
大规模数据存储是当今许多大型互联网服务的关键基础设施组件，这些服务支持数百万并发用户与数 PB 级数据进行交互，支持这些服务所需的基础设施的极端规模为探索分布式存储设计提供了一个新的环境。[与专注于小规模本地操作的传统分布式存储系统相比，这些服务通常以跨几个到几十个数据中心的大规模广域部署为特征，点击查看](./img/the-general-architecture-of-modern-web-services.png)；
在这些系统中，每个数据中心都存储数据的完整副本，即每个数据中心都有每个数据项的副本（*将这项工作扩展到数据可能仅在某些数据中心子集上复制的设置是未来工作的有趣方向），例如，Facebook 在其每个数据中心存储所有用户个人资料、评论、好友列表和点赞，以这种方式跨地理位置复制数据称为地理复制，地理复制为 Web 服务带来两个主要优势：容错和低延迟，它通过冗余提供容错能力：如果一个位置发生故障，其他位置可以继续提供服务，它通过邻近性提供低延迟：用户可以被引导到附近的位置并由其提供服务，以避免与跨国或全球通信相关的光速延迟；
每个数据中心存储的数据规模都过于庞大，无法容纳在单台物理机器上，而是必须分布在由数十到数千台机器组成的集群中。一种典型的数据分布技术是分片，即将不相交的数据子集放在不同的服务器上。举个简单的例子，服务器 1 可能存储并提供姓名以 A 开头的用户资料，服务器 2 存储并提供 B 开头的用户资料，以此类推。这种分片技术可以扩展集群的容量和吞吐量。向集群中添加更多机器会导致在新的更大的服务器集之间重新平衡分片，新集群中的每台服务器负责较小的数据分片。这样，随着更多机器的添加，集群的吞吐量和容量就可以增加或扩展。因此，支持这些大型 Web 服务的存储系统是双重分布式系统，副本按地理位置分布，并且每个副本本身通过分片分布在许多机器上；
当用户发起与服务的连接时，他们会被定向到附近的数据中心，在该数据中心内，用户连接到许多相同的前端 Web 服务器，前端通过从存储层节点读取和写入数据来满足请求（在 3 层 Web 服务中，Web 服务器和应用程序逻辑被视为单独的层，我们描述 2 层 Web 服务，其中它们是与本文相结合的，但所有讨论也适用于三层模型（应用逻辑层作为客户端），对于存储系统，客户端是代表人类用户发出读写操作的前端 Web 服务器。当我们说“客户端写入值”时，是指在 Web 服务器或应用服务器上运行的应用程序将值写入存储系统；
在本论文中，我们致力于构建一个面向低延迟应用的地理复制数据存储。这类应用很常见：亚马逊、eBay 和谷歌都声称，用户感知延迟的轻微增加会转化为具体的收入损失。用户感知延迟由许多部分组成，这些部分至少包括：(i) 用户连接到的服务实例的网络延迟；(ii) 接受用户请求的前端 Web 服务器的处理时间；(iii) 对存储系统的一次或多次访问；(iv) 前端 Web 服务器的额外处理时间；(v) 响应返回给用户的网络延迟；(vi) 用户端渲染时间；不同服务的性能目标各不相同，但典型的目标是数百或数千毫秒；
这些较低的页面加载时间目标有助于证明地理复制的延迟优势：当用于用户到服务和返回延迟（也称为 rd trip time 或 rtt）的时间较少时，更容易满足这些目标。[点击查看从普林斯顿用户到世界各地服务器的 rtts 示例](./img/rtts-example.png)，显然，普林斯顿用户访问纽约服务实例（8ms rtt）的页面加载时间要比访问洛杉矶（72ms rtt）或悉尼（240ms rtt）的服务实例快得多；
鉴于页面加载时间的目标很低，且其中包含许多组件，因此对数据存储的访问必须具有低延迟。为此，许多存储系统（包括本论文中描述的系统）都从数据存储的本地副本提供所有访问服务，读取操作返回来自该副本的当前数据视图，它们不检查远程副本，写入操作会立即应用于该副本，并在应用于远程副本之前返回；
仅本地副本设计有助于确保低延迟，因为它避免了对远程副本的缓慢访问，这至少需要 rtt bt 数据中心，[table1.1](./img/rtts-example.png) 再次很好地参考了这些可避免的 rtt 的大小，然而，仅本地副本设计无法提供最强的一致性形式，这限制了操作的执行，使数据存储对用户和程序员来说更直观，例如，线性一致性模型规定，在写入返回后发出的读取必须看到该写入的效果，线性一致性显然与仅本地副本设计不兼容，因为写入在写入其本地副本后返回，但在写入返回后立即对不同副本发出的读取必须看到其效果，即使由于光速延迟，它们不可能传播到其他副本，理论结果正式确定了低延迟和最强一致性形式之间的权衡；
最强的一致性形式也与地理复制存储系统的其他理想属性相冲突，著名的 CAP 定理表明，不可能创建一个线性化、始终可用于读写、并且能够在网络分区期间继续运行的系统，因此，许多现代 Web 服务选择以线性化为代价来拥抱可用性和分区容忍度。我们将具有这三个属性的可扩展系统称为 ALPS 系统，因为它们提供可用性、低延迟、分区容忍度和高可扩展性。我们将更严格地定义 ALPS 属性，并在未来的 ALPS 系统和权衡部分中深入讨论其与一致性之间的权衡；
鉴于 ALPS 系统必须牺牲强一致性（即可线性化），我们寻求在这些约束条件下可实现的最强一致性模型。这种更强的一致性是可取的，因为它使程序员更容易推理系统，并减少向系统最终用户暴露的异常。这里我们考虑具有收敛冲突处理的因果一致性，我们称之为因果+一致性。许多先前被认为可以实现较弱的因果一致性的系统，例如 Bayou、PRACTI，实际上实现了更有用的因果+一致性，尽管没有一个以可扩展的方式实现；因果+一致性的因果部分确保数据存储尊重操作之间的因果依赖关系。考虑这样一种场景：用户将图片上传到网站，图片被保存，然后对其的引用被添加到该用户的相册中，该引用依赖于正在保存的图片。在因果+一致性下，这些依赖关系始终得到满足，与最终一致性等保证较弱的系统不同，程序员无需考虑他们可以获得图片的引用但无法获得图片本身的情况；
因果一致性的收敛冲突处理组件确保副本永远不会永久发散，并且对同一键的冲突更新在所有站点都得到相同的处理。当与因果一致性结合时，此属性可确保客户端仅看到键的逐渐更新的版本。相比之下，最终一致性系统可能会无序地暴露版本。通过结合因果一致性和收敛冲突处理，因果一致性可确保客户端看到因果正确、无冲突且始终在进行的数据存储；
除了因果一致性之外，我们还寻求从数据存储中获得更强大的语义，为程序员提供更强大的数据交互工具，这些更强大的语义包括丰富的数据模型、只读事务和只写事务；
键值存储——也许是数据存储提供的最简单的数据模型——如今被许多服务所使用，然而，这种数据模型不提供通常用于构建 Web 服务的抽象，例如计数器、列表和图表。相比之下，BigTable、Cassandra 等系统提供的列族数据模型提供了这些抽象。这些丰富的数据模型提供了分层排序的列族和数值计数器。列族与许多服务（例如 Facebook）非常匹配，而计数器列对于数值统计特别有用，例如协同过滤（Digg、Reddit）、点赞（Facebook）或转发（Twitter）。这里我们演示如何为简单的键值数据模型和丰富的列族数据模型提供因果一致性；
在可扩展的环境中，数据分布在多台机器上，对持续更新的数据存储的异步请求不足以为系统客户端提供一致性。同时，向不同服务器发出的请求可能在不同时间到达，从而返回不同的、可能不一致的数据视图。相反，我们需要的是只读事务，它能为程序员提供分布在系统中多台机器上的一致且最新的数据视图；只写事务同样可以使程序员受益，因为它使程序员能够原子地更新分布在系统中多台机器上的数据；
在本论文中，我们描述了三个提供因果一致性的系统：COPS、COPS-RT 和 Eiger。每个处理系统都为可扩展的地理复制存储迈出了一步，其中 Eiger 取代了我们最初在 COPS 和 COPS-RT 上的努力；
我们的 COPS 顺序保持服务器集群系统提供因果一致性，旨在支持托管在小型/大型数据中心的复杂在线应用程序，每个数据中心由前端服务器（COPS 的客户端）和后端键值数据存储组成，COPS 以线性化的方式在本地数据中心执行所有读写操作，然后在后台以因果一致性的顺序跨数据中心复制数据；
COPS-RT（COPS-RT 中的 RT 代表只读事务）在 COPS 的基础上添加了只读事务，使客户端能够对分布在多台服务器上的多个键获得一致的视图。即使在完全线性化的系统中，也需要只读事务来获取多个键的一致视图。我们的只读事务不需要锁，是非阻塞的，并且最多只需要 2 个并行的数据中心内请求。这些只读事务确实需要付出一些代价：与常规版本的 COPS 相比，COPS-RT 对于某些负载（例如写入密集型）效率较低，并且对长时间网络分区和瞬时数据中心故障的鲁棒性较差；
我们的 Eiger 系统延续了这一思路，为 ALPS 系统提供了更强大的语义。这些更强大的语义包括为富列族数据模型提供因果一致性、改进的只读事务和只写事务。Eiger 的只读和只写事务算法均代表了当前最佳技术的进步；COPS 引入了一种只读事务算法，该算法通常在本地读取的 1 次方内完成，在最坏情况下则需要 2 次方。Eiger 的只读事务算法具有相同的属性，但它使用逻辑时间而不是显式依赖关系来实现这些属性。不存储显式依赖关系可以使 Eiger 避免 COPS-RT 的缺点。对于所有工作负载，Eiger 都与 COPS 一样高效，并且对长网络分区和瞬时数据中心故障具有很强的鲁棒性；
ALPS 系统的可扩展性要求是这项工作与之前的因果+一致系统之间最大的区别，以前的系统要求所有数据都放在一台机器上，或者所有可能一起访问的数据都放在一台机器上，相比之下，存储在 COPS、Eiger 中的数据可以分布在任意大小的数据中心，具有依赖关系、只读和只写事务，可以在数据中心的许多服务器上进行绘制。据我们所知，我们的工作描述了第一个实现因果+（以及因果）一致性的可扩展系统。
### ALPS 系统及其权衡
分布式存储系统有多个目标，有时甚至是相互竞争的：可用性、低延迟和分区容忍度，以提供始终在线的用户体验；可扩展性，以适应不断增长的负载和存储需求；以及足够强大的一致性模型，以简化编程并为用户提供他们期望的系统行为。更深入地说，理想的属性包括：（i）可用性 - 所有发送到数据存储的操作都完全成功，没有操作可以无限期阻塞或返回表示数据不可用的错误。可用性对于数据存储非常重要，因为它确保客户端可以与正常运行的服务交互。无响应的存储会导致无响应的服务很快变成未使用的服务。我们对可用性的定义也很重要，避免了通过快速返回人工神经网络错误来欺骗以提供低延迟的设计； (ii)低延迟-客户端操作快速完成，商业服务级别目标表明平均性能为几毫秒，最坏情况性能（即 99.9 百分位数）为十几毫秒或几百毫秒，低延迟对于数据存储至关重要，它可以防止与收入损失相关的缓慢的页面加载时间；(iii)分区容忍度-数据存储在网络分区下继续运行，例如将亚洲的数据中心与美国分开，数据中心的故障相当于将其与系统的其他部分分区，因此分区容忍系统在数据中心发生故障时也会继续运行；分区容错性和可用性是不同的，但具有互补的要求，系统可以提供一个属性而不能提供另一个属性，基于仲裁的系统被认为是分区容错的，因为大多数分区可以继续运行，但不被认为是可用的，因为少数分区中的节点不能形成有效的仲裁，因此无法完成操作，假设远离分区的系统可以被描述为可用的，即使在发生分区时该属性在实践中并不成立；同时提供分区容错性和可用性保证，只要系统的所有副本没有发生故障，它们就将继续运行并可用； (iv) 高可扩展性 - 数据存储呈线性扩展，向系统添加 N 个资源会使总吞吐量和存储容量增加 O(N)，可扩展性对于现代存储系统至关重要，支持 Web 服务的数据量持续增长，将所有数据放在一台机器上的系统已不再实用，而是必须使用可扩展的存储，将数据分布在多台机器上，并可随数据增长而增长，更大的存储系统可以支持更大的用户数，这是几乎所有 Web 服务的重要目标，更大的存储空间还可以增加每个用户的存储量，从而提供更丰富的 Web 服务，这也是许多 Web 服务的重要目标；(v) 更强的一致性 - 数据存储提供的一致性比最终一致性更强，我们在本论文中将其定义为实现因果一致性，我们将在后面的“因果一致性”部分中定义因果一致性；
理想的数据存储应该提供线性一致性（有时也称为强一致性），这意味着操作似乎在整个系统中作为单个实例生效，直到操作的调用和完成。在提供线性一致性的数据存储中，一旦客户端完成对一个数据中心中某个对象的写入操作，所有其他数据中心中对同一对象的读取操作都将反映其新写入的状态。线性一致性简化了编程，分布式系统提供了单一、一致的映像，用户体验到了他们期望的存储行为；许多大型分布式系统中常见的较弱的最终一致性模型不太直观：不仅后续读取可能无法反映最大值，跨多个对象的读取也可能反映新旧值的不连贯混合；
CAP 定理证明，具有可用性和分区容忍性的共享数据系统无法实现线性一致性，这种不可能性可以用非常直观的方式解释：如果发生网络分区，可用性要求分区一侧的写入必须能够完成，而线性一致性要求分区另一侧的后续读取必须返回该写入的结果，这显然是不可能的；
一些强一致性形式，例如线性一致性、串行一致性和顺序一致性，已被证明与低延迟不兼容，它定义为对于小于副本间最大光速延迟的一半的所有操作的延迟，例如，线性一致性很容易被证明是不兼容的：它要求对亚洲数据中心的写入操作必须立即反映在对美国数据中心的读取操作中。为了实现此属性，要么写入操作必须在返回前传播到美国站点，要么读取操作必须在返回前查询亚洲站点。这两种方法都与低延迟的目标不相容。一般来说，所有对多个数据位置上的操作进行全序的一致性模型都被证明与低延迟不兼容。
### 因果一致性
存储系统的一致性保证可以限制整个系统中操作的可能顺序和时间，从而有助于简化程序员必须推理的可能行为以及客户端可能看到的异常；
鉴于最强的一致性形式——线性化、序列化和顺序一致性——已被证明与我们的低延迟要求不兼容，之前的分区容忍、低延迟系统选择了最弱的一致性形式——最终一致性。最终一致性的普遍定义是，对一个副本的写入最终会出现在其他副本上，并且如果所有副本都收到了相同的写入集合，则它们对所有数据都将具有相同的值。这种弱一致性形式不会以任何方式限制不同键上操作的排序，迫使程序员推理所有可能的排序，并向用户暴露许多不一致性；
幸运的是，因果一致性可以避免许多此类不便的排序，同时保证低延迟；
为了定义具有收敛冲突处理的因果一致性（因果+一致性），我们首先描述其运作的抽象模型，我们的模型有两个基本操作：读取和写入，数据在逻辑副本中存储和检索，每个逻辑副本都承载整个键空间，这里单个逻辑副本对应整个本地节点集群；
我们模型中的一个重要概念是操作之间的潜在因果关系，3 条规则定义了潜在的因果关系，表示为 -->：（i）thread-of-exec - 如果 a 和 b 是单个 exec 线程中的 2 个操作，则如果操作 a 发生在操作 b 之前，则 a-->b，（ii）reads-from - 如果 a 是写操作而 b 是读操作，并返回由 a 写入的值，则 a-->b，（iii）传递性 - 对于操作 a、b、c，如果 a-->b、b-->c，则 a-->c，因此操作之间的因果关系是前 2 条规则的传递闭包；这些规则在同一个 exec 线程中建立了操作之间的潜在因果关系，并且其 exec 线程通过数据存储进行交互，我们的模型与许多模型一样，不允许线程直接通信，而是要求所有通信都通过数据存储进行； [点击查看图 3.1(a) 中操作的示例执行结果如图 3.1(b) 所示](./img/causal-consistency-example.png)，这里演示了所有 3 条规则，thread-of-exec 规则给出 w1~->w4，reads-from 规则给出 w1~->w2，传递性规则给出 w1~->w8；
**定义：**我们将因果一致性定义为两个属性的组合：因果一致性和收敛冲突处理。我们这里给出的是直观的定义，而非正式的定义。先前的研究描述了提供因果一致性的系统，我们的贡献包括识别因果和因果一致性之间的差异、命名因果一致性以及明确定义因果一致性；
因果一致性要求副本上读取操作返回的值与因果关系定义的顺序一致，即写入值的操作必须出现在所有因果关系先于该操作的操作之后。例如，在[图3.1(b)](./img/causal-consistency-example.png)中，w1必须出现在w3之前，而w3又出现在w6之前。如果客户端看到w6但没有看到w1，则因果一致性被违反；
因果一致性不区分并发操作的顺序。如果a不出现在b，且b不出现在a，则a和b是并发的。通常情况下，这可以提高实现效率。2. 不相关的写入可以按任意顺序复制，从而避免了它们之间需要序列化点。但是，如果a和b都是对同一个键的写入，则它们会发生冲突；
冲突是不可取的，原因有二：(i) 由于冲突是无序的，符合因果一致性原则，因此冲突会导致副本永远发散。例如，如果 a 向位置 X 写入 1，而 b 向位置 X 写入 2，则因果一致性允许一个副本永远返回 X 的 1，另一个副本永远返回 X 的 2。(ii) 冲突可能代表需要特殊处理的特殊情况。例如，在购物车应用程序中，如果登录到同一帐户的两个人同时将商品添加到购物车中，则期望的结果是购物车中同时包含这两件商品；
收敛冲突处理要求所有冲突写入在所有副本都以相同的方式处理，使用处理函数 h，h 必须是结合律和交换律，以便副本能够按照接收顺序处理冲突的写入，并且这些处理的结果能够收敛（例如，一个副本的 h(a,h(b,c)) 和另一个副本的 h(c,h(b,a)) 一致）；
一种以收敛方式处理冲突写入的常见方法是“后写入者获胜”规则（也称为 Thomas 写入规则），该规则声明其中一个冲突写入发生在较晚，并使其覆盖较早的写入。另一种处理冲突写入的常见方法是将其标记为冲突，并要求通过其他方式解决，例如通过用户直接干预（如 Coda 中所述），或通过编程过程（如 Bayou、Dynamo 中所述）；
所有潜在的收敛冲突处理形式都避免了第一个问题——冲突的更新可能会持续发散——通过确保副本在交换操作后达到相同的结果，另一方面，第二个与冲突有关的问题——应用程序可能需要对冲突进行特殊处理——只能通过使用更明确的冲突解决程序来避免。这些明确的程序为应用程序提供了更大的灵活性，但需要额外的程序员复杂性和/或性能开销，尽管我们的系统可以配置为明确检测冲突的更新并应用一些应用程序定义的解决方案，但我们系统的默认版本使用最后写入者获胜规则；
**因果+ 与其他一致性模型：**分布式系统文献定义了几种流行的一致性模型，包括：（i）线性化（或强一致性）-维持全局、实时排序，（ii）序列化-确保事务的全局排序，（iii）顺序一致性-确保操作的全局排序，（iv）因果一致性-确保依赖于操作的部分排序，（iv）fifo（pram）一致性-仅保留执行线程的部分排序，而不是线程，（v）每个键的顺序一致性-确保每个单独的键，所有操作都有一个全局顺序，（vi）最终一致性，一个包罗万象的术语，今天用来表示最终收敛到某种类型的协议； [点击查看我们引入的因果+一致性介于顺序和因果一致性之间](./img/a-spectrum-of-consistency-models.png)，它比顺序一致性弱，但顺序一致性在 ALPS 系统中是无法实现的，它比因果一致性和每个键的顺序一致性强，并且对于 ALPS 系统是可以实现的；
为了说明因果+模型中因果成分的效用，请考虑一个例子：用户将照片上传到互联网服务，然后将该照片添加到相册。在因果和因果+一致性下，照片将始终在 add2album 操作之前到达，这迫使程序员思考这种情况：他们应该删除 add2album 操作，在照片到达时排队回调，添加对相册的损坏引用，还是做其他事情？这种场景的概括是，操作 B 出现在正在进行的操作 A 之前，这映射到许多不符合用户期望和/或程序员必须推断乱序操作的情况，这些其他情况包括：(i) 删除好友的状态更新 (A)，然后回复 (B)；(ii) 将商品添加到购物车 (A)，然后检查商品是否存在 (B)；(iii) 状态更新 (A)，然后尝试隐藏它 (B)；(iv) 帐户操作 (A)，然后尝试登录 (B)；
如果数据存储是顺序一致的或线性化的，仍然有可能同时对一个键进行 2 次更新，但是，在这些更强大的模型中，可以实现互斥算法，用于完全避免产生冲突；
因果一致性：我们在系统中使用 2 个抽象：版本和依赖项，为了帮助我们推理因果一致性，我们将数据位置具有的不同值称为位置的版本，我们将其表示为 loc_version，在我们的系统中，版本的分配方式确保如果 xi-->yj 则 i<j，一旦副本返回位置 xi 的版本 i，因果一致性确保它将只返回该版本或因果更新的版本（请注意，冲突的处理因果关系晚于它解决的冲突写入，为了看到这一点，通过矛盾来思考以下场景：假设副本首先返回 xi 然后返回 xk，其中 i 不等于 k ​​且 xi 不 --> xk，因果一致性确保如果 xk 在 xi 之后返回，则 xk 不 --> xi，因此 xi、xk 冲突，但是，如果 xi、xk 冲突，则收敛冲突处理确保只要两者都预设在副本中，它们的处理 h(xi,xk)（因果关系在两者之后）将被返回，而不是 xi 或 xk，这与我们的假设相矛盾），因此我们系统中的每个副本始终返回密钥的非递减版本，我们将其称为因果+一致性进展性质；
因果一致性规定，所有在因果上先于给定操作的操作都必须先于该操作生效，即，如果 xi-->yj，则 xi 必须在 yj 之前写入，我们将这些先前的值称为依赖关系。更正式地说，当且仅当 write(xi)-->write(yj) 时，我们说 yj 依赖于 xi。这些依赖关系与写入的因果顺序相反，并且根据定义，它们与先行关系相同。[点击查看图 3.3(a) 中操作的因果关系图](./img/causal-consistency-example-2.png)(这与 [图 3.1(b)](./img/causal-consistency-example.png) 相同)，图 3.3(b) 显示了写入操作的相应依赖关系；
我们的系统在复制过程中提供因果一致性，即仅在所有依赖关系都得到满足后才写入版本，这意味着这些写入操作已在集群中应用；
**可扩展的因果关系**：据我们所知，我们的工作是第一个命名并正式定义因果一致性的系统。有趣的是，之前一些被认为实现了因果一致性的系统实际上实现了更强的因果一致性保证。这些系统并非设计用于提供可扩展的因果（或因果一致性）一致性，然而，由于它们都使用某种形式的日志序列化和交换，逻辑副本上的所有操作都按序列化顺序写入单个日志，通常用版本向量标记，然后不同的副本交换这些日志，使用版本向量建立潜在的因果关系并检测不同副本上操作的并发性；
基于日志交换的序列化削弱了副本的可扩展性，因为它依赖于每个副本中的单个序列化点来建立排序，因此，要么键的因果依赖关系仅限于可存储在一个节点 [12,25,46,59] 上的键集，要么单个节点（或复制状态机）必须为集群中的所有操作提供提交排序和日志；我们的系统通过采用不同的方法实现可扩展性，每个数据中心中的节点负责键空间的不同分区，但系统可以跟踪和强制存储在不同节点上的键之间的依赖关系，我们的系统将依赖关系显式编码到与每个键的版本关联的元数据中，当远程复制键时，接收数据中心在提交传入版本之前执行依赖关系检查，它们通过直接与负责存储其他键的节点通信来执行这些检查，通过这种方式，它们提供了实现可扩展因果+复制的第一个机制。
****
### Scheduling & MapReduce Scheduling
### VMware Distributed Resource Management :Design, Implementation, and Lessons Learned
### [Gulati12](https://www.waldspurger.org/carl/papers/drs-vmtj-mar12.pdf)
automated mgmt of physical resources is critical for reducing the operational costs of virtualized environments, an effective resource-mgmt solution must provide perf isolation among vms, handle resource fragmentation across physical hosts, and optimize scheduling for multiple resources, it must also utilize underlying hw infra efficiently, here we present design and implementation of 2 such mgmt solutions :DRS, DPM, we also highlight some key lessosn from production customer deployments over a period of more than 5yrs; 
VMware's DRS -distributed resource scheduler manages allocation of physical resources to a set of vms deployed in a cluster of hosts, each running the VMware ESX hypervisor, DRS maps vms to hosts and performs intelligent load balancing in order to improve perf and to enforce both user-specific policies and system-level constraints, using a variety of experiements, augumented with simulation results, we show that DRS significantly improves overall perf of vms running in a cluster, DRS also supports a what-if mode, making it possible to evaluate the impact of changes in wkloads or cluster config; 
VMware's DPM -distributed power mgmt extends DRS with the ability to reduce power consumption by consolidating vms onto fewer hosts, DPM recommends evacuating andd powering off hosts when cpu and mem resources are lightly utilized, it recommends powering on hosts appropriately as demand increases, or as required to satisfy resource-mgmt policies and constraints, our extensive evaluation shows that in clusters with non-trivial periods of lowered demand, DPM reduces server power consumption significantly. 
initially the rapid adoption of virtualization was fueled by significant cost savings resulting from server consolidation, running several vms on a single physical host improved hw utilization, allowing admins to do more with less and reduce capital expenses, later, more advanced vm capabilities such as cloning, template-based deployment, checkpointing, and live migration of running vms led to more agail it infras, as a result, it became much easier to create and manage vms; 
the ease of deploying wkloads in vms is leading to increasingly large vm installations, moreover, hw technology trends continue to produce more powerful servers with higher core counts and increased mem density, causing consolidation ratios to rise, however, the operational expense of managing vms now represents a significant fraction of overall costs for datacenters using virtualization, ideally, the complexity of managing a virtualize environment should also benefit from consolidation, scaling with #hosts, rather than #vms, otherwise, managing a virtual infra would be as hard -or arguably harder, due to sharing and contention -as managing a physical environment, where each application runs on its own dedicated hw; 
in practice, we observed that a large fraction of the operational costs in a virtualized environment were related to the inherent complexity of determining good vm2host mappings, and deciding when to use vMotion, VMware's live migration technology, to rebalance load by changing those mappings, the difficulty of this problem is exacerbated by the fragmentation of resources across many physical hosts and the need to balance utilization of multiple resources(including cpu and mem) simultaneously; 
we also found that admins needed a reliable way to specify resource-mgmt policies, in consolidated environments, aggregate demand can often exceed supply of physical resources, admins need expressive resource controls to prioritize vms of varying importance, in order to isolate and control perf of diverse wkloads competing for the same physical hw; 
we designed DRS to help reduce the operational complexity of running a virtualized datacenter, DRS enables managing a cluster containing many potentially-heterogeneous hosts as if it were a single pool of resources, in particular, DRS provides several key capabilities including: (i)a cluster abstraction for managing a collection of hosts as a single aggregate entity, with the combined processing and mem resources of its constituent hosts, (ii)a powerful resource pool abstraction, which supports hierarchical resource mgmt among both vms and groups of vms, at each level in the hierarchy, DRS provides a rich set of controls for flexibility expressing policies that manage resource contention, (iii)automatic initial placement, assigning a vm to a specific host within the cluster when it is powered on, (iv)dynamic load balancing of both cpu and mem resources across hosts in response to dynamic fluctuations in vm demands, as well as changes to the physical infra, (v)custom rules to constrain vm placement, including affinity and anti-affinity rules both among vms and bt vms and hosts, (vi)a host maintenance mode for hw changes or sw upgrades, which evaluates running vms from one host to other hosts in the cluster; 
here we discuss design and implementation of DRS and a related technology called DPM, DRS provides automated resource-mgmt capabilities for a cluster of hosts, DPM extends DRS with automated power mgmt, powering off hosts(placing them in standby) during periods of low utilization, and powering them back on when needed; 
our experimental evaluation demonstrates that DRS is able to meet resource-mgmt goals while improving utilization of underlying hw resources, we also show that DPM can save significant power in large configs with many hosts and vms, both of these features have been shipping as VMware prodcucts for more than 5yrs, they have been used by thousands of customers to manage hundreds of thousands of hosts and millions of vms worldwide.
### Resource Model
here we discuss DRS resource model and its associated resoure controls, a resource model explains capabilities and goals of a resource-mgmt solution, DRS offers a powerful resource model and provides a flexible set of resource controls, a wide range of resource-mgmt policies can be specified by using these controls to provide differentiated QoS to groups of vms; 
**basic resource controls:** VMware's resource controls allow admins and users to express allocations in term of either absolute vm allocation or relative vm importance, control knobs for processor and mem allocations are provided at the individual host level by the VMware ESX hypervisor, DRS provides exactly the same controls for a distributed cluster consisting of multiple ESX hosts, allowing them to be managed as a single entity, the basic VMware resource controls including: (i) reservation: specifies a minimum guaranteed amount of a certain resource, i.e. a lower bound that applies even when this resource is over-committed heavily, reservations are expressed in absolute units such as MHz for cpu, MB for mem, admission control during VMpower-on ensures that sum of reservations for a resource does not exceed its total capacity; 
(ii) limit: specifies an upper bound on consumption of a certain resource, even when this resource is under-committed, a vm is prevented from consuming more than its limit, even if that leaves some resources idle, like reservations, limits are expressed in concrete absolute units such as MHz and MB; 
(iii) shares: specify relative importance, and are expressed using abstract numeric values, a vm is entitled to consume resources proportional to its share allocation, it is guaranteed a minimum resource fraction equal to its fraction of total shares when there is contention for a resource, in the literature, this control is sometimes referred to as a weight; 
the ability to express hard bounds on allocations using reservations and limits is extremely important in a virtualized environment, without such guarantees, it is easy for vms to suer from unacceptable or unpredictable perf, meeting perf objectives would require resorting to crude methods such as static partitioning or over-provisioning of physical hw, negating the advantages of server consolidation, this motivated the original implementation of reservations and limits in ESX, as well as their inclusion in DRS; although DRS focuses on cpu and mem resources, similar controls for io resources have been validated by a research prototype, VMware also offers shares and limit controls for network and storage bdwidth, a new StorageDRS functionality for virtual disk placement and load-balancing across storage devices; 
**resource pools:** in addition to the basic, per-vm resource controls, admins and users can specify flexible resource-mgmt policies for groups of vms, this is facilitated by introducing concept of a logical resource pool -a container that species an aggregate resource allocation fro a set of vms, a resource pool is a named object with associated settings for each managed resource -the same familiar shares, reservation, and limit controls used for vms, admission control is performed at the pool level :sum of reservations for a pool's children must not exceed pool's own reservation; resource pools may be configured in a flexible hierarchical org :each pool has an enclosing parent pool, and children that may be vms or sub-pools, resource pools are useful for dividing or sharing aggregate capacity among groups of users or vms, for example, admins often use resource-pool hierarchies to mirror human organizational structures, resource pools also provide direct support for delegated administration :an admin can allocate resources in bulk to sub-admins using sub-pools; [click for a resource pool structure defined by an example org](./img/resource-pool-structure.png), here resources are first split across 2 groups :business, testing, business is further sub-divided into it, sales groups, while testing is a flat collection of vms, separate per-pool allocation provide both isolation bt pools and sharing within pools; for example, if some vms within the sales pool are idle, their unused allocation will be reallocated preferentially to other vms within the same pool, any remaining spare allocation flows preferentially to other vms within business, its enclosing parent pool, then to Org, its ancestor, note that in a resource-pool hierarchy, shares are meaningful only with respect to siblings :each pool effectively defines a scope(similar to a currency) within which share values are interpreted; a distinguished root resource pool for a cluster represents the physical capacity of the entire cluster, which is divided up among its children, all resource pools and vms in a cluster are descendants of the root resource pool; 
**resource pool divvy:** a resource pool represents an aggregate resource allocation that may be consumed by its children, we refer to the process of computing the entitled reservation, limit, and shares of its child sub-pools and VMs as divvying; divvying is performed in a hierarchical manner, dividing resources associated with a parent pool among its children, divvying starts from root of the resource pool hierarchy, and ultimately ends with vms at its leaves, DRS uses resulting entitlements to update host-level settings properly, ensuring that controls such as reservations and limits are enforced strictly by ESX hypervisor, DRS also uses host-level entitlement imbalances to inform vm migration decisions; during divvying, DRS computes resource entitlements for individual pools and vms, divvying computations incorporate user-sepcified resource controls settings, as well as per-vm and aggregate wkload demands, since pool-level resource allocations reflect vm demands, DRS allows resources to flow among vms as demands change, this enables convenient multiplexing of resources among groups of vms, without the need to set any per-vm resource controls; divvying must handle several cases in order to maintain the resource pool abstraction, for example, it is possible for the total reservation of a parent pool to be greater than sum of its children's reservations, similarly, the limit at a parent pool can be smaller than sum of its children's limit values, finally, a parent's shares need to be distributed among its children in proportion to each child's shares value, in all such cases, parent values need to be divided among children based on the user-set values of reservation, limit, shares, and actual runtime demands of children; these divvy ops are defined :reservation-divvy, limit-divvy, share-divvy, named for their respective resource controls, DRS carries out these divvy ops periodically(by defult, every 5mins), reflecting current vm demands, DRS also initiates divvy ops in response to changes in resource allocation settings; the divvy algorithm works in 2 phases: (i)in the first bottom-up phase, divvying starts with demand values of individual vms, which are leaf nodes, it then accumulates aggregate demands up the resource pool tree; vm demands are computed by ESX hypervisor for both cpu and mem, a vm's cpu demand is computed as its actual cpu consumption CPU_used plus a scaled portion of CPU_ready, the time it was ready to execute but queued due to contention is: *CPU_demand = CPU_used + (CPU_run/(CPU_run+CPU_sleep))timesCPU_ready*; a vm's mem demand is computed by tracking a set of randomly-selected pages in the vm's physical addr space, and computing how many of them are touched within a certain time interval, for example, if 40% of the sampled pages are touched for a vm with 16GB mem allocation, its active mem is estimated to be 0.4times16=6.4GB; once the per-vm demand values have been computed, they are aggregated up the tree, demand values are updated to always be no less than the reservation and no more than the limit value, hence at each node, the following adjustments are made: *demand=MAX(demand, reservation) demand=MIN(demand, limit)*; (ii)the second divvying phase proceeds in a top-down manner, reservation and limit values at each parent are used to compute respective resource settings for its children such that the following constraints are met including: (i)child allocations are in proportion to their shares, (ii)each child is allocated at least its own reservation, (iii)no child is allocated more than its own limit, in order to incorporate demand info, if sum of the children's demands is larger than the quantity(reservation or limit) that is being parceled out, we replace limit values of children by their demand values: *limit=MIN(demand, limit)*, this allows reservation and limit settings at a parent to flow among its children based on their actual demands; conceptually, the divvy algorithm at a single level in the tree can be implemented by first giving reservation to every child, then the extra reservation or limit, depending on what is being divvied out, can be allocated in small chunks to children, by giving a chunk to the child with minimum value of current allocation/shares, if a child's current allocation hits its limit, that child can be taken out from further consideration and capped at the limit; the share-divvy is performed by dividing a parent's shares among its children, in proportion to shares of each child, demands do not play any role in the share-divvy op; we illustrate divvy ops using a simple example with a 2-level resource pool tree, [click for a resource pool tree](./img/resource-pool-tree.png) with cpu reservation R=10GHz, limit L=20GHz, and shares S=1000 at the root node, shares are unitless and the other two settings are in absolute cpu units(GHz), the user-set resource control values are denoted by U, and the final divvied values are denoted by D, although this example shows only cpu divvying, a similar computation is performed for mem, here in fig2, there are 2 resource pools under the root, RP1, RP2, each with 2 child vms, first, the bottom-up divvying phase aggregates demands up the tree, starting with individual vm demands, next, the top-down divvying phase starts at the root node, doing reservation and limit divvy recursively until reaching the leaf nodes; the 10GHz reservation at the root is greater than the 5GHz sum of its children's reservations, hence the reservation is divvied based on shares, while meeting all divvying constraints, in this case, shares-based allocation of 8GHz and 2GHz satisfies the reservation and limit values of the 2 resource pools, next, the reservation of 8GHz at RP1 is divvying among vm1, vm2, since 8GHz is smaller than aggregate demand of 10GHz from vm1 and vm2 we replace limit values of vm1 and vm2 by their demand values of 3GHz and 7GHz respectively, as a result, even though shares of these 2 vms are equal, the divvied reservations are 3GHz and 5GHz(instead of 4GHz each), this illustrates how demands are considered while allocating a parent's reservation; note that vm demands do not act as an upper bound while divvying limit values here, since sum of the children's demands is smaller than the limit at the parent, as a result, the actual limit values of the children are used in the divvying, shares are simply distributed in proportion to shares of children, although we have not walked through every computation in this resource pool tree, one can verify that both reservation and limit values can be divvied in a top-down pass starting from root, while considering user-set values and current demands.
### DRS Overview and Design
DRS is designed to enforce resource-mgmt policies accurately, delivering physical resources to each vm based on the resource model described in the previous section, [click for DRS runs as part of the vCenter Server centralized mgmt sw](./img/drs-runs-part-of-the-vcenter-server-centralized-management-software.png), it manages resources for a cluster of ESX hosts as well as vms running on them, more specifically, DRS performs 4 key resource-mgmt ops including: (i)it computes the amount of resources to which each vm is entitled based on the reservation, limit, and shares values and the runtime demands for all related vms and resource pools, (ii)it recommends and performs migration of powered-on vms to balance load across hosts in a dynamic environment where vm's resource demands may change over time, (iii)it optioanlly saves power by invoking DPM, (iv)it performs initial placement of vms onto hosts, so that a user does not have to make manual placement decisions; 
DRS load balancing is inboked periodically(by default, every 5mins) to satisfy cluster constraints and ensure delivery of entitled resources, it is also invoked on demand when user makes cluster config changes, such as adding a host to the cluster or requesting that a host enter maintenance mode, when DRS is invoked, it performs the first 3 resource-mgmt ops listed above, along with a pass to correct cluster constraint violations, for example, constraint correction evacuates vms from hosts that user has requested to enter maintenance or standy mode; 
DRS initial placement assigns a vm to a host within a cluster when vm is powered-on, resumed from a suspended state, or migrated into cluster manually, DRS initial placement shares code with DRS load balancing to ensure that placement recommendations respect constraints and resource entitlements; 
**load balancing:** we first examine DRS load-balancing metric and algorithm, we then consider in more detail how DRS analyzes possible load-balancing moves in terms of their impact on addressing imbalance, their costs and benefits, and their interaction with pending and dependent actions, including: (i) load balancing metric: is dynamic entitlement, which differs from the more commonly used metric of host utilization in that it reflects resource delivery in accordance with both needs and importance of vms, dynamic entitlement is computed based on overall cluster capacity, resource controls, and the actual demand for cpu and mem resources from each vm; a vm's entitlement for a resource is higher than its reservation and lower than its limit :the actual value depends on cluster capacity and total demand; dynamic entitlement is equivalent to demand when demands of all vms in the cluster can be met, otherwise, it is a scaled-down demand value with the scaling dependent on cluster capacity, the demands of other vms, the vm's place in the resource pool hierarchy, and its shares, reservation, and limit; dynamic entitlement is computed by running the divvy algorithm over the resource pool hierarchy tree, for entitlement computation, we use the cluster capacity at the root as the quantity that is divvied out, this is done for both cpu and mem resources separately; DRS currently uses normalzied entitlement as its core per-host load metric, reflecting host capacity as well as entitlements of running vms, for a host *h*, normalized entitlement *Nb* is defined as sum of the per-vm entitlements *Ei* for all vms running on *h*, divided by host capacity *Ch* available to vms: *Nh=(sum_Ei)/Ch*, if *Nh<=1* then host *h* is deemed to have insufficient resources to meet entitlements of all its vms, and as a result, vms on that host would be treated unfairly as compared to vms running on hosts whose normalized entitlements were not above 1; after calculating *Nh* for each host, DRS computes the cluster-wide imbalance *Ic* which is defined as the standard deviation over all *Nh* values, the cluster-wide imbalance considers both cpu and mem imbalance using a weighted sum, in which weights depend on resource contention, if mem is highly contended, i.e. its max normalized entitlement on any host is above 1, then it is weighted more heavily than cpu, if cpu is highly contended, then it is weighted moe heavily than mem, equal weights are used if neither resource is highly contended, the ration 3:1 derived by experimentation, is used when one resource is weighted more heavily than the other; 
(ii) [load balancing algorithm, click to view](./img/drs-load-balancing-algorithm.png): here this overview of DRS algorithm has been greatly simplified to focus on its core load-balancing metric, the actual load-balancing algorithm considers many other factors, including the impact of move on imbalance and risk-adjusted benefit of each move; DRS load-balancing algorithm uses a greedy hill-climbing technique, this approach, as opposed to an exhaustive, offline approach that would try to find the best target balance, is driven by practical considerations, the live migration ops used to improve load-balancing have a cost, and vm demand is changing overtime, so optimizing for a particular dynamic situation is not worthwhile; DRS aims too minimize cluster-wide imbalance *Ic* by evaluating all possible single-VMmigrations, many filtered quickly in practice, and selecting the move that would reduce *Ic* the most, the selected move is applied to the algorithm's current internal cluster state so that it reflects the state that would result when migration completes, this move-selection step is repeated until no additional beneficial moves remain, there are enough moves for this pass, or the cluster imbalance is at or below threshold *T* specified by DRS admin, after the algorithm completes, an exec engine performs the recommended migrations, optionally requiring user-approval; 
(iii) minimum goodness: DRS load balancing rejects a move if it does not produce enough benefit in terms of improvement in the standard deviation value representing imbalance, the threshold used for this filtering is computed dynamically based on #hosts and vms, the threshold si reduced significantly when imbalance is very high and moves to correct it are filtered by the normal threshold, so that many low-impact moves can be used to correct high imbalance; 
(iv) cost-benefit analysis: the main check DRS uses to filter unstable moves is cost-benefit analysis, it considers various costs involved in the move by modeling the vMotion costs as well as cost to the other vms on the destination host which will have an additional vm competing for resources, the benefit is computed as how much the vms on the source and the migrating vm will benefit from the move, cost-benefit analysis also considers risk of the move by predicting how the wkload might change on both the source and destination and if this move still makes sense when the wkload changes; the cost is modeled by estimating how long this vm would take to migrate, the migration time mainly depends on mem size of the vm as well as how actively the vm modifies its pages during migration, if the vm dirties its pages frequently during vMotion, they must be copied to the destination host multiple times; DRS keeps track of the transfer rate based on history of migration times for each vm and host, based on the transfer rate and current mem size, it calculates the time required for single rd of copyig, during the time the cost is computed as the resources required for the migrating vm to exist on the destination, the vMotion cost is in the unit of resources(MHz or MB) overtime; the vMotion process itself consumes some resources on both the source and destination hosts and this is also added as a cost, DRS then computes how much the wkload inside the vm would suer due to migration, this is done by measuring how actively the vm writes to its mem pages and how much time each transfer takes, DRS approximates this by measuring #times the vm had to be descheduled by the cpu scheduler during past vMotions and how active the vm was at that time, the perf degradation due to increased cost of modifying mem during migration is computed, this value is used to extrapolate how much the vm would suer by taking into account its current consumption and added to the cost, this cost is also measured in units of resources overtime, such as cpu sec; the vMotion benefit is also measured in resources overtime, the benefit is computed by calculating how much of the demand for the candidate vm is being clipped on the source versus the increased amount of demand that is expected to be satisfied on the destination, the benefit also includes the amount of unsatisfied demand for other vms on the source that will be satisfied after vm moves off, to represent possible demand changes, demand is predicted for all vms on the source and destination, based on wkloads of all these vms, a stable time is predicted after which we assume the vm wkloads will change to the worst config for this move, given recent demand history(by default, for the previous hr), use of this worst-case value is intended to make the algorithm more conservative when recommending migrations; with respect to the source host, the worst possible situation is computed as one in which, after the stable time, wkloads of the vms on the source exhibit the minimum demand observed during the previous hr, with respect to the destination host, the worst possible situation is computed as one in which demands for the vms on the destination including the vm being moved are the maximum values observed in the last 1hr, by assuming this worst-case behavior, moves with benefits that depend on vms with unstable wkloads are filtered out; 
(v) pending recommendations: DRS considers recommendations in flight and any pending recommendations not yet started, so that it does not correct the same constraint violation or imbalance several times, vms that are currently migrating are treated as if they exist on both source and destination, new recommendations that would conflict with pending recommendations are not generated; 
(vi) move dependency: when DRS generates a sequence of moves, some vm migrations in the sequence may depend on capacity freed up on a host by a vm migration off that host earlier in the sequence, for example, during the constraint violation correction step, DRS may generate a recommendation to move vm x off host A to correct a rule violation, and then during the load-balancing step, it may generate a recommendation to move vm y to host A that depends on resources freed up by the migration of vm x, for such sequences, the second move should be executed only after the first move succeeds, DRS can designate dependencies within its output recommendations, and these dependencies are respected by DRS recommendation exec engine, DRS does not issue sequences of moves whose dependencies cannot be satisfied; 
(vii) host-level resource settings: in maintaining the illusion that the cluster is a single large host with the aggregate capacity of its constituent hosts, DRS breaks up the user-specified resource-pool hierarchy into per-host resource pool hierarchies with appropriate host-level resource pool settings, hence while vms are running on a host, the local schedulers on each ESX host allocate resources to vms fairly, based on vm resource settings and on the host-level resource pool tree provided to the host by DRS; at the beginning of each balancing invocation, DRS runs the reservation-divvy and limit-divvy algorithm to capture impact of any changes in vm demand overtime, based on any differences bt the updated divvy results and those the algorithm last produced, DRS generates recommendations to adjust the resource trees and settings on hosts in the cluster, in accordance with vms running on that host; 
**vm initial placement:** is invoked to perform admission control and host selection for vm power-on, for resumption of a suspended vm, or for manual migration of a running vm into a cluster, DRS can be asked to place a single vm, for which it attempts to generate one or more alternative host placement recommendations, or it can be asked to place a set of vms, for which it attempts to generate a single coordinated recommendation comprised of a placement action for each vm, the latter handles placement of multiple vms more efficiently and effectively than a series of individual vm placements, because it builds a single representation of the cluster to place the set of vms and because the algorithm can order its placement of vms to facilitate bin-packing, specific errors are issued for any vms DRS cannot place; during placement, DRS does not have an estimate of the vm's current cpu and mem demand, it makes the conservative assumption that the vm being placed will consume its maximum possible load, i.e. that its mem demand will match it configured mem size and its cpu demand will be such that each of its virtual cpus(vCPUs) will consume a physical core, DRS placement code leverages DRS load-balancing code to evaluate the relative goodness of each possible placement; however, one way that placement differs from load balancing is that placement considers any prerequisite moves that may be necessary, if a vm cannot be placed without a constraint violation on any powered-on host in the cluster, DRS next considers placing the vm on each of the hosts, while attempting to correct the resulting constraint violations via prerequisite moves of other vms on the host, and if no such placement on powered-on hosts is possible, DRS considers powering-on standby hosts for the vm via DPM; DRS also handles initial placement of fault tolerant vms, an ft vm consists of both primary and secondary vms, with the secondary being a replica of the primary that runs in lock-step to allow immediate failover, the primary and secondary vms must be placed on different hosts that are vMotion compatible, since it is computationally very expensive to consider all possible pairs of hosts, DRS places the primary vm on the best possible host in terms of goodness, then from the subset of hosts that are vMotion-compatible with the primary, it picks the best host for the secondary, if no secondary host can be found, it considers the second-best host for the primary and tries again, this process is repeated until a host is found for both primary and secondary or there are no more hosts left to consider for the primary vm; 
**constraints:** the fundamental constraint respected by DRS is vm2host compatibility, i.e. the ability of a host to satisfy a vm's exec requirements, vm2host compatibility info is passed to DRS from the compatibility-checking module of the vCenter mgmt sw, and DRS respects this constraint during load balancing and vm placement; in addition, DRS provides support for enforcing a set of constraints to handle various use cases as co-location of vms for perf, placement of vms on separate hw to handle licensing issues, it also handles evacuation of hosts the user has requested to enter maintenance or standby mode, preservation o spare resources for failover, and the role of special vms that provide services to other vms, DRS respect constraints during vm initial placement and runs a pass prior to load balancing to correct violations of DRS-enforced constraints, reporting errors for any of those constraints that cannot be corrected, we discuss several constraints and their specific use cases next including: (i) affinity rules: DRS supports vm2vm or vm2host rules, which are used for a variety of common business scenarios, vm2vm anti-affinity rules define a set of vms that are to be kept on separate hosts, these rules are typically used for availability and are mandatory, i.e. DRS will not make any recommendation that would violate them such as avoiding a single point of failure due to running 2 vms on the same host, vs. vm2vm affinity rules define a set of vms that are to kept on the same host and are used to enhance perf of communicating vms, because infra-host vm2vm networking is optimized to perform in-mem packet transfers, without using NIC hw, these rules are mandatory for load balancing, but DRS will violate them if necessary to place vms during power-on, vm2vm rule violations are corrected during initial phase of a load balancing run and that corrected state is maintained during load balancing, for vm2vm anti-affinity, potential balancing moves introducing violations are filtered :for vm2vm affinity, potential balancing moves are formed by treating each set of affine vms on a host as if it were a single large vm; vm2host rules define a set of vms being affine or anti-affine with a set of hosts, these rules can be specified as mandatory or preferred, with the latter meaning that they are enforced unless they engender additional constraint violations or cannot be respected without causing host overutilization, vm2host rules are useful for a variety of reasons, mandatory vm2host affinity rules are often used to enforce licensing , to associate vms requiring a host-based sw license with hosts having that license, preferred vm2host affinity or anti-affinity rules are often used to manage availability and/or site locality; mandatory vm2host rules are represented in the vm2host compatibility info passed to DRS and hence are handled as a fundamental constraint, preferred vm2host rules are represented as alternative vm2host compatibility info, DRS handles preferred vm2host rules by running a what-if pass with the preferred rules treated as mandatory, if the resulting cluster state has no constraint violations or overutilized hosts, the result of the pass is accepted, otherwise, DRS retries the what-if pass with the preferred rules dropped and accepts the latter result if it has fewer constraint violations or overutilized hosts, else it accepts the former result; 
(ii) high availability: vSphere HA -high availability is a cluster service that handles host failures, and restarts failed vms on remaining healthy costs, ha runs as a decentralized service on the ESX hosts and keeps track of liveness info through heart-beat mechanisms, DRS supports ha functionality in 2 ways :by preserving powered-on idle resources to be used for vm restart in accordance with ha policy and by defragmenting resources in the cluster when ha restart cannot find sufficient resources; for DRS to preserve enough powered-on idle resources to be used for vm restart, ha needs to express to DRS the resources required based on ha pilicy settings, ha supports 3 months by which users can express resources they need preserved for failover including: (i)#host failures they would like to tolerate, (ii)percentage of resources they would like to keep as spare, (iii)designation of particular hosts as failover hosts, which are not used for running vms except during failover; ha expresses implication of these policies to DRS using 2 kinds of constraints :minimum amount of cpu and mem resources to be kept powered-on and size and location of unfragmented chunks of resources to be preserved for use during failover, the latter is represented in DRS as spare vms, which act like special vms with reservations used to ensure DRS maintains spare resource slots into which vms can be failed over and whose vm2host compatibility info is used to create those slots on appropriate hosts; spare vms are also used in the unusual event that more resources are needed for failure restart than expected due to failure of more hosts than configured by the ha policy, for any vms that ha cannot restart, DRS is invoked with spare vms representing their config, with the idea that it may be able to recommend host power-ons and migrations to provide needed resources; 
(iii) ESX agent vms: some services that an ESX host provides for vm use(such as vShield) are encapsulated in vms, these vms are called ESX agent vms, an agent vm needs to be powered on and ready to serve prior to any non-agent vm being powered on or migrated to that host, and if the host is placed in maintenance or standby mode, the agent vm needs to be powered off after the non-agent vms are evacuated from the host; to support agent vms, DRS respects their role as part of the host platforms, which has a number of implications, the DRS algorithm does not produce recommendations to migrate or place non-agent vms on hosts on which required agent vms are not configured, on hosts with configured agent vms, DRS algorithm respects the agent vm's reserved resources even when they are not in a ready2serve state, the DRS exec engine understands that non-agent vms need to wait for required agent vms to be powered on and ready to serve on the target host, and the DRS load-balancing code understands that agent vms do not need to have evacuation recommendations produced for them when a host is entering maintenance or standby mode :the agent vms are automatically powered off by the agent vm framework after the non-agent vms are evacuated.
### DPM Overview and Design
DPM is a feature of DRS that opportunistically saves power by dynamically right-sizing cluster capacity to match wkload demands, while respecting all cluster constraints, DPM recommends vm evacuation and powering off ESX hosts when the cluster remains sufficient spare cpu and mem resources, it recommends powering ESX hosts back on when either cpu or mem resource utilization increases appropriately or additional host resources are needed to meet cluster constraints, with DRS itself recommending host power-ons for the latter reason, DPM runs as part of the DRS balancing invocation as an optional final phase; 
note that in addition to DPM, each ESX host performs HPM -host power mgmt, which uses ACPI P-states and C-states on ESX hosts to reduce host power consumption while respecting vm demand, HPM works synergistically with DPM in reducing #running hosts and HPM reducing power consumed by those hosts; 
DPM decouples detection of the need to consider host power-on or power-off from selection of the host, which allows host selection to be independent of how hosts are currently utilized, this means that DPM host selection for power-off does not depend on finding hosts on which both cpu and mem utilization are currently low :DPM can migrate vms to exploit low utilization of these resources currently not manifest on the same host, and more importantly, this decoupling means that DPM host selection for power-off can consider criteria that are more critical in the longer term than current utilization, such as headroom to handle demand burst, power efficiency, or temp; 
for example, host A may currently have lower cpu and mem utilization than host B, but host B may be a better candidate for power-off, because it is smaller than host A and hence is less able to handle vm demand burst than host A, or it is less power-efficient than host A, or it is in a hotter part of the datacenter than host A; 
**utilization evaluation:** to determine whether the currently powered-on cluster capacity is appropriate for resource damands of the running vms, DPM evaluates cpu and mem resource utilization(vm demand over capacity) of powered-on hosts in the cluster, utilization can exceed 100% since demand is an estimate of vm resource consumption assuming no contention on the host, as mentioned in section resource pool divvy, a vm's cpu demand includes both its cpu usage and a portion of its ready time, if any, a vm's mem demand is an estimate of its working set size; DPM considers demand over an extended time period and it uses the mean demand over that period plus 2 standard deviations for the utilization calculation, this makes DPM conservative with respect to host power-off, which is the bias customers prefer, in order to be relatively quick to power-on a host in response to demand increases and relatively slow to power-off a host in response to demand decreases, the time period used for host power-off is the last 40mins and for host power-on is the last 5mins; utilization is characterized with respect to a targer range of 45-81% (i.e. 63+-18), with utilization above 81% considered high and utilization below 45% considered low, DPM evaluates cluster utilization using a measure that incorporates cpu and mem utilization on a per-host basis in such a manner that higher utilizations on certain hosts are not offset by lower utilizations on other hosts, this is done because DRS cannot always equalize vm load access hosts due to constraints; use of this metric allows DPM to consider host power-on to address high utilization on a few hosts in the cluster (such as ones licensed to run Oracle vms) when the overall utilization of the powered-on hosts in the cluster is not high, and to consider host power-off to address low utilization on a few hosts in the cluster (such as ones not connected to a popular shared storage device) when the overall utilization of the powered-on hosts in the cluster is not loww, note that DPM executes on a representation of the cluster produced by a complete DRS load balancing pass; DPM computes a cluster-wide high and a low score for each resource, considering hosts whose utilization for the power-on demand period is above the high-utilization threshold(default 81%): *HighUtilizationScore=sqrt^{sum_(DistanceAboveThreshold^2)}*, considering hosts whose utilization for the power-off demand period is below the low utilization threshold(default 45%): *LowUtilizationScore=sqrt^{sum_(DistanceBelowThreshold^2)}*, if the high score is greater than zero for either cpu or mem, DPM considers producing host power-on recommendation, otherwise, if low score is greater than zero for both cpu and mem(possibly on different hosts), DPM considers producing host power-off recommendations, the key intuition behind this scoring is that we want to identify cases where a subset of hosts may have very high or very low utilizations that can be acted upon by suggesting power-on and power-off ops respectively; 
**host power-off recommendations:** DPM considers host power-off in the current invocation if 2 criteria are met :non-zero low scores for both cpu and mem and zero high scores for both, to evaluate various alternatives, DPM considers each candidate host in a sorted order, if a candidate host power-off passes both cluster utilization and cost-benefit evaluations described below, the recommendation to power it off along with its prerequisite vm evacuation recommendations is added to the list of the recommendations that will be issued by this invocation of DRS, the internal representation of the cluster is then updated to reflect effects of this recommendation, DPM continues to consider host power-off until either the cluster utilization scores no longer show both low cpu and mem utilization or there are no more candidate hosts to be considered; for a condidate host, DPM runs DRS in a what-if mode to correct constraints and load balance the cluster assuming that the host were entering standby, if what-if DRS can fully evacuate the host, DPM evaluates the resulting cluster state using the utilization scores given in the previous section and compares scores with those of the cluster state before the host power-off was considered, if the cluster low utilization score is lowered and if the cluster high utilization score is not increases, then the candidate host power-off is next subjected to DPM power-off cost-benefit analysis; DPM cost-benefit analysis considers risk-adjusted costs and benefits of the power-off, costs include cpu and mem resources associated with migrating vms off the powering-off host and the corresponding resources associated with repopulating the host when it powers back on in the future, costs also include risk-adjusted resource shortfall with respect to satisfying vm demand if the host's cpu and/or mem resources are needed to meet that demand while host is entering standby, powered-off, or rebooting, the risk-adjusted resource shortfall is computed assuming the current demand persists for the expected stable time of that demand given an analysis of recent historical stable time, after which demand spikes to a worst-case value, this value is defined to be the mean of the demand over the last 60mins plus 3 standard deviations; the benefit is the power saved during the time the host is expected to be down, which is translated into cpu and mem resource currency(the host's MHz or MB multiplied by time in standby) to be weighed against cost, by default, the benefit must outweigh the cost by 40x to have the power-off pass DPM's cost benefit filter :this value was chosen based on experimentation to match customer preference of not impacting perf to save power; 
**host power-on recommendations:** DPM evaluates host power-on given high cpu or mem utilization of any powered-on host, DPM considers each candidate host in a sorted order, if a condidate host power-on passes the cluster utilization evaluation described below, the recommendation to power it on is added to the list of recommendations generated by DRS, the internal representation of the cluster is also updated to reflect effects of this recommendation, DPM continues to consider host power-on until either cluster utilization scores no longer show either high cpu or mem utilization or there are no more candidate hosts to be considered; for a condidate host, DPM runs DRS in a what-if mode to correct constraints and load balance the cluster assuming that host were powered-on, if the host power-on reduces the high utilization score, then the stability of that improvement is evaluated, stability of the improvement involves checking that the greedy rebalancing performed by DRS in the presence of that host was better because of the addition of that host, this is checked by doing a what-if power-off of the same candidate host and checking that the improvement in the high-utilization score obtained by considering its power-on does not remain, if the host power-on benefit is stable, then the host is selected for power-on, some extra evaluation is performed for the last host needed to address all remaining high utilization in the cluster, in that case, DPM evaluates multiple hosts until it finds one that decreases the high score and provides minimum or zero increase in the low utilization score; 
**scoring hosts for power-on/off:** as mentioned ealier, DPM considers hosts for evaluation in a sorted order, DPM currently prefers to keep larger servers powered-on, motivated by the fact that they provide more resource headroom and that they typically are more efficient at amortizing their idle power consumption, which for datacenter-class servers often exceeds 60% of their peak power, for servers of the same size, cost of evacuation is a secondary criteria for power-off consideration and randomization for wear-leveling is a secondary criteria for power-on consideration; as long as some host in the order satisfies the necessary conditions, it is selected and the corresponding op is applied to that host in the internal snapshot, the rest of the DPM evaluation is conducted with that host in the new state, hence DPM does not select a host that leads to maximum improvement in the scores, but uses this sorted list as the greedy order.
### DRS and DPM in the Field
when DRS was introduced in early 2006, vMotion was just beginning to gain widespread adoption, but customers were wary of automated migration of vms, in the early days, one req from customers was support for manual-move recommendations, a human admin would inspect the recommendations and apply the moves only if they made sense, some admins would run DRS in manual mode and if the same move was recommended over a substantial #DRS invocations, then the admin would apply the move, the use of DRS manual mode diminished overtime as the DRS algorithm became more mature and as admins became more comfortable with automated vm migration :as of vSphere5.0, the use of DRS manual mode is very low; 
the first version of DRS did not have cost-benefit analysis turned on, as the code was considered experimental, this lead to the problem that DRS could make recommendations that moved vms back and forth in response to frequently changing demand, in the very next release, cost-benefit feature was enabled by default, leading to higher-quality moves and fewer migrations; 
the DRS algorithm tries to get the biggest bang for its vMotion buck, i.e. to minimize total #moves needed for load-balancing, moving the largest most-active vms can have the highest impact on correcting imbalance, and hence DRS would favor such moves, while choosing such vms for vMotion in order to issue fewer moves seemed good in theory, some customers did not like this selection since their largest most-active vms were also their most important and perf-sensitive vms, and vMotioning those vms could adversely impact their perf during migration; 
to address this issue, DRS cost-benefit analysis was changed to take into account the impact of vMotion on the wkload which it had not done previously, as vSphere's vMotion continued to be improved, the cost modeling of that impact required updating as well, overtime we learned that the modeling aspects of the algorithm should be separated from parts of the algorithm that use the model, to ease maintenance of the algorithm code as technology changes, for example, we moved to having the algorithm consider the vMotion time, with the details of params related to that gen of vMotion technology handled in modeling-specific code; 
ealier versions of DRS did not support affinity bt vms and hosts and it was thought that affinities bt vms should be sufficient, we also wanted admins to think less about individual hosts and more about aggregate clusters, while vm2vm affinity was sufficient for most technical use cases, there were other requirements such as sw licensing that made admins want to isolate vms onto a set of hosts, admins started rolling out their own solutions to pinning vms to a set of hosts, such as adding dummy networks to vms and adding networks only to a subset of hosts, making the other hosts imcompatible; 
vm2host rules were added to DRS in vSphere4.1 and have been used for licensing and other use cases such as supporting availability zones, similarly, partners asked for DRS to support ESX agent vms, which did not need to be migrated off hosts entering maintenance or standby mode and which needed to be ready to serve on an active host before non-agent vms could placed or migrated to that host :DRS support for agent vms was introduced in vSphere5.0; 
another area where admins wanted improvement was in reporting and tracking reasons for DRS recommendations, we added reason descriptions to the recommendations and descriptive faults for situations that prevented recommendations from being issued, users also wanted to know how DRS evaluates the cluster as the cluster-wide entitlement depends on demands and resource settings of all vms and resource pools in the cluster, we added graphical displays of metrics to the DRS user interface, showing demand and entitlement for vms and resource pools, as well as cluster-wide metrics including overall imbalance and the percentage of a vm's entitled resources being delivered to it; 
when DPM was first introduced, the only technology it supported to bring hosts out of standby was WoL -wake on lan, this made some admins uncomfortable, since WoL packets were required to be sent over the vMotion network(on the same subnet) from another host in the cluster, hence requiring at least one connected powered-on host in the cluster to power the others on, subsequent versions of DPM supported IPMI, iLO, allowing vCenter to power-on a host by talking directly with its baseboard controller, improving adoption; 
admins were also uncomfortable when DPM shut down many hosts even when they were idle, based on this concern, multiple options were added to DPM to make it more conservative as well as take into account the intentions of admins regarding how many hosts can be powered down; 
a third area that was improved over several releases was the interoperability bt DPM and the VMware ha product, since DPM reduced #powered-on hosts ha could choose from for restarting failing-over vms, ha needed to convey to DPM the user's configured vm failover coverage, spare vms were introduced to express this info, and they prevented consolidation beyond their reservation requirements.
### Future Directions
**DRS:** one important area for improvement is to expand DRS scaling for cloud-scale resource mgmt, in vSphere5.0, the supported maximum DRS cluster size is 32 hosts and 3000 vms, which satisfies enterprise department deployments but falls short of cloud scale, elsewhere we proposed 3 techniques for increasing DRS scale including: hierarchical scaling(build a meta-load-balancer on top of DRS clusters), at scaling(build an overlay network on ESX hosts and run DRS as a decentralized algorithm), and statistical scaling(run DRS on a selected subset of hosts in the cluster), and we argued for statistical scaling as being the most robust; another future direction is to expand computing resources that DRS manages beyond cpu and mem, in the area of network resource mgmt, vSphere currently supports Network I/O control, which runs on ESX hosts to provide shares and limits for traffic types, but there is the additional opportunity for DRS to provide cross-host network mgmt, including vMotion to respect NIC bdwidth reservations, avoid NIC saturation, or locate communicating vms closer together; in the area of storage resource mgmt, vSphere recently introduced SIOC(vSphere4.1) to manage data-store io contention and Storage DRS(vSphere5.0) to place and redistribute vm disks using storage vMotion across a cluster of datastores for out-of-space avoidance and io load balancing, but there is the additional opportunity for DRS to support corss-host storage mgmt, including storage vMotion perhaps coupled with vMotion to allow vm power-on placement to respect io bdwidth reservations; a third direction is to support proactive op of DRS and DPM, currently DRS and DPM operate reactively, with the last 1hr's worth of behavior used in making its calculations of current demand more conservative, the reactive model works well in ensuring that recommendations made by DRS and DPM are worthwhile; however, when there is a sudden steep increase in demand, a reactive op can result in undesirably-high latency to obtain resources(such as time to power on a host or to reclaim mem resources from other vms) or difficulty in obtaining resources needed to respond while those resources are being highly contended, when such sudden steep increases in demand are predictable(such as an 8am spike in vm usage), proactive op can allow preparation for the demand spike to occur, to hide the latency of obtaining the resources and to avoid competing for resources with the demand spike itself; 
**DPM:** one future direction for DPM is to revise DPM's host sorting criteria for choosing host power-on/-off candidates, the host sorting currently does not use host power efficiency, which is becoming more practical as a wider variety of hosts report server power consumption in comparable industry-standard ways, interestingly, this has not been a frequent req from DPM users for 2 reasons :many use hosts from the same hw gen in a cluster so the hosts' efficiency is similar, and in clusters with mixed server gens, the newer servers are both larger and more efficient, so the existing sorting based on host size happens to coincide with host efficiency; DPM's sorting could also consider host temp :powering off hosts with measurably higher ambient temp reduces the usage of hosts that could be more likely to fail; another future direction is to revise DPM(and DRS) to understand host and cluster power caps such as those in HP Dynamic Power Capping, Intel Node Manager, Power caps are a mechanism to allow provisioning of power with respect to a specified peak value, which can limit effective MHz capacity of hosts governed by the cap, given knowledge of the power cap and the power efficiency of hosts, DPM and DRS can decide how to distribute vms on hosts with the goal of meeting vm's cpu resource needs in accordance with caps; another for investigation is to allow user to tune how much consolidation is appropriate for DPM, for some users, consolidation based on cpu and mem demand is too aggressive, and they would prefer that the utilization metric be based on values derived for vm config.<br \>
物理资源的自动化管理对于降低虚拟化环境的运营成本至关重要。有效的资源管理解决方案必须提供虚拟机之间的性能隔离，处理跨物理主机的资源碎片，并优化多种资源的调度，还必须高效利用底层硬件基础设施。本文介绍了两种此类管理解决方案的设计和实施：DRS 和 DPM，并重点介绍了客户在超过 5 年的生产部署过程中获得的一些关键经验教训；
VMware 的 DRS 分布式资源调度程序负责管理部署在主机集群中的一组虚拟机的物理资源分配，每个虚拟机都运行 VMware ESX 虚拟机管理程序。DRS 将虚拟机映射到主机并执行智能负载平衡，以提高性能并强制执行用户特定的策略和系统级约束。通过各种实验，并结合模拟结果，我们表明 DRS 显著提高了集群中运行的虚拟机的整体性能。DRS 还支持假设分析模式，可以评估工作负荷或集群配置变化的影响；
VMware 的 DPM（分布式电源管理）扩展了 DRS，使其能够通过将虚拟机整合到更少的主机上来降低功耗。当 CPU 和内存资源利用率较低时，DPM 会建议撤离并关闭主机；当需求增加时，或根据需要满足资源管理策略和约束时，它会建议适当地开启主机。我们的广泛评估表明，在需求大幅降低的时期，DPM 可以显著降低服务器功耗。
虚拟化的快速普及最初得益于服务器整合带来的显著成本节约。在单个物理主机上运行多个虚拟机可以提高硬件利用率，使管理员能够以更少的资源完成更多工作并降低资本支出。后来，更高级的虚拟机功能（例如克隆、基于模板的部署、检查点和正在运行的虚拟机的实时迁移）带来了更多 IT 基础设施，因此，创建和管理虚拟机变得更加容易；
在虚拟机中部署工作负载的便捷性导致虚拟机安装规模越来越大，此外，硬件技术趋势继续推动服务器性能更强大、核心数量更多、内存密度更高，从而导致整合率上升。然而，管理虚拟机的运营成本如今已占到使用虚拟化的数据中心总成本的很大一部分。理想情况下，管理虚拟化环境的复杂性也应该受益于整合，即使用主机数量而非虚拟机数量进行扩展。否则，管理虚拟基础设施将与管理物理环境一样困难（或者可以说，由于共享和争用），因为在物理环境中，每个应用程序都在其专用硬件上运行；
在实践中，我们观察到虚拟化环境中很大一部分运营成本与确定良好的虚拟机到主机映射以及决定何时使用 VMware 的实时迁移技术 vMotion 通过更改这些映射来重新平衡负载的固有复杂性有关，而多台物理主机上的资源碎片化以及需要同时平衡多种资源（包括 CPU 和内存）的利用率，则加剧了这一问题的难度；
我们还发现，管理员需要一种可靠的方法来指定资源管理策略。在整合环境中，总需求通常会超过物理资源的供应，管理员需要富有表现力的资源控制来对不同重要性的虚拟机进行优先级排序，以便隔离和控制竞争同一物理硬件的不同工作负载的性能；
我们设计 DRS 是为了帮助降低运行虚拟化数据中心的操作复杂性，DRS 可以将包含许多潜在异构主机的集群视为单个资源池进行管理，具体而言，DRS 提供了几项关键功能，包括：（i）集群抽象，用于将一组主机作为单个聚合实体进行管理，并结合其组成主机的处理和内存资源；（ii）强大的资源池抽象，支持在层次结构的每个级别上对虚拟机和虚拟机组进行分层资源管理；DRS 提供了一组丰富的控件，可以灵活地表达管理资源争用的策略；（iii）自动初始放置，在启动集群时将虚拟机分配给集群中的特定主机；（iv）根据虚拟机需求的动态波动以及物理基础设施的变化，在主机之间动态平衡 CPU 和内存资源；（v）自定义规则以限制虚拟机的放置，包括虚拟机之间以及虚拟机和主机之间的亲和性和反亲和性规则。 (vi) 用于硬件更改或软件升级的主机维护模式，用于评估集群中一台主机上运行的虚拟机到其他主机的情况；
我们在这里讨论 DRS 及其相关技术称之 DPM 的设计和实现，DRS 为主机集群提供自动化资源管理功能，DPM 通过自动化电源管理扩展了 DRS 的功能，在利用率低时关闭主机（使其处于待机状态），并在需要时重新打开主机；
我们的实验评估表明，DRS 能够满足资源管理目标，同时提高底层硬件资源的利用率。我们还表明，DPM 可以在包含大量主机和虚拟机的大型配置中显著节省功耗。这两项功能已作为 VMware 产品推出 5 年多，全球数千名客户已使用它们来管理数十万台主机和数百万台虚拟机。
### 资源模型
这里我们讨论 DRS 资源模型及其相关的资源控制。资源模型解释了资源管理解决方案的功能和目标。DRS 提供了强大的资源模型和一套灵活的资源控制，可以使用这些控制指定各种资源管理策略，从而为虚拟机组提供差异化​​的 QoS；
**基本资源控制**：VMware 的资源控制允许管理员和用户以绝对虚拟机分配或相对虚拟机重要性来表示分配。VMware ESX 虚拟机管理程序在单个主机级别提供处理器和内存分配的控制旋钮。DRS 为由多个 ESX 主机组成的分布式集群提供完全相同的控制，允许将它们作为单个实体进行管理。VMware 的基本资源控制包括：(i) 预留：指定某种资源的最小保证量，即即使该资源过度使用，也适用的下限。预留以绝对单位表示，例如 CPU 为 MHz，内存为 MB。VM 启动期间的准入控制可确保资源的预留总和不超过其总容量；
(ii) 限制：指定某种资源消耗的上限，即使该资源未得到充分使用，虚拟机也不会消耗超过其限制的资源，即使这会导致某些资源处于闲置状态。例如预留，限制以具体的绝对单位表示，例如 MHz 和 MB；
(iii) 份额：指定相对重要性，并以抽象数值表示。虚拟机有权消耗与其份额分配成比例的资源。当存在资源争用时，保证其最小资源份额等于其在总份额中所占的份额。在文献中，这种控制有时被称为权重；
在虚拟化环境中，使用预留和限制来表达分配的硬性界限的能力极其重要。如果没有这样的保证，虚拟机很容易遭受不可接受或不可预测的性能损失。要达到性能目标，需要诉诸诸如静态分区或过度配置物理硬件等粗暴的方法，从而抵消服务器整合的优势。这促使了预留和限制在 ESX 中的最初实现，以及将其纳入 DRS；尽管 DRS 专注于 CPU 和内存资源，但类似的 I/O 资源控制已通过研究原型验证。VMware 还提供了网络和存储带宽的份额和限制控制，以及用于虚拟磁盘放置和跨存储设备负载平衡的全新 StorageDRS 功能；
**资源池：**除了基本的每个虚拟机的资源控制之外，管理员和用户还可以为虚拟机组指定灵活的资源管理策略，这可以通过引入逻辑资源池的概念来实现 - 一个容器，它为一组虚拟机分配聚合资源，资源池是一个命名对象，具有与每个托管资源相关的设置 - 与虚拟机相同的熟悉的共享、预留和限制控制，准入控制在池级别执行：池子项的预留总和不得超过池自己的预留；资源池可以配置在灵活的分层组织中：每个池都有一个封闭的父池，子池可能是虚拟机或子池，资源池可用于在用户组或虚拟机之间划分或共享总容量，例如，管理员经常使用资源池层次结构来镜像人类的组织结构，资源池还为委派管理提供直接支持：管理员可以使用子池将资源批量分配给子管理员； [单击查看示例组织定义的资源池结构](./img/resource-pool-structure.png)，这里的资源首先分为 2 个组：业务组、测试组，业务组进一步细分为销售组，而测试组是虚拟机的扁平集合，每个池的单独分配既提供池隔离，也提供池内共享；例如，如果销售池中的某些虚拟机处于空闲状态，则其未使用的分配将优先重新分配给同一池中的其他虚拟机，任何剩余的备用分配优先流向 business 内的其他虚拟机（其父池），然后是 Org（其祖先）。请注意，在资源池层次结构中，份额仅对兄弟池有意义：每个池实际上定义了一个范围（类似于货币），份额值在此范围内解释；集群中一个独特的根资源池代表整个集群的物理容量，该容量在其子资源池之间分配，集群中的所有资源池和虚拟机都是根资源池的后代；**资源池分配**：资源池代表其子资源池可能消耗的资源分配总量，我们将计算其子池和虚拟机的预留、限制和份额的过程称为分配；分配以分层方式执行，将与父池关联的资源在其子池之间分配，分配从资源池层次结构的根开始，最终以其叶子上的虚拟机结束，DRS 使用生成的权利正确更新主机级设置，确保 ESX 虚拟机管理程序严格执行预留和限制等控制，DRS 还使用主机级权利不平衡来通知虚拟机迁移决策；在分配过程中，DRS 计算各个池和虚拟机的资源权利，分配计算结合了用户指定的资源控制设置以及每个虚拟机和聚合负载需求，因为池级资源分配反映了虚拟机需求，DRS 允许资源随着需求的变化在虚拟机之间流动，这使得虚拟机组之间可以方便地多路复用资源，而无需设置任何每个虚拟机的资源控制；为了维护资源池抽象，分配必须处理几种情况，例如，父池的总预留量可能大于其子池预留量的总和，同样，父池的限制可能小于其子池限制值的总和，最后，父池的份额需要按照每个子池的份额值比例分配给其子池，在所有这些情况下，都需要根据用户设置的预留、限制、份额值以及子池的实际运行时需求，在子池之间分配父池值；这些分配操作定义为：reservation-divvy、limit-divvy、share-divvy，以各自的资源控制命名，DRS 定期（默认每 5 分钟）执行这些分配操作，反映当前的虚拟机需求，DRS 还会根据资源分配设置的变化启动分配操作；分配算法分为两个阶段：（i）在第一个自下而上的阶段，分配从各个虚拟机（叶节点）的需求值开始，然后沿资源池树累积总需求；虚拟机需求由 ESX 虚拟机管理程序针对 CPU 和内存计算，虚拟机的 CPU 需求计算为其实际 CPU 消耗 CPU_used 加上 CPU_ready 的比例部分，它准备好执行但由于争用而排队的时间为：*CPU_demand = CPU_used + (CPU_run/(CPU_run+CPU_sleep))timesCPU_ready*；虚拟机的内存需求是通过跟踪虚拟机物理地址空间中一组随机选择的页面并计算在一定时间间隔内触及了多少个页面来计算的，例如，如果对于分配了 16GB 内存的虚拟机，有 40% 的采样页面被触及，则估计其活动内存为 0.4times16=6.4GB；一旦计算出每个虚拟机的需求值，就会将它们聚合到树中，需求值会更新为始终不小于预留值且不超过限制值，因此在每个节点上都会进行以下调整：*需求=MAX（需求，预留）需求=MIN（需求，限制）*； (ii)第二个分配阶段以自上而下的方式进行，每个父级的预留和限制值用于计算其子级的相应资源设置，以满足以下约束，包括：（i）子级分配与其份额成比例，（ii）每个子级至少分配自己的预留，（iii）任何子级分配的数量都不能超过其自身的限制，为了合并需求信息，如果子级需求总和大于分配的数量（预留或限制），我们用子级的需求值替换子级的限制值：*limit=MIN(demand, limit)*，这允许父级的预留和限制设置根据其实际需求在其子级之间流动；从概念上讲，树中单个级别的分配算法可以这样实现：首先为每个子节点提供预留，然后根据要分配的内容，将额外的预留或限制以小块的形式分配给子节点，方法是将一个块分配给当前分配/份额最小的子节点，如果子节点的当前分配达到其限制，则可以将该子节点从进一步考虑中剔除，并限制在该限制内；份额分配是通过在其子女之间按每个子女的份额比例分配划分父节点的份额来实现的，需求在股份分配操作中不起任何作用；我们用一个简单的两级资源池树的例子来说明分配操作，[点击查看资源池树](./img/resource-pool-tree.png) 在根节点上，CPU预留 R=10GHz，限制 L=20GHz，共享 S=1000，共享无单位，其他两个设置以绝对 CPU 单位（GHz）为单位，用户设置的资源控制值用 U 表示，最终分配的值用 D 表示，虽然这个例子只显示了 CPU 的分配，但对内存也执行了类似的计算，在图 2 中，根下有 2 个资源池，RP1、RP2，每个池有 2 个子虚拟机，首先，自下而上的分配阶段从单个虚拟机需求开始，沿树向上汇总需求，接下来，自上而下的分配阶段从根节点开始，递归地进行预留和限制分配，直到到达叶节点；根处的 10GHz 预留大于其子级预留的 5GHz 总和，因此预留基于份额进行分配，同时满足所有分配约束，在这种情况下，基于份额的 8GHz 和 2GHz 分配满足两个资源池的预留和限制值，接下来，RP1 处的 8GHz 预留在 vm1、vm2 之间分配，由于 8GHz 小于 vm1 和 vm2 的 10GHz 总需求，我们分别用 vm1 和 vm2 的需求值 3GHz 和 7GHz 替换 vm1 和 vm2 的限制值，结果，即使这两个虚拟机的份额相等，分配的预留也是 3GHz 和 5GHz（而不是每个 4GHz），这说明了在分配父级预留时如何考虑需求；请注意，由于子节点的需求总和小于父节点的限制，因此在分配限制值时，虚拟机需求不作为上限，因此，在分配中使用子节点的实际限制值，份额只是按子节点份额的比例分配。虽然我们并未完成此资源池树中的每个计算，但可以验证预留值和限制值都可以从根节点开始自上而下地进行分配，同时考虑用户设置的值和当前需求。
### DRS 概述和设计
DRS 旨在准确执行资源管理策略，根据上一节中描述的资源模型为每个虚拟机提供物理资源。[点击查看 DRS 作为 vCenter Server 集中管理软件的一部分运行](./img/drs-runs-part-of-the-vcenter-server-centralized-management-software.png)，它管理 ESX 主机集群及其上运行的虚拟机的资源。更具体地说，DRS 执行 4 个关键的资源管理操作，包括：(i) 根据预留、限制和份额值以及所有相关虚拟机和资源池的运行时需求，计算每个虚拟机有权使用的资源量；(ii) 在虚拟机的资源需求可能随时间变化的动态环境中，建议并执行已启动虚拟机的迁移，以平衡主机之间的负载；(iii) 可选择通过调用 DPM 来节省电量；(iv) 执行虚拟机在主机上的初始放置，以便用户无需手动进行放置决策；
DRS 负载均衡会定期（默认每 5 分钟）进行请求，以满足集群约束并确保交付授权资源。当用户更改集群配置（例如向集群添加主机或请求主机进入维护模式）时，也会按需调用。调用 DRS 时，它会执行上面列出的前 3 个资源管理操作，并执行纠正集群约束违规的操作，例如，约束纠正会从用户请求进入维护或待机模式的主机中撤离虚拟机；
DRS 初始放置会在虚拟机启动、从挂起状态恢复或手动迁移到集群时，将虚拟机分配给集群中的主机。DRS 初始放置与 DRS 负载均衡共享代码，以确保放置建议符合约束和资源授权；
**负载平衡：**我们首先研究 DRS 负载平衡指标和算法，然后更详细地考虑 DRS 如何分析可能的负载平衡举措，包括其对解决不平衡的影响、成本和收益，以及它们与待处理和依赖操作的相互作用，包括：（i）负载平衡指标：是动态授权，它与更常用的主机利用率指标不同，因为它反映了根据虚拟机的需求和重要性进行的资源交付，动态授权是根据集群整体容量、资源控制以及每个虚拟机对 CPU 和内存资源的实际需求计算的；虚拟机对资源的授权高于其预留并低于其限制：实际值取决于集群容量和总需求；动态授权相当于当集群中所有虚拟机的需求都可以得到满足时的需求，否则，它是一个缩小的需求值，其缩放取决于集群容量、其他虚拟机的需求、虚拟机在资源池层次结构中的位置及其份额、预留和限制；动态授权是通过在资源池层次树上运行分配算法来计算的，对于授权计算，我们使用根处的集群容量作为分配的数量，这是分别针对 CPU 和内存资源完成的； DRS 目前使用规范化授权作为其核心的每台主机负载指标，反映主机容量以及正在运行的虚拟机的授权，对于主机 *h*，规范化授权 *Nb* 定义为在 *h* 上运行的所有虚拟机的每个虚拟机授权 *Ei* 的总和，除以虚拟机可用的主机容量 *Ch*：*Nh=(sum_Ei)/Ch*，如果 *Nh<=1* 则主机 *h* 被认为没有足够的资源来满足其所有虚拟机的授权，因此，与在规范化授权不高于 1 的主机上运行的虚拟机相比，该主机上的虚拟机将受到不公平的对待；在计算每个主机的 *Nh* 之后，DRS 会计算集群范围的不平衡 *Ic*，其定义为所有 *Nh* 值的标准差。集群范围的不平衡使用加权和来考虑 CPU 和内存的不平衡，其中权重取决于资源争用情况。如果内存争用非常激烈，即其在任何主机上的最大规范化权益大于 1，则其权重大于 CPU；如果 CPU 争用非常激烈，则其权重大于内存；如果两种资源争用都不激烈，则使用相同的权重。当一种资源的权重大于另一种资源时，使用通过实验得出的 3:1 的比例；
(ii) [负载均衡算法，点击查看](./img/drs-load-balancing-algorithm.png)：此处对 DRS 算法的概述已大大简化，仅关注其核心负载均衡指标，实际的负载均衡算法会考虑许多其他因素，包括移动对不平衡的影响以及每次移动的风险调整收益；DRS 负载均衡算法使用贪婪爬山技术，这种方法与试图找到最佳目标平衡的详尽离线方法相反，是由实际考虑驱动的，用于改善负载均衡的实时迁移操作是有成本的，并且虚拟机需求会随着时间而变化，因此针对特定的动态情况进行优化是不值得的； DRS 的目标也是最小化集群范围内的不平衡 *Ic*，其方式是评估所有可能的单虚拟机迁移（实践中许多迁移会被快速过滤），并选择能够最大程度降低 *Ic* 的迁移。所选迁移将应用于算法当前的内部集群状态，以反映迁移完成时的状态。重复此迁移选择步骤，直到没有其他有益的迁移、有足够的迁移可供本次迁移，或者集群不平衡达到或低于 DRS 管理员指定的阈值 *T*。算法完成后，执行引擎将执行建议的迁移，并可选择是否需要用户批准；
(iii) 最小优度：如果迁移在改善表示不平衡的标准差值方面没有产生足够的效益，DRS 负载均衡会拒绝该迁移。用于此过滤的阈值是根据主机数和虚拟机数动态计算的。当不平衡非常严重时，阈值会显著降低，用于纠正不平衡的迁移会被正常阈值过滤，以便可以使用许多影响较小的迁移来纠正高度不平衡；
(iv) 成本效益分析：DRS 用于过滤不稳定移动的主要检查是成本效益分析，它通过对 vMotion 成本以及目标主机上其他虚拟机的成本进行建模来考虑移动中涉及的各种成本，目标主机上将有额外的虚拟机竞争资源，收益计算为源上的虚拟机和迁移虚拟机将从移动中受益多少，成本效益分析还通过预测源和目标上的 wkload 可能如何变化以及当 wkload 发生变化时此移动是否仍然有意义来考虑移动的风险；成本是通过估计此虚拟机迁移所需的时间来建模的，迁移时间主要取决于虚拟机的内存大小以及虚拟机在迁移过程中修改其页面的活跃程度，如果虚拟机在 vMotion 期间频繁弄脏其页面，则必须将它们多次复制到目标主机； DRS 根据每个虚拟机和主机的迁移时间历史记录来跟踪传输速率，根据传输速率和当前内存大小，计算单次复制所需的时间，在此期间，成本计算为迁移虚拟机在目标上存在所需的资源，vMotion 成本以资源（MHz 或 MB）为单位加时；vMotion 过程本身会在源主机和目标主机上消耗一些资源，这也会作为成本添加，然后 DRS 计算虚拟机内部的工作负载会因迁移而承受多大程度的损失，这是通过测量虚拟机写入其内存页面的活跃程度以及每次传输所花费的时间来计算的，DRS 通过测量虚拟机在过去的 vMotion 期间必须被 CPU 调度程序取消调度的次数以及当时虚拟机的活跃程度来近似计算，计算由于迁移期间修改内存的成本增加而导致的性能下降，这个值用于推断虚拟机将承受多少损失，同时考虑到它的当前消耗并将其添加到成本中，这个成本也是以资源加班的单位来衡量的，比如 CPU 秒； vMotion 的好处也是通过资源超时来衡量的，好处是通过计算候选虚拟机在源上被削减的需求量与目标上预计满足的需求量的增加来计算的，好处还包括虚拟机移出后将满足的源上其他虚拟机的未满足需求量，为了表示可能的需求变化，根据所有这些虚拟机的工作负载来预测源和目标上所有虚拟机的需求，预测一个稳定的时间，在此之后我们假设虚拟机的工作负载将更改为此移动的最差配置，考虑到最近的需求历史（默认情况下，为前一个小时），使用这个最坏情况的值是为了使算法在推荐迁移时更加保守；对于源主机，最坏情况的计算方式是，在稳定时间之后，源主机上虚拟机的工作负载达到前一小时内观察到的最小需求。对于目标主机，最坏情况的计算方式是，目标主机上（包括正在迁移的虚拟机）的虚拟机需求达到过去一小时内观察到的最大值。通过假设这种最坏情况，依赖于工作负载不稳定的虚拟机的迁移将被过滤掉；
(v) 待处理建议：DRS 会考虑正在进行的建议和任何尚未启动的待处理建议，因此不会多次纠正相同的约束违规或不平衡情况。当前正在迁移的虚拟机将被视为同时存在于源主机和目标主机上，不会生成与待处理建议相冲突的新建议；
(vi) 移动依赖性：当 DRS 生成一系列移动操作时，该序列中的某些虚拟机迁移操作可能依赖于先前从该主机迁移虚拟机所释放的容量。例如，在约束违规纠正步骤中，DRS 可能会生成将虚拟机 x 从主机 A 移出以纠正规则违规的建议；然后在负载平衡步骤中，它可能会生成将虚拟机 y 移动到主机 A 的建议，该建议依赖于虚拟机 x 迁移所释放的资源。对于此类序列，只有在第一次移动操作成功后才能执行第二次移动操作。DRS 可以在其输出建议中指定依赖关系，并且 DRS 建议执行引擎会尊重这些依赖关系。DRS 不会发出依赖关系无法满足的移动序列；
(vii) 主机级资源设置：为了保持集群是一个具有其组成主机总容量的大型主机的假象，DRS 将用户指定的资源池层次结构分解为具有适当主机级资源池设置的按主机资源池层次结构，因此，当虚拟机在主机上运行时，每个 ESX 主机上的本地调度程序会根据虚拟机资源设置以及 DRS 提供给主机的主机级资源池树，公平地为虚拟机分配资源；在每次平衡调用开始时，DRS 都会运行预留分配和限制分配算法，以捕获虚拟机需求随时间变化带来的影响，并根据更新后的分配结果与算法上次生成的结果之间的差异，DRS 会根据该主机上运行的虚拟机，生成调整集群中主机资源树和设置的建议；
**虚拟机初始放置：**调用该函数来执行虚拟机启动的准入控制和主机选择，恢复挂起的虚拟机，或手动将正在运行的虚拟机迁移到集群中，可以要求 DRS 放置单个虚拟机，它会尝试为其生成一个或多个备选主机放置建议，或者可以要求它放置一组虚拟机，它会尝试为每个虚拟机生成一个由放置操作组成的单一协调建议，后者比一系列单独的虚拟机放置更高效、更有效地处理多个虚拟机的放置，因为它构建了集群的单一表示来放置虚拟机集，并且因为该算法可以对虚拟机的放置进行排序以方便装箱，对于 DRS 无法放置的任何虚拟机，都会发出特定错误；在放置过程中，DRS 无法估计虚拟机当前的 CPU 和内存需求，它保守地假设所放置的虚拟机将消耗其最大可能的负载，即其内存需求将与其配置的内存大小相匹配，并且其 CPU 需求将使得其每个虚拟 CPU（vCPU）将消耗一个物理核心，DRS 放置代码利用 DRS 负载平衡代码来评估每个可能放置的相对优劣；但是，放置与负载平衡的不同之处在于，放置会考虑可能需要的任何先决条件移动，如果在群集中任何已打开电源的主机上都无法放置虚拟机而不会违反约束，则 DRS 接下来会考虑将虚拟机放置在每个主机上，同时尝试通过主机上其他虚拟机的先决条件移动来纠正由此产生的约束违规，如果无法在已打开电源的主机上进行这样的放置，则 DRS 会考虑通过 DPM 为虚拟机打开备用主机； DRS 还负责容错虚拟机的初始放置。容错虚拟机由主虚拟机和辅助虚拟机组成，辅助虚拟机是主虚拟机的副本，以锁步运行以允许立即进行故障转移。主虚拟机和辅助虚拟机必须放置在兼容 vMotion 的不同主机上，因为考虑所有可能的主机对的计算成本非常高。DRS 会将主虚拟机放置在性能最佳的主机上，然后从与主虚拟机兼容 vMotion 的主机子集中，选择最佳主机作为辅助主机。如果找不到辅助主机，则会考虑将次优主机作为主虚拟机并再次尝试。此过程重复进行，直到找到主虚拟机和辅助虚拟机都可用的主机，或者没有其他主机可供主虚拟机考虑；
**约束：**DRS 尊重的基本约束是 vm2host 兼容性，即主机满足 vm 的执行要求的能力，vm2host 兼容性信息从 vCenter mgmt sw 的兼容性检查模块传递给 DRS，并且 DRS 在负载平衡和 vm 放置期间尊重此约束；此外，DRS 支持强制执行一组约束来处理各种用例，例如为性能而将虚拟机共置，将虚拟机放置在单独的硬件上以处理许可问题，它还处理用户请求进入维护或待机模式的主机的撤离，为故障转移保留备用资源，以及为其他虚拟机提供服务的特殊虚拟机的作用，DRS 在虚拟机初始放置期间尊重约束，并在负载平衡之前运行一次传递以纠正违反 DRS 强制约束的情况，报告任何无法纠正的约束的错误，我们接下来讨论几个约束及其具体用例，包括：（i）亲和性规则：DRS 支持 vm2vm 或 vm2host 规则，用于各种常见的业务场景，vm2vm 反亲和性规则定义了一组要保留在单独主机上的虚拟机，这些规则通常用于可用性并且是强制性的，即 DRS 不会提出任何违反它们的建议，例如避免由于运行 2 个虚拟机而导致的单点故障在同一主机上，vm2vm 亲和性规则定义了一组要保留在同一主机上的虚拟机，用于增强通信虚拟机的性能，因为主机底层的 vm2vm 网络经过优化，可以执行内存中数据包传输，而无需使用 NIC hw，这些规则对于负载平衡是强制性的，但如果在开机期间需要放置虚拟机，DRS 会违反这些规则，vm2vm 规则违规会在负载平衡运行的初始阶段得到纠正，并且在负载平衡期间保持纠正状态，对于 vm2vm 反亲和性，引入违规的潜在平衡移动会被过滤掉：对于 vm2vm 亲和性，潜在的平衡移动是通过将主机上的每组亲和虚拟机视为一个大型虚拟机来形成的； vm2host 规则定义一组与一组主机具有亲和性或反亲和性的虚拟机，这些规则可以指定为强制或首选，后者意味着除非它们产生额外的约束违规或在不导致主机过度利用的情况下无法遵守，否则将强制执行这些规则，vm2host 规则由于多种原因很有用，强制 vm2host 亲和性规则通常用于强制执行许可，将需要基于主机的 sw 许可证的虚拟机与具有该许可证的主机关联起来，首选 vm2host 亲和性或反亲和性规则通常用于管理可用性和/或站点局部性；强制 vm2host 规则在传递给 DRS 的 vm2host 兼容性信息中表示，因此被视为基本约束，首选 vm2host 规则表示为替代 vm2host 兼容性信息，DRS 通过运行假设检验来处理首选 vm2host 规则，其中首选规则被视为强制规则，如果生成的集群状态没有约束违规或过度使用的主机，则接受检验的结果，否则，DRS 将重试假设检验，并丢弃首选规则并接受后者结果，如果约束违规或过度利用的主机较少，否则接受前者结果；
（ii）高可用性：vSphere HA - 高可用性是一种集群服务，可处理主机故障，并在剩余健康成本的情况下重新启动故障虚拟机。HA 作为分散服务在 ESX 主机上运行，​​并通过心跳机制跟踪活跃度信息。DRS 通过两种方式支持 HA 功能：根据 HA 策略保留已启动的空闲资源以供虚拟机重启使用，以及在 HA 重启无法找到足够资源时对集群中的资源进行碎片整理；为了让 DRS 保留足够的已开启的空闲资源用于虚拟机重启，HA 需要根据 HA 策略设置向 DRS 表达所需的资源，HA 支持 3 个月的时间，用户可以在此期间表达他们需要为故障转移保留的资源，包括：（i）他们愿意容忍的主机故障数，（ii）他们愿意保留的备用资源百分比，（iii）指定特定主机作为故障转移主机，除了故障转移期间外，这些主机不用于运行虚拟机；HA 使用两种约束向 DRS 表达这些策略的含义：要保持开启状态的 CPU 和内存资源的最小数量以及在故障转移期间要保留的未碎片化资源块的大小和位置，后者在 DRS 中表示为备用虚拟机，其作用类似于具有预留的特殊虚拟机，用于确保 DRS 维护备用资源插槽，虚拟机可以故障转移到这些插槽中，并且其 vm2host 兼容性信息用于在适当的主机上创建这些插槽；备用虚拟机也用于在异常情况下，由于发生故障的主机数量超过高可用性策略配置的数量，导致故障重启所需的资源超出预期。对于任何高可用性策略无法重启的虚拟机，将使用代表其配置的备用虚拟机调用 DRS，以便可以建议主机启动和迁移以提供所需的资源；
（iii）ESX 代理虚拟机：ESX 主机为虚拟机提供的某些服务（例如 vShield）封装在虚拟机中，这些虚拟机称为 ESX 代理虚拟机。在任何非代理虚拟机启动或迁移到该主机之前，需要先启动代理虚拟机并准备好提供服务；如果主机处于维护或待机模式，则在非代理虚拟机从主机中撤离后，需要关闭代理虚拟机；为了支持代理虚拟机，DRS 尊重其作为主机平台一部分的角色，这具有许多含义，DRS 算法不会生成将非代理虚拟机迁移或放置在未配置所需代理虚拟机的主机上的建议；在已配置代理虚拟机的主机上，即使代理虚拟机未处于 ready2serve 状态，DRS 算法也会尊重代理虚拟机的预留资源；DRS 执行引擎了解非代理虚拟机需要等待所需代理虚拟机启动并准备好在目标主机上提供服务；DRS 负载平衡代码了解当主机进入维护或待机模式时，代理虚拟机不需要为其生成撤离建议：在非代理虚拟机撤离后，代理虚拟机框架会自动关闭代理虚拟机。
### DPM 概述和设计
DPM 是 DRS 的一项功能，它通过动态调整集群容量以满足负载需求，从而适时节省电量，同时满足所有集群约束。当集群中剩余足够的 CPU 和内存资源时，DPM 建议撤离虚拟机并关闭 ESX 主机。当 CPU 或内存资源利用率适当增加，或需要额外的主机资源来满足集群约束时，DPM 建议重新开启 ESX 主机。由于后者的原因，DRS 本身会建议开启主机。DPM 作为 DRS 平衡调用的一部分运行，作为可选的最终阶段；
请注意，除了 DPM 之外，每个 ESX 主机还执行 HPM 主机电源管理，该管理使用 ESX 主机上的 ACPI P 状态和 C 状态来降低主机功耗，同时满足虚拟机需求。HPM 与 DPM 协同工作，减少正在运行的主机数量，并降低这些主机的功耗；
DPM 将主机开关机需求的检测与主机的选择分离，这使得主机选择与主机当前的利用率无关，这意味着 DPM 的主机关机选择不依赖于查找当前 CPU 和内存利用率都较低的主机：DPM 可以迁移虚拟机以利用当前在同一主机上未体现的低利用率资源，更重要的是，这种分离意味着 DPM 的主机关机选择可以考虑比当前利用率更长期的关键标准，例如处理突发需求的余量、电源效率或温度；例如，主机 A 当前的 CPU 和内存利用率可能低于主机 B，但主机 B 可能是更适合关机的候选主机，因为它小于主机 A，因此其处理虚拟机突发需求的能力不如主机 A，或者其能效不如主机 A，或者其位于数据中心比主机 A 更热的区域；
**利用率评估**：为了确定当前已启动的集群容量是否适合正在运行的虚拟机的资源需求，DPM 会评估集群中已启动主机的 CPU 和内存资源利用率（虚拟机需求与容量之比）。由于需求是假设主机上没有争用的情况下对虚拟机资源消耗的估计值，因此利用率可能超过 100%。如资源池分配部分所述，虚拟机的 CPU 需求包括其 CPU 使用率和部分就绪时间（如果有）；虚拟机的内存需求是其工作集大小的估计值； DPM 考虑较长时间段内的需求，并使用该时间段内的平均需求加上 2 个标准差来计算利用率，这使得 DPM 在主机关机方面比较保守，而客户更喜欢这种偏见，为了在需求增加时相对较快地启动主机，在需求减少时相对较慢地关闭主机，主机关机的时间段是最后 40 分钟，主机开机的时间段是最后 5 分钟；利用率的特点是 45-81% 的范围（即 63+-18），81% 以上的利用率被认为高，45% 以下的利用率被认为低，DPM 使用一种基于每个主机结合 CPU 和内存利用率的度量来评估集群利用率，这样某些主机上的较高利用率不会被其他主机上的较低利用率所抵消，这样做是因为由于限制，DRS 不能总是均衡虚拟机负载访问主机；使用此指标，DPM 可以在集群中已启动主机的整体利用率不高时考虑启动主机来解决集群中少数主机（例如获得运行 Oracle VM 许可的主机）的高利用率问题，也可以在集群中已启动主机的整体利用率不低时考虑关闭主机来解决集群中少数主机（例如未连接到流行共享存储设备的主机）的低利用率问题。请注意，DPM 在完整的 DRS 负载平衡过程生成的集群表示上执行； DPM 会计算集群范围内每种资源的高分和低分，考虑在开机需求期间利用率高于高利用率阈值（默认值 81%）的主机：*HighUtilizationScore=sqrt^{sum_(DistanceAboveThreshold^2)}*，考虑在关机需求期间利用率低于低利用率阈值（默认值 45%）的主机：*LowUtilizationScore=sqrt^{sum_(DistanceBelowThreshold^2)}*，如果 CPU 或内存的高分大于零，DPM 会考虑生成主机开机建议；否则，如果 CPU 和内存的低分都大于零（可能位于不同的主机上），DPM 会考虑生成主机关机建议。此评分背后的关键直觉是，我们希望识别出一部分主机可能具有非常高或非常低的利用率的情况，可以通过分别建议开机和关机操作来采取行动；
**主机关机建议：** 如果满足两个条件，DPM 会在当前调用中考虑关机：CPU 和内存的低分均为非零，且高分均为零。为了评估各种替代方案，DPM 会按排序顺序考虑每个候选主机。如果候选主机关机通过了下面描述的集群利用率和成本效益评估，则将其关机的建议及其先决条件虚拟机撤离建议将添加到此次 DRS 调用将发出的建议列表中。然后更新集群的内部表示以反映此建议的效果。DPM 会继续考虑关机，直到集群利用率分数不再显示低 CPU 和内存利用率，或者没有更多候选主机可供考虑；对于候选主机，DPM 以假设模式运行 DRS 来纠正约束并对集群进行负载平衡，假设主机正在进入待机状态，如果假设 DRS 可以完全撤离主机，DPM 将使用上一节中给出的利用率分数评估最终的集群状态，并将分数与考虑主机关闭之前的集群状态进行比较，如果集群低利用率分数降低，而集群高利用率分数没有增加，则接下来对候选主机关闭进行 DPM 关闭成本效益分析；DPM 成本效益分析考虑了关闭电源的风险调整成本和收益，成本包括与从关闭电源的主机上迁移虚拟机相关的 CPU 和内存资源，以及与将来重新启动主机时重新填充主机相关的相应资源，成本还包括与满足虚拟机需求对应的风险调整资源缺口，如果在主机进入待机、关机或重启状态时，需要主机的 CPU 和/或内存资源来满足该需求，风险调整后的资源缺口计算方法如下：假设当前需求持续达到该需求的预期稳定时间，并根据近期历史稳定时间进行分析，在此之后需求飙升至最坏情况值，该值定义为过去 60 分钟内需求的平均值加上 3 个标准差；收益是在主机预计停机时间内节省的电量，转换为 CPU 和内存资源货币（主机的 MHz 或 MB 乘以待机时间）并与成本进行权衡，默认情况下，收益必须超过成本的 40 倍，关机才能通过 DPM 的成本效益过滤器：此值是根据实验选择的，以符合客户不影响性能以节省电量的偏好；
**主机开机建议：** 如果任何已开机主机的 CPU 或内存利用率较高，DPM 会评估主机开机，DPM 会按排序顺序考虑每个候选主机，如果候选主机开机通过了下面描述的集群利用率评估，则将其开机的建议将添加到 DRS 生成的建议列表中，集群的内部表示也会更新以反映此建议的效果，DPM 会继续考虑主机开机，直到集群利用率分数不再显示较高的 CPU 或内存利用率，或者没有更多候选主机可供考虑；对于候选主机，DPM 会以假设模式运行 DRS，假设该主机已开机，以纠正约束并对集群进行负载平衡。如果主机开机降低了高利用率分数，则评估该改进的稳定性。改进的稳定性包括检查 DRS 在该主机存在的情况下执行的贪婪重新平衡是否由于该主机的添加而变得更好。检查方法是：对同一候选主机执行假设关机，并检查考虑其开机后获得的高利用率分数的改进是否不再存在。如果主机开机效益稳定，则选择该主机进行开机。对于解决集群中所有剩余高利用率问题所需的最后一个主机，将执行一些额外的评估。在这种情况下，DPM 会评估多个主机，直到找到一个既能降低高分数，又能使低利用率分数增幅最小或为零的主机；
**对主机进行开机/关机评分**：如前所述，DPM 会按排序顺序考虑要评估的主机。目前，DPM 倾向于让大型服务器保持开机状态，因为它们可以提供更多的资源空间，并且通常能够更有效地摊销其闲置功耗（对于数据中心级服务器，闲置功耗通常超过其峰值功耗的 60%）。对于同等规模的服务器，撤离成本是关机考虑的次要标准，而磨损均衡的随机化是开机考虑的次要标准；只要顺序中的某个主机满足必要条件，就会被选中并在内部快照中将相应的操作应用于该主机，DPM 的其余评估将在该主机的新状态下进行，因此，DPM 不会选择能够最大程度提高分数的主机，而是使用此排序列表作为贪婪顺序。
### DRS 和 DPM 的实际应用
DRS 于 2006 年初推出，当时 vMotion 才刚刚开始被广泛采用，但客户对虚拟机的自动迁移持谨慎态度。早期，客户的一个要求是支持手动迁移建议，人工管理员会检查建议，并仅在合理的情况下应用迁移。有些管理员会在手动模式下运行 DRS，如果在大量 #DRS 调用中都建议了相同的迁移，则管理员会应用该迁移。随着 DRS 算法的日益成熟以及管理员对虚拟机自动迁移的日益熟悉，DRS 手动模式的使用逐渐减少：从 vSphere 5.0 开始，DRS 手动模式的使用率非常低；
DRS 的第一个版本没有启用成本效益分析，因为该代码被认为是实验性的，这导致了一个问题：DRS 可能会根据频繁变化的需求来回移动虚拟机，并提出建议。在下一个版本中，成本效益功能默认启用，从而提高了移动质量并减少了迁移次数；DRS 算法试图从 vMotion 中获得最大的收益，即为了最大限度地减少负载平衡所需的总移动次数，移动最大的、最活跃的虚拟机对纠正不平衡的影响最大，因此 DRS 会倾向于这样的移动。虽然选择这样的虚拟机进行 vMotion 以减少移动次数在理论上似乎是好的，但一些客户不喜欢这种选择，因为他们最大的、最活跃的虚拟机也是他们最重要且对性能最敏感的虚拟机，而对这些虚拟机进行 vMotion 迁移可能会在迁移过程中对其性能产生不利影响；
为了解决这个问题，DRS 成本效益分析进行了修改，将 vMotion 对负载的影响纳入考量，这是之前没有考虑过的。随着 vSphere vMotion 的不断改进，该影响的成本建模也需要更新。随着时间的推移，我们了解到算法的建模部分应该与使用该模型的算法部分分开，以便在技术变化时简化算法代码的维护。例如，我们开始让算法考虑 vMotion 时间，并将与该代 vMotion 技术相关的参数细节在特定于建模的代码中处理；
早期版本的 DRS 不支持虚拟机和主机之间的亲和性，人们认为虚拟机之间的亲和性就足够了，我们还希望管理员少考虑单个主机，多考虑聚合集群。虽然虚拟机到虚拟机的亲和性对于大多数技术用例来说已经足够了，但还有其他要求，例如软件许可，这使得管理员想要将虚拟机隔离到一组主机上，管理员开始推出自己的解决方案来将虚拟机固定到一组主机上，例如向虚拟机添加虚拟网络，并仅向一部分主机添加网络，从而使其他主机不兼容；
在 vSphere 4.1 中，DRS 中添加了 vm2host 规则，用于许可和其他用例，例如支持可用区。同样，合作伙伴要求 DRS 支持 ESX 代理虚拟机，这些虚拟机无需从进入维护或待机模式的主机上迁移，并且需要先准备好在活动主机上提供服务，然后才能将非代理虚拟机放置或迁移到该主机：vSphere 5.0 中引入了对代理虚拟机的支持；
管理员希望改进的另一个领域是报告和跟踪 DRS 建议的原因，我们为建议添加了原因描述，并为阻止发布建议的情况添加了描述性故障。用户还希望了解 DRS 如何评估集群，因为集群范围的授权取决于集群中所有虚拟机和资源池的需求和资源设置，我们在 DRS 用户界面中添加了指标的图形显示，显示虚拟机和资源池的需求和授权，以及集群范围的指标，包括总体不平衡和虚拟机授权资源交付给它的百分比；
DPM 首次推出时，其支持的唯一使主机退出待机状态的技术是 WoL（局域网唤醒），这让一些管理员感到不适应，因为 WoL 数据包需要通过 vMotion 网络（位于同一子网）从集群中的另一台主机发送，因此需要集群中至少一台已连接的已启动主机才能启动其他主机。DPM 的后续版本支持 IPMI 和 iLO，允许 vCenter 通过直接与主机基板控制器通信来启动主机，从而提高了采用率；
DPM 关闭许多空闲主机也让管理员感到不适应。基于这种担忧，DPM 添加了多个选项，使其更加保守，并考虑管理员关于可以关闭多少台主机的意图；
在多个版本中得到改进的第三个领域是 DPM 和 VMware ha 产品的互操作性，因为 DPM 减少了 ha 可以选择用于重新启动故障转移 vm 的已启动主机数，ha 需要向 DPM 传达用户配置的 vm 故障转移覆盖范围，因此引入了备用 vm 来表达此信息，并且它们阻止了超出其预留要求的整合。
### 未来方向
**DRS：**一个重要的改进领域是扩展 DRS 的扩展性，以实现云规模的资源管理。在 vSphere 5.0 中，支持的最大 DRS 集群规模为 32 台主机和 3000 台虚拟机，这可以满足企业部门部署的需求，但与云规模相比存在差距。此外，我们提出了三种提升 DRS 规模的技术，包括：分层扩展（在 DRS 集群之上构建元负载均衡器）、平面扩展（在 ESX 主机上构建覆盖网络，并以分散式算法运行 DRS）和统计扩展（在集群中选定的主机子集上运行 DRS），我们认为统计扩展是最稳健的；未来的另一个方向是将 DRS 管理的计算资源扩展到 CPU 和内存之外，在网络资源管理方面，vSphere 目前支持网络 I/O 控制，该控制在 ESX 主机上运行，​​为流量类型提供共享和限制，但 DRS 还可以提供跨主机网络管理，包括使用 vMotion 来尊重网卡带宽预留，避免网卡饱和，或将通信虚拟机定位得更紧密；在存储资源管理方面，vSphere 最近引入了 SIOC（vSphere4.1）来管理数据存储 io 争用，并引入了存储 DRS（vSphere5.0）来放置和重新分配虚拟机磁盘通过跨数据存储集群使用存储器 vMotion，以避免空间不足和实现 io 负载平衡，但 DRS 还可以支持跨主机存储管理，包括存储 vMotion 可能与 vMotion 结合使用，以允许虚拟机启动时放置尊重 io 带宽预留；第三个方向是支持 DRS 和 DPM 的主动操作，目前 DRS 和 DPM 是被动操作，使用过去 1 小时的行为来使其对当前需求的计算更加保守，被动模型可以很好地确保 DRS 和 DPM 提出的建议是有价值的；然而，当需求突然急剧增加时，被动操作可能会导致获取资源的延迟过高（例如启动主机或从其他虚拟机回收内存资源的时间），或者在资源竞争激烈的情况下难以获取响应所需的资源。当这种需求的突然急剧增加是可以预测的（例如早上 8 点虚拟机使用量激增）时，主动操作可以为需求高峰的发生做好准备，隐藏获取资源的延迟，并避免与需求高峰本身争夺资源；
**DPM：**DPM 未来的一个方向是修改 DPM 的主机排序标准，以选择主机开/关候选者，主机排序目前不使用主机电源效率，这变得越来越实用，因为越来越多的主机以可比的行业标准方式报告服务器功耗，有趣的是，这并不是 DPM 用户的常见要求，原因有二：许多用户在集群中使用来自同一硬件代的主机，因此主机的效率相似，并且在具有混合服务器代的集群中，较新的服务器既更大又更高效，因此基于主机大小的现有排序恰好与主机效率相吻合；DPM 的排序还可以考虑主机温度：关闭环境温度明显较高的主机可以减少更容易发生故障的主机的使用；另一个未来的方向是修改 DPM（和 DRS）以了解主机和集群功率上限，例如 HP 动态功率上限、Intel 节点管理器中的功率上限，功率上限是一种允许根据指定的峰值提供功率的机制，它可以限制受上限控制的主机的有效 MHz 容量，在了解功率上限和主机的功率效率的情况下，DPM 和 DRS 可以决定如何在主机上分配虚拟机，目的是根据上限满足虚拟机的 CPU 资源需求；另一个需要调查的是允许用户调整适合 DPM 的整合程度，对于某些用户来说，基于 CPU 和内存需求的整合过于激进，他们更希望利用率指标基于从虚拟机配置中得出的值。
****
### Kubernetes
### Borg, Omega, and Kubernetes
### [Burns2016](https://dl.acm.org/doi/pdf/10.1145/2898442.2898444)
