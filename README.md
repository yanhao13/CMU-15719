# CMU-15719
https://www.cs.cmu.edu/~15719/
### Introduction, Use cases, and Elasticity
### A View of Cloud Computing
### [Armbrust2010](https://dl.acm.org/doi/10.1145/1721654.1721672)
### Clearing the clouds away from the true potential and obstacles posed by this computing capability.
### Oracle's CEO Larry Ellison vent his frustration: "The interesting thing about cloud computing is that we've redefined cloud computing to include exerything that we already do.... I don't understand what we would do differently in the light of cloud computing other than change the wording of some of our ads."
### Just as large ISPs use multiple network providers so that failure by a single company will not take them off the air, we believe the only plausible solution to very high availability is multiple cloud computing providers.
cloud computing, the long-held dream of computing as a utility, has the potential to transform a large part of it industry, making sw even more attractive aaS and shaping the way it hw is designed and purchased; 
developers with innovative ideas for new internet services no longer require the large capital outlays in hw to deploy their service or the human expense to operate it, they need not be concerned about overprovisioning for a service whose popularity does not meet their predictions, hus wasting costly resources, or underprovisioning for one that becomes wildly popular, thus missing potential customers and revenue; 
moreover, companies with large batch-oriented tasks can get results as quickly as their programs can scale, since using 1000 servers for 1hr costs no more than using 1 server for 1000hrs, this elasticity of resources, without paying a premium for large scale, is unprecedented in history of it.
### Defining Cloud Computing
cloud computing refers to both the applications delivered aaS over the internet and the hw and systems sw in the data centers that provide those services, the services themselves have long been referred to as SaaS(*here we use the term SaaS to mean applications delivered over the internet, the broadest definition would encompass any on demand sw, including those that run sw locally but control use via remote sw licensing), some vendors use terms such as IaaS, PaaS to describe their products, but we eschew these because accepted definitions for them still vary widely, the line bt low-level infra and a higher-level platform is not crisp, we believe the two are more alike than different, and we consider them together; similarly, the related term grid computing, from the hp computing community, suggests protocols to offer shared computation and storage over long distances, but those protocols did not lead to a sw environment that grew beyond its community; 
the data center hw and sw is what we will call a **cloud**, when a cloud is made available in a pay-as-you-go manner to the general public, we call it a **public cloud**, the service being sold is **utility computing**, we use the term **private cloud** to refer to internal data centers of a business or other org, not made available to the general public, when they are large enough to benefit from the advantages of cloud computing that we discuss here; thus cloud computing is sum of SaaS and utility computing, but does not include small or medium sized data centers, even if these rely on virtualization for mgmt, people can be users or providers of SaaS, or users or providers of utility computing; we focus on SaaS providers(cloud users) and cloud providers, which have received less attention than SaaS users, [click for making provider-user relationships clear](./img/making-provider-user-relationships-clear.png), in some cases, the same actor can play multiple roles, for example, a cloud provider might also host its own customer-facing services on cloud infra; 
from a hw provisioning and pricing point of view, 3 aspects are new in cloud computing including: the appearance of infinite computing resources available on demand, quickly enough to follow load surges, thereby eliminating need for cloud computing users to plan far ahead for provisioning; elimination of an up-front commitment by cloud users, thereby allowing companies to start small and increase hw resources only when there is an increase in their needs(*note, however, that upfront commitments can still be used to reduce per-usage charges, for exmaple, AWS also offers long-term rental of servers, which they call reserved instances); the ability to pay for use of computing resources on a short-term basis as needed(such as processors by the hr and storage by the day) and release them as needed, thereby rewarding conservation by letting machines and storage go when they are no longer useful; 
we argue that construction and operation of extremely large scale, commodity-computer data centers at low cost locations was the key necessary enabler of cloud computing, for they uncovered factors of 5`7 decrease in cost of electricity, network bdwidth, ops, sw, and hw available at these very large economies of scale; these factors, combined with statistical multiplexing to increase utilization compared to traditional data centers, meant that cloud computing could offer services below the costs of a medium sized data center and yet still make a good profit; 
out proposed definition allows us to clearly identify certain installations as examples and non-examples of cloud computing, consider a public-facing internet service hosted on an ISP who can allocate more machines to the service given 4hrs notice, since load surges on the public internet can happen much more quickly than that(we saw its load to double every 12 hrs for nearly 3 days), this is not cloud computing, in contrast, consider an internal enterprise data center whose applications are modified only with significant advance notice to administrators, in this scenario large load surges on the scale of mins are highly unlikely, so as long as allocation can track expected load increases, this scenario fulfills one of the necessary conditions for operating as a cloud, the enterprise data center may still fail to meet other conditions for being a cloud, however, such as the appearance of infinite resources for fine-grained billing; a private data center may also not benefit from economies of scale that make public clouds financially attractive; 
omitting private clouds from cloud computing has led to considerable debate in the blogosphere, we believe the confusion and skepticism illustrated by Larry Ellison's quote occurs when advantages of public clouds are also claimed for medium sized data centers, except for extremely large data centers of hundreds of thousands of machines, such as those that might be operated by Google, Microsoft, [most data centers enjoy only a subset of the potential advantages of public clouds, click to view](./img/most-data-centers-enjoy-only-a-subset-of-the-potential-advantages-of-public-clouds.png), hence we believe that including traditional data centers in the definition of cloud computing will lead to exaggerated claims for smaller so-called private clouds, which is why we exclude them, however, here we describe how so-called private clouds can get more of the benefits of public clouds through surge computing, hybrid cloud computing.
### Classes of Utility Computing
any application needs a model of computation, a model of storage, and a model of comm, the statistical multiplexing necessary to achieve elasticity and the appearance of infinite capacity available on demand requires automatic allocation and mgmt, in practice, this is done with virtualization of some sort, we argue that different utility computing offerings will be distinguished based on the cloud sytem sw's level of abstraction and on the level of mgmt of resources; 
Amazon EC2 is at one end of the spectrum, an EC2 instance looks much like physical hw, and users can control nearly the entire sw stack, from the kernel upward, this low level makes it inherently difficult for Amazon to offer automatic scalability and failover because the semantics associated with replication and other state mgmt issues are highly application-dependent; at the other extreme of the spectrum are application domain-specific platforms such as Google AppEngine, which is targeted exclusively at traditional web applications, enforcing an application structure of clean separation bt a stateless computation tier and a stateful storage tier, AppEngine's impressive automatic scaling and high-availability mechanisms, and the proprietary MegaStore data storage available to AppEngine applications, all rely on these constraints; applications for Microsoft's Azure are written using .NET libs, and compiled to the Common Language Runtime, a lang-independent managed environment, the framework is significantly more flexible than AppEngine's, but still constrains user's choice of storage model and application structure, hence Azure is intermediate bt hw vms like EC2 and application frameworks like AppEngine.
### Cloud Computing Economics
we see 3 particularly compelling use cases that favor utility computing over conventional hosting, including: (i)when demand for a service varies with time, for example, provisioning a data center for the peak load it must sustain a few days per month leads to underutilization at other times, instead, cloud computing lets an org pay by hr of computing resources, potentially leading to cost savings even if the hourly rate to rent a machine from a cloud provider is higher than the rate to own one; (ii)when demand is unknown in advance, for example, a web startup will need to support a spike in demand when it becomes popular, followed potentially by a reduction once some visitors turn away; (iii)orgs that perform batch analytics can use the cost associativity of cloud computing to finish computations faster :using 1000 EC2 machines for 1hr costs the same as using 1 machine for 1000hrs; 
although the economic appeal of cloud computing is often described as CapEx2OpEx -converting capital expenses to operating expenses, we believe the phrase pay-as-you-go more directly captures the economic benefit to the buyer, hrs purchased via cloud computing can be distributed non-uniformly in time(for example, use 100 server hrs today and no server hrs tomorrow, and still pay only for 100), in the networking community, this way of selling bdwidth is already known as usage-based pricing(*usage-based pricing is not renting, renting a resources involves paying a negotiated cost to have the resource over some time period, whether or not you use the resource, vs., pay-as-you-go involves metering usage and charging based on actual use, independently of the time period over which the usage occurs); in addition, absence of upfront capital expense allows capital to be rediected to core business investment; 
hence, even if Amazon's pay-as-you-go pricing was more expensive than buying and depreciating a comparable server over the same period, we argue that the cost is outwrighed by the extremely important cloud computing economic benefits of elasticity and transference of risk, esp. risks of overprovisioning(underutilization) and underprovisioning(saturation); 
we start with elasticity, key observation is that cloud computing's ability to add or remove resources at a fine grain(1 server at a time with EC2) and with a lead time of mins rather than weeks allows matching resources to wkload much more closely, real world estimates of average server utilization in data centers range from 5%`20%, this may sound shockingly low, but it is consistent with the observation that for many services the peak wkload exceeds the average by factors of 2`10, since few users deliberately provision for less than the expected peak, resources are idle at nonpeak times, the more pronounced the variation, the more the waste, [click for example in figure2a](./img/elasticity.png), here, we assume our service has a predictable demand where the peak requires 500 servers at noon but the trough requires only 100 servers at midnight, as long as the average utilization over a whole day is 300 servers, the actual cost per day(area under the curve) is 300times24=7200 server hrs, but since we must provision to the peak of 500 servers, we pay for 500times24=12000 server hrs, a factor of 1.7 more; hence, as long as the pay-as-you-go cost per server hr over 3 years(typically amortization time) is less than 1.7 times cost of buying the server, utility computing is cheaper; in fact, this example underestimates benefits of elasticity, because in addition to simple diurnal patterns, most services also experience seasonal or other periodic demand variation(such as e-commerce in December and photo sharing sites after holidays) and some unexpected demand bursts due to external events(such as news events), since it can take weeks to acquire and rack new equipment, to handle such spikes you must provision for them in advance, we already saw that even if service operators predict the spike sizes correctly, capacity is wasted, and if they overestimate the spike they provision for, it is even worse; 
[they may also underestimate spike, click to view in figure2b](./img/elasticity.png), however, accidentally turning away excess users, while cost of overprovisioning is easily measured, cost of underprovisioning is more difficult to measure yet potentially equally serious :not only do rejected users generate 0 revenue, they may never come back; [for example, Friendster's decline in popularity relative to competitors Facebook, MySpace, is believed to have resulted partly from user dissatisfaction with slow repsonse times(up to 40 secs), click to view in figure2c](./img/elasticity.png) :users will desert an underprovisioned service until peak user load equals data center's usable capability, at which point users again receive acceptable service; for another simplified example, assume that users of a hypothetical site fall into 2 classes :active users(those who use the site regularly) and defectors(those who abandon the site due to poor perf), further, suppose that 10% of active users who receive poor service due to underprovisioning are permanently lost opportunities(become defectors), i.e., users who would have remained regular visitors with a better experience, the site is initially provisioned to handle an expected peak of 400000 users(1000 users per server times 400 servers), but unexpected positive press drives 500000 users in the first hr, of the 100000 who are turned away or receive bad service, by our assumption 10000 of them are permanently lost, leaving an active user base of 390000, the next hr sees 250000 new unique users, the first 10000 do fine, but the site is still overcapacity by 240000 users, this results in 24000 additional defections, leaving 376000 permanent users, if this pattern continues, after lg(500000) or 19hrs, #new users will approach 0 and the site will be at capacity in steady state, clearly, the service operator has collected less than 400000 users' worth of steady revenue during those 19hrs, however, again illustrating the underutilization argument, to say nothing of the bad reputation from disgruntled users; 
do such scenarios really occur in practice? when we made our service available via Facebook, it experienced a demand surge that resulted in growing from 50 servers to 3500 servers in 3 days, even if the average utilization of each server was low, no one could have foreseen that resource needs would suddenly double every 12hrs for 3 days, after the peak subsided, traffic fell to a lower level, so in this real world example, scale-up elasticity allowed the steady-state expenditure to more closely match the steady-state wkload.
### Top 10 Obstacles and Opportunities for Cloud Computing
[click to view](./img/top-ten-obstacles-and-opportunities-for-cloud-computing.png), the first 3 affect adoption, the next 5 affect growth, the last 2 are policy and business obstacles, each obstacle is paired with an opportunity to overcome that obstacle, ranging from product development to research projects.
### 拨开云雾，展现云计算的真正潜力，并消除其带来的障碍。
### Oracle 首席执行官 Larry Ellison 表达了他的不满：“云计算的有趣之处在于，我们重新定义了云计算，将我们已经在做的一切都涵盖其中……除了修改一些广告措辞之外，我不知道在云计算的背景下，我们还能做什么不同的事情。”
云计算，作为计算作为一种实用工具的长期梦想，有可能改变 IT 行业的很大一部分，使软件即服务 (aaS) 更具吸引力，并塑造硬件的设计和购买方式；
拥有创新互联网服务理念的开发人员不再需要投入大量的硬件资金来部署他们的服务，也不再需要投入大量的人力来运营它，他们不必担心为受欢迎程度未达预期的服务过度配置资源，从而浪费昂贵的资源，也不必担心为已经非常流行的服务配置资源不足，从而错失潜在客户和收入；
此外，拥有大量批处理任务的公司能够像其程序扩展一样快速获得结果，因为使用 1000 台服务器 1 小时的成本不超过使用 1 台服务器 1000 小时的成本，这种资源弹性在历史上是前所未有的，无需为大规模支付额外费用。
### 定义云计算
云计算既指通过互联网以“即服务”（aaS）形式交付的应用程序，也指数据中心中提供这些服务的硬件和系统软件。这些服务本身长期以来被称为 SaaS（*此处我们使用 SaaS 一词表示通过互联网交付的应用程序，最广泛的定义涵盖任何按需软件，包括在本地运行软件但通过远程软件许可控制使用的软件）。一些供应商使用 IaaS、PaaS 等术语来描述他们的产品，但我们避免使用这些术语，因为它们的公认定义仍然差异很大，低级基础设施和高级平台之间的界限并不明确，我们认为两者的相同之处多于不同之处，我们认为将它们结合在一起；同样，来自惠普计算社区的相关术语“网格计算”提出了提供远距离共享计算和存储的协议，但这些协议并没有催生出一个超越其社区的软件环境；数据中心硬件和软件就是我们所说的**云**，当云以按需付费的方式向公众开放时，我们称之为**公共云**，所出售的服务是**效用计算**，我们使用术语**私有云**来指代企业或其他组织的内部数据中心，这些数据中心不向公众开放，但当它们规模足够大到能够从我们在此讨论的云计算优势中受益时；因此，云计算是 SaaS 和效用计算的总和，但不包括中小型数据中心，即使这些数据中心依赖虚拟化进行管理，人们可以是 SaaS 的用户或提供商，也可以是效用计算的用户或提供商；我们关注的是 SaaS 提供商（云用户）和云提供商，这两者的关注度低于 SaaS 用户。[点击查看提供商-用户关系](./img/making-provider-user-relationships-clear.png)，在某些情况下，同一个参与者可以扮演多个角色，例如，云提供商可能还会在云端基础设施上托管自己的面向客户的服务；
从硬件配置和定价的角度来看，云计算有三个新特点：无限计算资源的出现，可以按需提供，并且能够快速应对负载激增，从而消除了云计算用户提前规划配置的需要；取消了云用户的前期承诺，从而允许公司从小规模开始，仅在需求增加时才增加硬件资源（*但请注意，前期承诺仍然可以用来降低每次使用的费用，例如，AWS 也提供服务器的长期租赁，他们称之为预留实例）；能够根据需要短期支付计算资源的使用费用（例如按小时支付处理器费用和按天支付存储费用），并根据需要释放这些资源，从而通过在不再使用时释放机器和存储来奖励节约；
我们认为，在低成本地点建设和运营超大规模商用计算机数据中心是云计算的关键必要因素，因为它们揭示了在这些非常大的规模经济下，电力、网络带宽、操作、软件和硬件成本降低 5 至 7 倍的因素；这些因素与统计复用相结合，提高了与传统数据中心相比的利用率，意味着云计算可以提供低于中型数据中心成本的服务，同时仍然获得良好的利润；
我们提出的定义使我们能够清楚地识别某些安装作为示例和非云计算的例子，考虑一个托管在 ISP 上的面向公众的互联网服务，该 ISP 可以在提前 4 小时通知的情况下为该服务分配更多机器，因为公共互联网上的负载激增可能发生得更快（我们看到它的负载每 12 小时翻一番，持续了近 3 天），这不是云计算。相反，考虑一个内部企业数据中心，其应用程序只有在提前通知管理员的情况下才会进行修改，在这种情况下，分钟级的大规模负载激增极不可能发生，因此只要分配可以跟踪预期的负载增长，这种情况就满足了作为云运营的必要条件之一。然而，企业数据中心可能仍然无法满足成为云的其他条件，例如出现无限资源以实现细粒度计费；私有数据中心也可能无法从使公共云具有经济吸引力的规模经济中受益；
将私有云从云计算中剔除在博客圈引发了相当大的争议，我们认为，拉里·埃里森的引言所体现的困惑和怀疑，发生在公有云的优势也适用于中型数据中心的情况下，除了拥有数十万台机器的超大型数据中心，例如谷歌、微软运营的那些。[大多数数据中心只享有公有云的部分潜在优势，点击查看](./img/most-data-centers-enjoy-only-a-subset-of-the-potential-advantages-of-public-clouds.png)，因此，我们认为将传统数据中心纳入云计算的定义会导致对规模较小的所谓私有云的夸大宣传，这就是我们将其排除在外的原因。然而，我们在这里描述了所谓的私有云如何通过激增计算、混合云计算获得更多公有云的优势。
### 效用计算的类别
任何应用程序都需要一个计算模型、一个存储模型和一个comm，实现弹性和按需无限容量所需的统计复用需要自动分配和管理，实际上，这是通过某种虚拟化实现的，我们认为不同的效用计算产品将根据云系统软件的抽象级别和资源管理级别进行区分；
Amazon EC2 处于一个极端，EC2 实例看起来很像物理硬件，用户可以控制几乎整个软件堆栈，从内核向上。这种低级别使得 Amazon 很难提供自动可扩展性和故障转移，因为与复制和其他状态管理问题相关的语义高度依赖于应用程序；另一个极端是应用程序领域特定平台，例如 Google AppEngine，它专门针对传统的 Web 应用程序，强制执行无状态计算层和有状态存储层干净分离的应用程序结构，AppEngine 令人印象深刻的自动扩展和高可用性机制，以及 AppEngine 应用程序可用的专有 MegaStore 数据存储，都依赖于这些约束；微软 Azure 的应用程序使用 .NET 库编写，并编译为公共语言运行时（Common Language Runtime，一个独立于语言的托管环境）。该框架比 AppEngine 的框架灵活得多，但仍然限制了用户对存储模型和应用程序结构的选择，因此 Azure 介于 EC2 等硬件虚拟机和 AppEngine 等应用程序框架之间。
### 云计算经济学
我们发现 3 个特别引人注目的用例表明，效用计算优于传统托管，包括：(i) 当服务需求随时间变化时，例如，为数据中心配置每月必须维持几天的峰值负载，会导致其他时间利用率不足。相反，云计算允许组织按小时支付计算资源费用，即使从云提供商租用机器的小时费率高于拥有机器的费率，也有可能节省成本； (ii) 当需求事先未知时，例如，一家网络初创公司需要支持其在流行时出现的需求激增，而一旦一些访问者流失，需求可能会减少；(iii) 执行批量分析的组织可以利用云计算的成本关联性来更快地完成计算：使用 1000 台 EC2 机器 1 小时的成本与使用 1 台机器 1000 小时的成本相同；
虽然云计算的经济吸引力通常被描述为资本支出转化为运营支出（CapEx2OpEx），即将资本支出转化为运营支出，但我们认为“按需付费”这一说法更直接地体现了买方的经济效益，通过云计算购买的服务器小时数可以不均匀地分配（例如，今天使用 100 小时服务器时间，明天不使用，仍然只需支付 100 小时的费用），在网络社区中，这种销售带宽的方式被称为基于使用量的定价（*基于使用量的定价不是租用，租用资源涉及支付协商好的费用以在一段时间内拥有资源，无论是否使用资源；而按需付费则涉及计量使用量并根据实际使用情况收费，与使用发生的时间段无关）；此外，由于没有前期资本支出，资本可以重新用于核心业务投资；因此，即使亚马逊的按需付费定价比在同一时期购买和折旧一台同类服务器的成本更高，我们认为，其成本也被云计算极其重要的经济效益所抵消，即弹性和风险转移，尤其是过度配置（利用不足）和配置不足（饱和）的风险；
我们从弹性开始，关键的观察是云计算能够以细粒度添加或删除资源（使用 EC2 一次添加或删除一台服务器），并且只需几分钟而不是几周的准备时间，就可以更紧密地匹配资源以进行负载均衡，现实世界中数据中心的平均服务器利用率估计在 5% 到 20% 之间，这听起来可能低得惊人，但这与以下观察结果一致：对于许多服务而言，峰值负载是平均值的 2 到 10 倍，因为很少有用户故意提供低于预期峰值的资源，资源在非峰值时段处于闲置状态，变化越明显，浪费就越多，[单击图 2a 中的示例](./img/elasticity.png)，在这里，我们假设我们的服务具有可预测的需求，峰值在中午需要 500 台服务器，但低谷在午夜只需要 100 台服务器，只要全天的平均利用率为 300 台服务器，则每天的实际成本（曲线下面积）为300x24=7200 服务器小时，但由于我们必须为 500 台服务器的峰值进行配置，因此我们需要支付 500x24=12000 服务器小时的费用，比峰值高出 1.7 倍；因此，只要 3 年内（通常是摊销期）每服务器小时的即用即付成本小于购买服务器成本的 1.7 倍，效用计算就会更便宜；事实上，这个例子低估了弹性的优势，因为除了简单的昼夜模式外，大多数服务还会经历季节性或其他周期性需求变化（例如 12 月的电子商务和节假日后的照片分享网站）以及一些由于外部事件（例如新闻事件）导致的意外需求爆发，因为购买和上架新设备可能需要数周时间，为了应对这样的峰值，您必须提前做好准备，我们已经看到，即使服务运营商正确预测了峰值规模，容量也会被浪费，如果他们高估了他们所准备的峰值，情况会更糟；
[他们也可能低估峰值，单击查看图 2b](./img/elasticity.png)，然而，意外地拒绝了多余的用户，虽然过度配置的成本很容易衡量，但配置不足的成本更难衡量，但可能同样严重：被拒绝的用户不仅不会产生任何收入，而且可能永远不会回来；[例如，Friendster 相对于竞争对手 Facebook、MySpace 的受欢迎程度下降被认为部分是由于用户对缓慢的响应时间（长达 40 秒）不满意，单击查看图 2c](./img/elasticity.png)：用户将放弃配置不足的服务，直到峰值用户负载等于数据中心的可用容量，此时用户再次获得可接受的服务；另一个简化的例子，假设一个网站的用户分为两类：活跃用户（经常使用该网站的用户）和流失用户（由于性能不佳而放弃该网站的用户）。进一步假设，由于配置不足而获得较差服务的用户中有 10% 永远失去了机会（成为流失用户），即那些本来可以成为常客并获得更好体验的用户。该网站最初配置为处理预期峰值 400,000 个用户（每台服务器 1,000 个用户乘以 400 台服务器），但意外的正面报道在第一个小时内吸引了 500,000 名用户，在 100,000 名被拒之门外或获得不良服务的用户中，根据我们的假设，其中 10,000 人永久流失，剩下 390,000 个活跃用户。下一个小时将看到 250,000 个新的独立用户，前 10,000 名用户表现良好，但该网站仍然超负荷 240,000 个用户。这导致24000名用户流失，留下376000名永久用户。如果这种模式持续下去，经过lg(500000)或19小时后，新增用户数量将接近0，站点将达到稳定状态。显然，服务运营商在这19个小时内获得的稳定收入还不到400000名用户的收入，但这再次证明了利用率不足的论点，更不用说来自不满用户的坏名声了。
这种情况在实践中真的会发生吗？当我们通过Facebook提供服务时，它经历了需求激增，导致服务器数量在3天内从50台增加到3500台。即使每台服务器的平均利用率很低，也没有人能预见到资源需求会在3天内每12小时突然翻一番。峰值消退后，流量会下降到更低的水平。因此，在这个现实世界的例子中，扩展弹性使稳定状态的支出能够更紧密地匹配稳定状态的工作负荷。
### 云计算的 10 大障碍和机遇
[点击查看](./img/top-ten-obstacles-and-opportunities-for-cloud-computing.png)，前 3 个影响采用，接下来的 5 个影响增长，后 2 个是政策和业务障碍，每个障碍都与克服该障碍的机会配对，从产品开发到研究项目。
### The NIST Definition of Cloud Computing
### [NISTdef2011](https://www.nist.gov/publications/nist-definition-cloud-computing)
### NIST -National Institute of Standards and Technology Definition of Cloud Computing
cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources(such as networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal mgmt effort or service provider interaction; this cloud model is composed of 5 essential characteristics, 3 service models, and 4 deployment models; 
essential characteristics: #1 on-demand self-service -a consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with each service provider, #2 broad network access -capabilities are available over the network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms(such as mobile phones, tablets, laptops, and workstations), #3 resource pooling -provider's computing resources are pooled to serve multiple consumers using a multitenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand, there is a sense of location independence in that the customer generally has no control or knowledge over the exact location of the provided resources but may be able to specify location at a higher level of abstraction(such as country, state, or datacenter), examples of resources include storage, processing, mem, and network bdwidth, #4 rapid elasticity -capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward commensurate with demand, to the consumer, the capabilities available for provisioning often appear to be unlimited and can be appropriated in any quantity at any time, #5 measured service -cloud systems automatically control and optimize resource use by leveraging a metering capability(*typically this is done on a pay-as-you-go or charge-per-use basis) at some level of abstraction appropriate to type of service(such as storage, processing, bdwidth, and active user accounts), resources usage can be monitored, controlled, and reported, providing transparency for both provider and consumer of the utilized service; 
service models: #1 SaaS -the capability provided to the consumer it to use provider's application running on a cloud infra(*a cloud infra is collection of hw and sw that enables the 5 essential characteristics of cloud computing, the cloud infra can be viewed as containing both a physical layer and an abstraction layer, the physical layer consists of the hw resources that are necessary to support the cloud services being provided and typically includes server, storage, and network components, the abstraction layer consists of sw deployed across the physical layer which manifests the essential cloud chracteristics, conceptually the abstraction layer sits above the physical layer), the applications are accessible from various client services through either a thin client interface, such as a web browser(such as web-based email), or a program interface, consumer does not manage or control th eunderlying cloud infra including network, servers, os, storage, or even individual application capabilities, with the possible exception of limited user-specific application config settings; #2 PaaS -the capability provided to the consumer is to deploy onto the cloud infra consumer-created or acquired applications created using programming langs, libs, services, and tools supported by the provider(*this capability does not necessarily preclude use of compatible programming langs, libs, services, and tools from other sources), the consumer does not manage or control the underlying cloud infra including network, servers, os, or storage, but has control over the deployed applications and possibly config settings for the application-hosting environment; #3 IaaS -the capability provided to consumer is to provision processing, storage, networks, and other fundamental computing resources where consumer is able to deploy and run arbitrary sw, which can include os and applications, consumer does not manage or control the underlying cloud infra but has control over os, storage, and deployed applications, and possibly limited control of select networking components(such as host firewalls); 
deployment models: #1 private cloud -the cloud infra is provisioned for exclusive use by a single org comprising multiple consumers(such as business units), it may be owned, managed, and operated by the org, a third party, or some combinition of them, and it may exist on or off premises; #2 community cloud -the cloud infra is provisioned for exclusive use by a specific community of consumers from orgs that have shared concerns(such as mission, security requirements, policy, and compliance considerations), it may be owned, managed, and operated by one or more of the orgs in the community, a third party, or some combinition of them, and it may exist on or off premises; #3 public cloud -the cloud infra is provisioned for open use by the general public, it may be owned, managed, and operated by a business, academic, or government org, or some combinition of them, it exists on the premises of the cloud provider; #4 hybrid cloud -the cloud infra is a composition of two or more distinct cloud infras that remain unique entities, but are bound together by standardized or proprietary technology that enables data and application portability(such as cloud bursting for load balancing bt clouds).
### Dynamically Scaling Applications in the Cloud
### [Vaquero11](https://dl.acm.org/doi/pdf/10.1145/1925861.1925869)
### Scalability is said to be one of the major advantages brought by the cloud paradigm and, more specifically, the one that makes it different to an advanced outsourcing solution, however, there are some important pending issues before making the dreamed automated scaling for applications come true.
cloud computing is commonly associated to offering of new mechanisms for infra porvisioning including: the illusion of a virtually infinite computing infra, the employment of advanced billing mechanisms allowing a pay-per-use model on shared multitenant resources, the simplified programming mechanisms(platform); among these features, those introduced by adding scalability and automated on-demand self-service are responsible for making any particular service something more than just an outsourced service with a prettier marketing face; 
as a result of its relevance, the wealth of systems dealing with cloud application scalability is slowing gaining weight in the available literature; automation is typically achieved by using a set of service provider-defined rules that govern how the service scales up or down to adapt to a variable load, these rules are themselves composed of a condition, which when met triggers some action on infra or platform, the degree of automation, abstraction for the user(service provider) and customization of rules governing the service vary, some systems offer users chance of building rather simple conditions based on fixed infra or platform matrics(such as cpu, mem, etc.), while others employ server-level metrics(such as cost to benefit ration) and allow for more complex conditions(such as arithmetic and logic combinations of simple rules) to be included in the rules, regarding the subsequent actions launched when conditions are met, available efforts focus on service horizontal scaling(i.e. adding new server replicas and load balancers to distribute load among all available replicas) or vertical scaling(i.e. on-th-fly changing of assigned resources to an already running instance, such as letting more physical cpu to a running vm), unfortunately, the most common os do not support on-the-fly(without rebooting) changes on the available cpu or mem to support this vertical scaling; 
beyond mere server scalability, some other elements need to be taken into account that affect the overall application scaling potential, for example, LBs -load balancers need to support aggregation of new servers(typically, but not necessarily, in the form of new vms) in order to distribute load among several servers, Amazon already provides strategies for load balancing your replicated vms via its elastic load balancing capabilities, hence LBs and the algorithms that distribute load to different servers are essential elements in achieving application scalability in the cloud; 
having several servers and the mechanisms to distribute load among them is a definitive step towards scaling a cloud application, however, there is another element of the datacenter infra to be considered towards complete application scalability, network scalability is an often neglected element that should also be considered; in a consolidated datacenter scenario, several vms share the same network, potentially producing a huge increase in the equired bdwidth(potentially collapsing the network), hence it is necessary to extend infra clouds to other kinds of underlying resources beyond servers, LBs, and storage; cloud applications should be able to request not only virtual servers at multiple points in the network, but also bdwidth-provisioned network pipes and other network resources to interconnect them -NaaS; 
clouds that offer simple virtual hw infra such as vms and networks are usually denoted IaaS, vs., PaaS clouds provide sets of online libs and service for supporting application design, implementation, and maintenance, despite being somewhat less flexible than IaaS clouds, PaaS clouds are becoming important elements for building applications in a faster manner and many important it players scuh as Google, Microsoft, have developed new PaaS clouds systems such as Google AppEngine, Microsoft Azure, we discuss scalability in PaaS clouds at 2 different levels -container level and db level; 
[click for an overview of mechanisms handy to accomplish the goal of the whole application scalability](./img/overview-of-mechanisms-handy-to-accomplish-the-goal-of-the-whole-application-scalability.png).
### Server Scalability
most of the available IaaS clouds deal with single vm mgmt primitives(such as elements for adding/removing vms), lacking mechanisms for treating applications as a whole single entity and dealing with relations among different application components, for example, relationships bt vms are often not considered, ordered deployment of vms contaning sw for different tiers of an appliation is not automated(such as the db's ip is only known at deployment time hence the db needs to be deployed first in order to get its ip and configure the web server connecting to it), etc., application providers typically want to deal with their application only, being released from burden of dealing with (virtual) infra terms;
**towards increased abstraction and automation :the elasticity controller:** such a fine-grained mgmt(vm-based) may come on handy for few services or domestic users, but it may become intractable with a big number of deployed applications composed of several vms each, the problem gets worse if application providers aim at having their application automatically scaled according to load, they would need to monitor every vm for every application in the cloud and make decisions on whether or not every vm should be scaled, its LB re-configured, its network resources resizes, etc., 2 different approaches are possible: increasing the abstraction level of provided APIs and/or advancing towards a higher automation degree;
automated scaling features are being included by some vendors, but the rules and policies they allow to express still deal with individual vms only, one cannot easily relate the scaling of vms at tier-1 with its load balancers or the scaling of vms at tier-3;
on the other hand, more abstract frameworks are proposed(they allow users to deal with applications as a whole rather than per individual vm) that also convey automation, unavoidably, any scalability mgmt system(or [elasticity controller, click to view](./img/elasticity-controller.png)) is bound to the underlying cloud API(the problem of discrete actuators), hence one essential task for any application-level elasticity controller is mapping user scaling policies from the appropriate level of abstraction for the user to actual mechanisms provided by IaaS clouds(depending on specific underlying API, *in practice, most of the available cloud APIs expose vm-level mgmt capabilities, so that controller-actuator pairs are limited to adding/removing vms); the implementation of elasticity controller can be done in several different ways with regard to the provided abstraction level: (i)a per-tier controller, so that there is a need for coordination and sync among multiple controller-actuator pairs, treating each tier as an independent actuator with its own control policy can cause shifting of the perf bottleneck bt tiers, a tier can only release resources when an interlock is not being held by the other tiers, (ii)a single controller for the whole application(for all tiers), which let users specify how an application should scale in a global manner, for example, application provider could specify(based on its accurate knowledge of application) to scale the application logic tier whenever #incoming reqs at the web tier is beyond a given threshold;
**expressing how and when to scale :feeding controller with rules and policies:** all the works above rely on traditional control theory in which several sensors feed a decision making module(elasticity controller) with data to operate on an actuator(cloud API), [click to view](./img/elasticity-controller.png), similarly, all the systems above answer the question on how to automate scalability(i.e. how to implement elasticity controller) in a similar manner either for vm-level or for application-level scalability mgmt systems;
user-defined rules are the chosen mechanism(as opposed to preconfigured equations sets) to express the policies controlling scalability as shown below: *RULE: if CONDITION(s) then ACTION(s) CONDITION: (1..*) (metric.value MODIFIER) ACTION: (*) IaaS cloud-enabled actions(such as deploy new vm)*, a rule is composed of a series of conditions that when met trigger some action over the underlying cloud, every condition itself is composed of a series of metrics or events that may be present in the system, such as money available, authorization received, etc.(either provided by infra or obtained by application provider itself), which are used together with a modifier(either a comparison against a threshold, the presence of such events) to trigger a set of cloud-provided actions;
if we focus on application-level scalability(rather than dealing with per vm scaling), a mathematical foumulation in which user just configures threshold for replicating storage servers so as to increase the cloud storage capability is proposed(*called proportional thresholding mechanism, which considers the fact that going from 1 to 2 machines can increase capacity by 100% but going from 100 to 101 machines increases capacity by no more than 1%), a leave full control for users to express their rules and use a rule engine as a controller is also proposed, the OVF -open virtual format is extended to define the application, its components, its contextualization needs, and the rules for scaling, this ways, service providers can generate a description of their application components, the way their application behaves with regard to scalability and the relevant metrics that will trigger action expressed in such rules;
 


